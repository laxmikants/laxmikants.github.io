---
title: "Regression"
author: "Laxmikant"
description: "Statistics, Regression, Machine Learning"
date: 2019-03-05
lastmod: 2019-03-05
categories: ["Machine Learning"]
subtitle: Regression
tags: ["Statistics","Regression","Machine Learning"]
lua:
  image:
    url: "/img/cover.png"
    width: 800
    height: 600
---

<!--more-->


```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


## Definition

A linear equation that models a function such that if we give any `x` to it, it will predict a value `y` , where both `x and y` are input and output variables respectively. These are numerical and continous values.
It is the most simple and well known algorithm used in machine learning.

## Flowchart 

<p align = 'center'><img src = '/img/Linear_Reg_Flowchart.png'>
</p>

<br>

The above Flowchart represents that we choose our training set, feed it to an algorithm, it will learn the patterns and will output a function called `Hypothesis function 'H(x)'`. We then give any `x` value to that function and it will output an estimated `y` value for it.

For historical reasons, this function `H(x)` is called `hypothesis function.`

## Cost Function

The best fit line to our data will be where we have least distance between the `predicted 'y' value` and `trained 'y' value`.

## Formula for Cost Function

<p align = 'center'><img src = '/img/MSE.png'></p>

> Where :
>- h(x<sub>i</sub>) ðŸ‘‰ hypothesis function
>- y<sub>i</sub> ðŸ‘‰ actual values of `y`
>- 1/m ðŸ‘‰ gives Mean of Squared Errors
>- 1/2 ðŸ‘‰ Mean is halved as a convenience for the computation of the `Gradient Descent`.


```{r}
computeCost <- function (X, y, theta){
        # number of training examples
        m <- length(y);
        # need to return
        J <- 0;
        
        predictions <-  X %*% theta;
        sqerrors = (predictions - y)^2;
        J = 1/(2*m)* sum(sqerrors);
        
        J
    }

```
The above formula takes the sum of the distances between <i>`predicted values` and `actual values` of training set, sqaure it, take the average and multiply it by `1/2`.</i>
<br>
<br>
This cost function is also called as `Squared Error Function` or `Mean Squared Error`.
<br>
<br>
Why do we take squares of the error's?<br>
The `MSE` function is commonly used and is a reasonable choice and works well for most Regression problems.
<br>
<br>
Let's subsititute `MSE` function to function `J` :
<p align = 'center'><img src = '/img/MSE1.png'></p>

<br>
<br>


## Gradient Descent 

So now we have our hypothesis function and we have a way of measuring how well it fits into the data. Now we need to estimate the parameters in the hypothesis function. That's where `Gradient Descent` comes in.<br>
`Gradient Descent` is used to minimize the cost function `J`, minimizing `J` is same as minimizing `MSE` to get best possible fit line to our data.

$$\displaystyle \min_{\theta_0,\theta_1}\frac{1}{2m}\sum_{i=1}^{m} \left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2$$


## Formula for Gradient Descent
<p align = 'center'><img src = '/img/Gradient_Descent.PNG'></p>

> Where :
>- `:=`  Is the Assignment Operator
>- `Î±`  is `Alpha`, it's the number which is called learning rate. If its too high it may fail to converge and if too low then descending will be slow.
>- 'Î¸<sub>j</sub>'  Taking Gradient Descent of a feature or a column of a dataset.
> - âˆ‚/(âˆ‚Î¸<sub>j</sub>) J(Î¸<sub>0</sub>,Î¸<sub>1</sub>)  Taking partial derivative of `MSE` cost function.

<br>

```
    gradientDescent <- function(X, y, theta, alpha, num_iters){
        m <- length(y);  
        J_history = rep(0, num_iters);
        for (iter in 1:num_iters){
            predictions <-  X %*% theta;
            updates = t(X) %*% (predictions - y);
            theta = theta - alpha * (1/m) * updates;
            J_history[iter] <- computeCost(X, y, theta);
        }
        list("theta" = theta, "J_history" = J_history)  
    }
```



**Now Let's apply Gradient Descend to minmize our `MSE` function.**
<br>
In order to apply `Gradient Descent`, we need to figure out the partial derivative term.<br>
So let's solve partial derivative of cost function `J`.

<br>

<p align = 'center'><img src = '/img/Solving_Partial_Derivative.PNG'></p>

<br>

Now let's plug these 2 values to our `Gradient Descent`:

<br>

<p align = 'center'><img src = '/img/Final_Gradient_Descent.PNG'></p>

<br>


## Example Code in R


```R

```{r}
computeCost  <- function(X, y, theta) {
  m <- length(y) 
  dif <- X %*% theta - y
  J <- (t(dif) %*% dif) / (2 * m)
  J
}
```


```{r}
linear_regression_gd <- function(X, y, theta, alpha, num_iters) {
  
  m <- length(y) 
  
  J_history = rep(0, num_iters + 1)
  
  theta_history = matrix(0, num_iters + 1, length(theta))
  
  theta_history[1,] = t(theta)
  
  J_history[1] = computeCost(X, y, theta)
  
  for (iter in 2:(num_iters + 1)) {

        theta_prev = theta
    
        # number of features.
        p = dim(X)[2]
    
        # simultaneous update theta using theta_prev.
        for (j in 1:p) {
          deriv = (t(X %*% theta_prev - y) %*% X[, j]) / m
          theta[j] = theta_prev[j] - (alpha * deriv)
        }
    
      # Save the cost J in every iteration
      J_history[iter] = computeCost(X, y, theta)
      theta_history[iter,] = t(theta)
  }
  
  list(theta = theta, J_history = J_history, theta_history = theta_history)
  # ------------------------------------------------------------
}

```
```{r}
plotData <- function (x, y) {
  print(length(x))
  print(length(y))
  plot(
    x, y, col = "red", pch = 4,cex = 1.1,lwd = 2,
    xlab = '',
    ylab = ''
  )
  
  # ------------------------------------------------------------
  
}

```



```{r}

computeSlopeIntercept <- function(X,y) {
  
  m <- length(y) # number of training examples
  
  X <- cbind(rep(1,m),X)
  
  X <- as.matrix(X)
  
  theta <- c(0,0)

  iterations <- 1000000
  
  alpha <- 0.01
  
  gd <- linear_regression_gd(X, y, theta, alpha, iterations)

  theta <- gd$theta
  
  J_history <- gd$J_history
  
  
  theta_history <- gd$theta_history
  
  print(length(gd$theta_history))

  plot(1:length(gd$J_history), gd$J_history, type = 'l')
  
  rm(gd)

  list(intercept=theta[1], slope=theta[2])

}

```

```{r}
predictY <- function(newdata,theta) {
    
  X <- cbind(rep(1,length(newdata)),newdata)
  
  predicted_y = as.matrix(X) %*% theta
  
  predicted_y

}

```

```{r}
featureNormalize <- function(X) {
  
  X_norm <- X
  mu <- rep(0,dim(X)[2])
  sigma <- rep(0,dim(X)[2])

    for (p in 1:dim(X)[2]) {
    mu[p] <- mean(X[,p])
  }
  
  for (p in 1:dim(X)[2]) {
    sigma[p] <- sd(X[,p])
  }
  
  for (p in 1:dim(X)[2]) {
    if (sigma[p] != 0)
      for (i in 1:dim(X)[1])
        X_norm[i, p] <- (X[i, p] - mu[p]) / sigma[p]
      else
        X_norm[, p] <- t(rep(0,dim(X)[1]))
  }
  
  list(X_norm = X_norm, mu = mu, sigma = sigma)
}
```


``` {r}

Input =("
x	    y  
1	    1
2      4	
3      9	
")

Data = read.table(textConnection(Input),header = TRUE)

csi = computeSlopeIntercept(Data$x,Data$y)


newdata = c(4,5)

theta = c(csi$intercept,csi$slope)

ypredicted = predictY(as.vector(newdata),theta)

print(ypredicted)

print(csi)

```



    [1] 200002
    [1] "coefficeints"
    [1] -3.333333
    [1] 4
    [1] "Predicted values for 4 & 5"
    [1] 12.66667 16.66667
    
    
### Applications

- Sales Forecasting
- Demand Supply Forecasting
- Operations cost optimization
- Insurance industry - claim prediction
- Banking
- Healthcare industry - cost prediction
- Ecommerce industry - Recommandation System 

### Key Points

- If sample is small ( < 10000) then normal equation can be used to get the theta values

- As the training set size increases it is better to use gradient descent algorithm instead of normal equation 

- If sample data contains large digits for x, y values then it is better to scale the values around mean before applying cost function and gradient descent

- In R language `lm(x~y)` can be used directly for determining theta values which is more efficient than using gradient descent algorithm

- Initially it is better to calculate correlation coeficient to ensure that variables are related in some way

- Normalizing data is important to deal with when individual values are numerically large ( > 4 digits)

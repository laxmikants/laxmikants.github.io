<!doctype html><html lang=en itemscope itemtype=http://schema.org/WebPage><head><script data-ad-client=ca-pub-3804322353139756 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-155379268-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-155379268-1');</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Decision Trees","mainEntityOfPage":{"@type":"WebPage","@id":"https://laxmikants.github.io/post/decision-trees-in-ml/"},"image":{"@type":"ImageObject","url":"https://laxmikants.github.io/img/cover.png","width":800,"height":600},"genre":"post","keywords":"Statistics, Regression, Machine Learning, Random Forest","wordcount":1703,"url":"https://laxmikants.github.io/post/decision-trees-in-ml/","datePublished":"2018-06-05","dateModified":"2018-06-05","license":"This work is licensed.","publisher":{"@type":"Organization","name":"Laxmi K Soni","logo":{"@type":"ImageObject","url":"https://laxmikants.github.io/img/laxmikant-soni-1.jpg","width":60,"height":60}},"author":{"@type":"Person","name":"Laxmi K Soni"},"description":"Decision Trees in ML"}</script><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=keywords content="Posts and Resources on Data Science | Laxmikant Soni,Statistical population,Probability,False positives,Statistical inference,Regression,Fitting,Categorical data,Classification,Clustering,Statistical comparison,CodingDistributions,Data mining,Decision trees,Machine learning,Munging and wrangling,Visualization,D3,Regularization,Assessment,Cross-validation,Neural networks,Boosting,Lift,Mode,Outlier,Predictive modeling,Big data,Confidence interval,Python,R,Jupyter Notebook,Tensorflow,Javascript,ReactJS,NodeJS,Posts and Resources on Data Science,Data Science,Hadoop,Java,Spring,Hibernate,Struts,MySQL,Oracle,DB2,Websphere,Weblogic"><meta name=robots content=index><meta name=description content="Posts and Resources on Data Science | Laxmikant Soni"><title>Laxmikant Soni</title><meta property=og:title content="Posts and Resources on Data Science | Decision Trees"><meta property=fb:app_id content=428818034507005><meta name=description content="Decision Trees in ML"><meta property=og:description content="Decision Trees in ML"><meta name=author content="Laxmikant Soni"><script type=application/ld+json>{"@context":"http://schema.org","@type":"Article","author":{"@type":"Person","name":"Laxmi K Soni"},"headline":"Decision Trees","description":"Decision Trees in ML","inLanguage":"en","wordCount":1703,"datePublished":"2018-06-05","dateModified":"2018-06-05","image":"https://laxmikants.github.io/img/cover.png","keywords":["Statistics, Regression, Machine Learning, Random Forest"],"mainEntityOfPage":"https://laxmikants.github.io/post/decision-trees-in-ml/","publisher":{"@type":"Organization","name":"Laxmi K Soni","logo":{"@type":"ImageObject","url":"https://laxmikants.github.io/img/laxmikant-soni-1.jpg","height":60,"width":60}}}</script><link href=https://laxmikants.github.io/img/favicon.ico rel=icon type=image/x-icon><meta property=og:image content=https://laxmikants.github.io/img/laxmikant-soni-1.jpg><meta property=fb:app_id content=428818034507005><meta property=og:url content=https://laxmikants.github.io/post/decision-trees-in-ml/><meta property=og:type content=article><meta property=og:site_name content="Laxmikant Soni"><meta property=og:description content="Data Science blog: Posts, Tutorials and resources for Data Science programming languages, frameworks, tools, etc."><meta property=og:locale content=en_IN><meta property=og:type content=article><meta content="Decision Trees in ML" property=og:description><meta property=og:url content=https://laxmikants.github.io/post/decision-trees-in-ml/><meta property=og:site_name content="Laxmikant Soni"><meta property=article:section content="Posts and Resources on Data Science | Laxmikant Soni"><meta property=article:section content="Statistical population"><meta property=article:section content=Probability><meta property=article:section content="False positives"><meta property=article:section content="Statistical inference"><meta property=article:section content=Regression><meta property=article:section content=Fitting><meta property=article:section content="Categorical data"><meta property=article:section content=Classification><meta property=article:section content=Clustering><meta property=article:section content="Statistical comparison"><meta property=article:section content=CodingDistributions><meta property=article:section content="Data mining"><meta property=article:section content="Decision trees"><meta property=article:section content="Machine learning"><meta property=article:section content="Munging and wrangling"><meta property=article:section content=Visualization><meta property=article:section content=D3><meta property=article:section content=Regularization><meta property=article:section content=Assessment><meta property=article:section content=Cross-validation><meta property=article:section content="Neural networks"><meta property=article:section content=Boosting><meta property=article:section content=Lift><meta property=article:section content=Mode><meta property=article:section content=Outlier><meta property=article:section content="Predictive modeling"><meta property=article:section content="Big data"><meta property=article:section content="Confidence interval"><meta property=article:section content=Python><meta property=article:section content=R><meta property=article:section content="Jupyter Notebook"><meta property=article:section content=Tensorflow><meta property=article:section content=Javascript><meta property=article:section content=ReactJS><meta property=article:section content=NodeJS><meta property=article:section content="Posts and Resources on Data Science"><meta property=article:section content="Data Science"><meta property=article:section content=Hadoop><meta property=article:section content=Java><meta property=article:section content=Spring><meta property=article:section content=Hibernate><meta property=article:section content=Struts><meta property=article:section content=MySQL><meta property=article:section content=Oracle><meta property=article:section content=DB2><meta property=article:section content=Websphere><meta property=article:section content=Weblogic><meta property=article:section content=AWS><meta property=article:published_time content="2018-06-05 00:00:00 &#43;0000 UTC"><meta name=twitter:site content=@laxmikantsoni09><meta name=twitter:creator content=@laxmikantsoni09><meta name=twitter:description content="HUGOMORE42 Decision treesDecision trees area tree-like tool which can be used to represent a cause and its effect.In Machine Learning Decision trees are a type of Supervised machine learning where datais split according to given parameter while constructing a tree to solve a given problem.In decision tree there is a predictor variable and target variable or the desired output.The predictor variable could be anything such as technical indicators etc and the targetvariable could be desired output for example whether to invest in a given financial security or not."><meta name=twitter:title content="Decision Trees"><meta name=twitter:card content=summary_large_image><meta name=twitter:image content><meta name=generator content="Hugo 0.53"><link rel=alternate href=https://laxmikants.github.io/index.xml type=application/rss+xml title="Laxmikant Soni"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css integrity=sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.5.0/css/all.css integrity=sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU crossorigin=anonymous><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css integrity=sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u crossorigin=anonymous><link rel=stylesheet href=https://laxmikants.github.io/css/main.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic"><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800"><link rel=stylesheet href=https://laxmikants.github.io/css/highlight.min.css><link rel=stylesheet href=https://laxmikants.github.io/css/codeblock.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css integrity=sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css integrity=sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R crossorigin=anonymous><script src=https://yihui.org/js/math-code.js></script><script async src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-155379268-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body><span itemscope itemtype=https://schema.org/Organization><link itemprop=url href=http://laxmikants.github.io><a itemprop=sameAs href=http://www.facebook.com/laxmikantsoni09>FB</a>
<a itemprop=sameAs href=http://www.twitter.com/laxmikantsoni09>Twitter</a>
<a itemprop=sameAs href=http://github.com/laxmikants>Github</a>
<a itemprop=sameAs href=http://linkedin.com/in/laxmikantsoni09>LinkedIn</a>
<a itemprop=sameAs href=http://www.kaggle.com/laxmikantsoni>Kaggle</a></span><nav class="navbar navbar-default navbar-fixed-top navbar-custom"><div class=container-fluid><div class=navbar-header><button type=button class=navbar-toggle data-toggle=collapse data-target=#main-navbar>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span><span class=icon-bar></span><span class=icon-bar></span></button>
<a class=navbar-brand href=https://laxmikants.github.io>Laxmikant Soni</a></div><div class="collapse navbar-collapse" id=main-navbar><ul class="nav navbar-nav navbar-right"><li class=navlinks-container><a class=navlinks-parent>About</a><div class=navlinks-children><a href=https://laxmikants.github.io/page/aboutme>About Me</a>
<a href=https://laxmikants.github.io/page/consulting>Consulting</a></div></li><li class=navlinks-container><a class=navlinks-parent>Data Science Resources</a><div class=navlinks-children><a href=https://laxmikants.github.io/page/ds-resources>Data Science Links</a></div></li><li class=navlinks-container><a class=navlinks-parent>Posts</a><div class=navlinks-children><a href=https://laxmikants.github.io/post/regression-in-ml/>Regression</a>
<a href=https://laxmikants.github.io/post/up-and-running-with-blogdown/>Blogdown</a>
<a href=https://laxmikants.github.io/post/authoring-scientific-paper/>Scientific paper</a></div></li></ul></div><div class=avatar-container><div class=avatar-img-border><a title="Laxmikant Soni" href=https://laxmikants.github.io><img class=avatar-img src=https://laxmikants.github.io/img/laxmikant-soni-1.jpg alt="Laxmikant Soni"></a></div></div></div></nav><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header class=header-section><div class="intro-header no-img"><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><h1>Decision Trees</h1><h2 class=post-subheading>Decision Trees in ML</h2><span class=post-meta><i class="fas fa-calendar"></i>&nbsp;Posted on June 5, 2018
&nbsp;|&nbsp;<i class="fas fa-clock"></i>&nbsp;8&nbsp;minutes
&nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;1703&nbsp;words
&nbsp;|&nbsp;<i class="fas fa-user"></i>&nbsp;Laxmi K Soni</span></div></div></div></div></div></header><div class=container role=main><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><article role=main class=blog-post>HUGOMORE42<div id=decision-trees class="section level1"><h1>Decision trees</h1><p>Decision trees area tree-like tool which can be used to represent a cause and its effect.
In Machine Learning Decision trees are a type of Supervised machine learning where data
is split according to given parameter while constructing a tree to solve a given problem.
In decision tree there is a predictor variable and target variable or the desired output.
The predictor variable could be anything such as technical indicators etc and the target
variable could be desired output for example whether to invest in a given financial security or not.</p></div><div id=how-it-works class="section level1"><h1>How it works</h1><p>Decision trees are one of the most interesting applications of machine learning.
A decision basically gives flowchart of how to make some decision.
You have some dependent variable, like whether to buy the stock depending on
factors like RSI, MACD etc. When you have a decision like that that depends on
multiple attributes or multiple variables, a decision tree could be a good choice.</p><p>There are many different aspects of the weather that might influence my decision
to buy a given stock. It might have to do with the stock closing prise today,
the RSI,MACD, EMA etc. A decision tree can look at all these different features
of the stock, and decide what are the thresholds. What are those factors which affects
the stock movement.</p><p>For example, Factors affecting the stock movement is shown by using decision tree</p></div><div id=understanding-decision-tree class="section level1"><h1>Understanding Decision Tree</h1><p>At each step of the decision tree flowchart, we find the attribute that we can partition
our data on that minimizes the entropy of the data at the next step. So we have a resulting
set of classifications in this case “BUY” or “SELL”, and we want to choose the attribute
decision at that step that will minimize the entropy at the next step. So we just walk
down the tree, minimize entropy at each step by choosing the right attribute to decide on,
and we keep on going until we run out.</p><p><img src=https://laxmikants.github.io/post/2018-06-05-DecisionTrees_files/figure-html/unnamed-chunk-2-1.png width=672></p><blockquote><p>The decision to buy the stock in majority of the cases is dependent on the stock fundamentals.</p></blockquote></div><div id=decision-tree-example class="section level1"><h1>Decision tree example</h1><p>Let’s say I want to build a system that will automatically predicts the stock movement at the
end of the day based on the opening price of the stock. Given a stock the system should decide
whether that stock is going to have upward movment or downward movement during intraday trading
so that investor can make a decision of investment in that stock.</p><p>So let’s make some totally fabricated stock data that we’re going to use in this example:</p><table><thead><tr class=header><th>Open</th><th>Close</th><th>RSI3</th><th>EMAAcross</th><th>MACDsignal</th><th>stochastic</th><th>WPR</th><th>class</th></tr></thead><tbody><tr class=odd><td>2646.1</td><td>2615.1</td><td>15.90374</td><td>-73.4791018</td><td>1.71651251</td><td>63.2385726</td><td>0.96266916</td><td>DOWN</td></tr><tr class=even><td>2620</td><td>2671.4</td><td>12.80266</td><td>-66.3860679</td><td>1.578709892</td><td>58.6011697</td><td>0.69995334</td><td>UP</td></tr><tr class=odd><td>2694</td><td>2672.9</td><td>52.3322</td><td>5.0759547</td><td>1.445803175</td><td>53.2889418</td><td>0.6929538</td><td>DOWN</td></tr><tr class=even><td>2681</td><td>2649.5</td><td>46.74773</td><td>-5.2826968</td><td>1.311960853</td><td>47.6979625</td><td>0.80214652</td><td>DOWN</td></tr><tr class=odd><td>2630</td><td>2644.05</td><td>28.71555</td><td>-37.5217979</td><td>1.151342446</td><td>41.4037634</td><td>0.67950192</td><td>UP</td></tr><tr class=even><td>2628</td><td>2657.85</td><td>28.07845</td><td>-26.3478653</td><td>0.978453007</td><td>34.5108298</td><td>0.62662835</td><td>UP</td></tr><tr class=odd><td>2659.1</td><td>2678.85</td><td>52.60546</td><td>3.1680898</td><td>0.823178316</td><td>27.6442234</td><td>0.54616858</td><td>UP</td></tr><tr class=even><td>2685</td><td>2683.25</td><td>66.76415</td><td>19.3787266</td><td>0.700641564</td><td>21.3573951</td><td>0.52931034</td><td>DOWN</td></tr><tr class=odd><td>2650</td><td>2782.35</td><td>41.5832</td><td>-10.4141823</td><td>0.582583825</td><td>15.3270181</td><td>0.14961686</td><td>UP</td></tr><tr class=even><td>2799</td><td>2801.55</td><td>82.8612</td><td>92.3905451</td><td>0.560836593</td><td>11.2955015</td><td>0.08447229</td><td>UP</td></tr><tr class=odd><td>2803</td><td>2785.8</td><td>83.33538</td><td>64.2603634</td><td>0.601242222</td><td>9.454404</td><td>0.14426727</td><td>DOWN</td></tr></tbody></table><p>In the preceding table, we have stock prices along with technical indicators. We are going to
pick some attributes that we think might be interesting or helpful to predict whether or not
they can predict movement of the stock (UP or DOWN). How much is William %R ? What is exponential
moving average ? What is the value of stochastic momentun index ? Is the stock overbought/oversold (RSI) ?
Depending on the factors which affect the stock price we can predict whether it will go up or down.</p><p>Now, obviously there’s a lot of information that isn’t in this model that might be very important,
but the decision tree that we train from this data might actually be useful in doing an initial
pass at weeding out some candidates. What we end up with might be a tree that looks like the following:</p><p><img src=https://laxmikants.github.io/post/2018-06-05-DecisionTrees_files/figure-html/unnamed-chunk-3-1.png width=672></p><p>So it just turns out that in totally fabricated data, if the william %R (WPR) is below 0.17
then the stock will go UP. So the first questioon decision tree related to WPR, i.e if WPR is
above 0.17 then go to left. At node 2 check if the exponential moving average is above 9.7, if
it is then we end up at the leaf node predicted value to stock going down.
If at node 2 exponential moving average is below 9.7 then go to node 5 and check the
value of stochastic mementun index, if it is above 42 then go to right at leaf node 11 having
stock predicted value of UP and so on.</p></div><div id=walking-through-a-decision-tree class="section level1"><h1>Walking through a decision tree</h1><p>So that’s how we walk through the results of a decision tree. It’s just like going through a flowchart, and it’s kind of awesome that an algorithm can produce this for us. The algorithm itself is actually very simple. Let me explain how the algorithm works.</p><p>At each step of the decision tree flowchart, we see the attribute that we can partition our data on that minimizes the entropy of the data at the next step. So we have a resulting set of classifications in this case UP or DOWN, and we want to choose the attribute decision at that step that will minimize the entropy at the next step.</p><blockquote><p>Entropy meausures the level of impurity in a group. The group having minimum entropy is helps determinie attribute most useful for descriminating between classes to be learned.</p></blockquote><p>At each step we want to make all of the remaining choices result in either as many downs or as many up decisions as possible. We want to make that data more and more uniform so as we work our way down the flowchart, and we ultimately end up with a set of candidates that are either all UPS or all DOWNS so we can classify into yes/no decisions on a decision tree. So we just walk down the tree, minimize entropy at each step by choosing the right attribute to decide on, and we keep on going until we run out.</p><p>There’s a fancy name for this algorithm. It’s called ID3 ( Iterative Dichotomiser 3 ). It is what’s known as a greedy algorithm. So as it goes down the tree, it just picks the attribute that will minimize entropy at that point. Now that might not actually result in an optimal tree that minimizes the number of choices that you have to make, but it will result in a tree that works, given the data that you gave it.</p><p>By using Gini index or cross-entropy measure the data is split into nodex.</p><p>The Gini index is defined as:</p><p><span class="math display">\[G = \sum_{k=1}^K \hat{p}_{mk}(1 - \hat{p}_{mk})\]</span></p><p>and is also referred as a measure of node purity.</p><p>Cross-entropy is defined as:</p><p><span class="math display">\[D = -\sum_{k=1}^K \hat{p}_{mk}log\hat{p}_{mk}\]</span></p><p>Start at the top, or root of the tree. 57% of the stock movements will have an DOWN movement with 43% going in UPWARD direction. If the William % R rating was equal to or above 0.17, we look left, otherwise you move right. To the right, we see only 17% of values having WPR below 0.17 will have UP movement, so the overall terminal node ends with the bucket having UP stock movement and so on.</p></div><div id=evaluating-the-decision-tree-model class="section level1"><h1>Evaluating the Decision Tree Model</h1><p>The decision tree model can be evaluated by constructing the confusion matrix as shown below.</p><pre><code>       DOWN  UP
 DOWN  231  78
 UP     87 220
 </code></pre><p>When applied to test data, the decision tree model have accuracy of 73%.</p></div><div id=benefit-of-decision-tree-over-neural-network class="section level1"><h1>Benefit of Decision Tree over Neural network</h1><p>The benefit of using Decision trees over Neural Network are:</p><ol style=list-style-type:decimal><li><p>They are easy to program.</p></li><li><p>The top nodes in the tree will give the information about what data affects the prediction.</p></li><li><p>Trees are interpretable and provide visual representation of data.</p></li><li><p>Performs faster than Neural Networks after training.</p></li></ol></div><div id=issues-with-dt class="section level1"><h1>Issues with DT</h1><p>Now one problem with decision trees is that they are very prone to overfitting, so you can end up with a decision tree that works well for the data that we trained it on, but it might not be that great for actually predicting the correct classification for values that it hasn’t seen before. Decision trees are all about arriving at the right decision for the training data that we gave it, but maybe we didn’t really take into account the right attributes, maybe we didn’t give it enough of a representative sample of values to learn from. This can result in real problems.</p><p>So to combat this issue, we use a technique called random forests, where the idea is that we sample the data that we train on, in different ways, for multiple different decision trees. Each decision tree takes a different random sample from our set of training data and constructs a tree from it. Then each resulting tree can vote on the right result.</p><p>Now that technique of randomly resampling our data with the same model is a term called bootstrap aggregating, or bagging. This is a form of what we call ensemble learning. The basic idea is that we have multiple trees, a forest of trees, each uses a random subsample of the data that we have to train on. Then each of these trees can vote on the final result, and that will help us combat overfitting for a given set of training data.</p><p>The other thing random forests can do is actually restrict the number of attributes that it can choose, between at each stage, while it is trying to minimize the entropy as it goes. And we can randomly pick which attributes it can choose from at each level. So that also gives us more variation from tree to tree, and therefore we get more of a variety of algorithms that can compete with each other. They can all vote on the final result using slightly different approaches to arriving at the same answer.</p><p>So that’s how random forests work. Basically, it is a forest of decision trees where they are drawing from different samples and also different sets of attributes at each stage that it can choose between.</p></div><div class=blog-tags><a href=https://laxmikants.github.io/tags/statistics/>Statistics</a>&nbsp;
<a href=https://laxmikants.github.io/tags/regression/>Regression</a>&nbsp;
<a href=https://laxmikants.github.io/tags/machine-learning/>Machine Learning</a>&nbsp;
<a href=https://laxmikants.github.io/tags/random-forest/>Random Forest</a>&nbsp;</div><hr><section id=social-share><div class="list-inline footer-links"><div class=share-box aria-hidden=true><ul class=share><li><a href="//twitter.com/share?url=https%3a%2f%2flaxmikants.github.io%2fpost%2fdecision-trees-in-ml%2f&amp;text=Decision%20Trees&amp;via=" target=_blank title="Share on Twitter"><i class="fab fa-twitter"></i></a></li><li><a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2flaxmikants.github.io%2fpost%2fdecision-trees-in-ml%2f" target=_blank title="Share on Facebook"><i class="fab fa-facebook"></i></a></li><li><a href="//reddit.com/submit?url=https%3a%2f%2flaxmikants.github.io%2fpost%2fdecision-trees-in-ml%2f&amp;title=Decision%20Trees" target=_blank title="Share on Reddit"><i class="fab fa-reddit"></i></a></li><li><a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2flaxmikants.github.io%2fpost%2fdecision-trees-in-ml%2f&amp;title=Decision%20Trees" target=_blank title="Share on LinkedIn"><i class="fab fa-linkedin"></i></a></li><li><a href="//www.stumbleupon.com/submit?url=https%3a%2f%2flaxmikants.github.io%2fpost%2fdecision-trees-in-ml%2f&amp;title=Decision%20Trees" target=_blank title="Share on StumbleUpon"><i class="fab fa-stumbleupon"></i></a></li><li><a href="//www.pinterest.com/pin/create/button/?url=https%3a%2f%2flaxmikants.github.io%2fpost%2fdecision-trees-in-ml%2f&amp;description=Decision%20Trees" target=_blank title="Share on Pinterest"><i class="fab fa-pinterest"></i></a></li></ul></div></div></section><h4 class=see-also>See also</h4><ul><li><a href=https://laxmikants.github.io/post/logistic-regression-in-ml/>Logistic Regression in ML</a></li><li><a href=https://laxmikants.github.io/post/regression-in-ml/>Regression in ML</a></li><li><a href=https://laxmikants.github.io/post/proportion-test-in-r/>Proportion test in R</a></li></ul></article><ul class="pager blog-pager"><li class=previous><a href=https://laxmikants.github.io/post/logistic-regression-in-ml/ data-toggle=tooltip data-placement=top title="Logistic Regression in ML">&larr; Previous Post</a></li><li class=next><a href=https://laxmikants.github.io/post/time-series-data-analysis/ data-toggle=tooltip data-placement=top title="Time-Series Forecasting">Next Post &rarr;</a></li></ul><div class=disqus-comments><button id=show-comments class="btn btn-default" type=button>Show <span class=disqus-comment-count data-disqus-url=https://laxmikants.github.io/post/decision-trees-in-ml>comments</span></button><div id=disqus_thread></div><script type=text/javascript>var disqus_config=function(){this.page.url='https:\/\/laxmikants.github.io\/post\/decision-trees-in-ml';};</script></div></div></div></div><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center footer-links"><li><a href title=RSS><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i><i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="credits copyright text-muted">&nbsp;&bull;&nbsp;</p><p class="credits theme-by text-muted"><a href=http://gohugo.io>Hugo v0.53</a> powered &nbsp;&bull;&nbsp; Theme by <a href=http://deanattali.com/beautiful-jekyll/>Beautiful Jekyll</a> adapted to <a href=https://github.com/halogenica/beautifulhugo>Beautiful Hugo</a></p></div></div></div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.js integrity=sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/contrib/auto-render.min.js integrity=sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe crossorigin=anonymous></script><script src=https://code.jquery.com/jquery-1.12.4.min.js integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin=anonymous></script><script src=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js integrity=sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa crossorigin=anonymous></script><script src=https://laxmikants.github.io/js/main.js></script><script src=https://laxmikants.github.io/js/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><script>$(document).ready(function(){$("pre.chroma").css("padding","0");});</script><script>renderMathInElement(document.body);</script><script src=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js integrity=sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js integrity=sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q crossorigin=anonymous></script><script src=https://laxmikants.github.io/js/load-photoswipe.js></script><script type=text/javascript>$(function(){$('#show-comments').on('click',function(){var disqus_shortname='lsoni';(function(){var disqus=document.createElement('script');disqus.type='text/javascript';disqus.async=true;disqus.src='//'+disqus_shortname+'.disqus.com/embed.js';(document.getElementsByTagName('head')[0]||document.getElementsByTagName('body')[0]).appendChild(disqus);})();$(this).hide();});});</script><script id=dsq-count-scr src=//lsoni.disqus.com/count.js async></script></body></html>
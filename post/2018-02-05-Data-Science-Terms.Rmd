---
title: "Data Science Terminology"
author: Laxmi K Soni 
description: "Data Science Terminology"
slug: Data Science Terminology
date: 2018-02-05
lastmod: 2018-02-05
categories: ["Data Science"]
tags: ["Data Science","Statistical Population","Probability","False Positives","Statistical Inference"]
summary: Data Science Terminology
subtitle: Data Science Terminology
lua:
  image:
    url: "/img/cover.png"
    width: 800
    height: 600
  author: "Laxmi K Soni"
  output:
  html_document:
    df_print: kable
    toc: true
---

<!--more-->


```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

## Data Science Terminology

Based upon personal experience, research, and various industry experts' advice, someone delving
into the art of data science should take every opportunity to understand and gain experience as well as proficiency with the following list of common data science terms:

  
### Statistical population

Statistical population as a recordset (or a set of records). This set or group of records will be of similar items or events that are of interest to the data scientist for some experiment. 

If a sample of a population is chosen accurately, characteristics of the entire population (that the sample is drawn from) can be estimated from the corresponding characteristics of the sample.


### Probability

This is possible upcoming events and the likelihood of them actually occurring. 
This compares to a statistical thought process that involves analyzing the
frequency of past events in an attempt to explain or make sense of the observations. 

In addition, the data scientist will associate various individual events, studying the relationship of these events. How these different events relate to each other governs the methods and rules that will need to be followed when we're studying their probabilities.


### False Positives

The idea of false positives is a very important statistical (data science) concept. A false positive is a mistake or an errored result. That is, it is a scenario where the results of a process or experiment indicate a fulfilled or true condition when, in fact, the condition is not true (not fulfilled). This situation is also referred to by some data scientists as a false alarm and is most easily understood by considering the idea of a recordset or statistical population that is determined not only by the accuracy of the processing but by the characteristics of the sampled population. In other words, the data scientist has made errors during the statistical process, or the recordset is a population that does not have an appropriate sample (or characteristics) for what is being investigated. The idea of false positives closely relates to confusion matrix.

> sklearn.metrics.confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None)

### Statistical Inference

Given a sample from a population, Statistical inference makes estimates about population characteristics. This can be either point estimate or interval estimate.
It includes testing various hypotheses and deriving estimates.

> statsmodels in a python library for difference statistical models

### Regression

Regression is a process or method (selected by the data scientist as the best fit technique for the experiment at hand) used for determining the relationships among variables. 

More precisely, regression is the process that helps the data scientist comprehend how the typical value of the dependent variable (or criterion variable) changes when any one or more of the independent variables is varied while the other independent variables are held fixed


### Fitting

Fitting is the process of measuring how well a statistical model or process describes a data scientist's observations pertaining to a recordset or experiment. These measures will attempt to point out the discrepancy between observed values and probable values. The probable values of a model or process are known as a distribution or a probability distribution. Therefore, a probability distribution fitting (or distribution fitting) is when the data scientist fits a
probability distribution to a series of data concerning the repeated measurement of a variable phenomenon.

The object of a data scientist performing a distribution fitting is to predict the probability or to forecast the frequency of, the occurrence of the phenomenon at a certain interval.


### Categorical data

Categorical variables are measured on nominal or ordinal measurement scales, In many ways qualitative variables are similar to categorical variables as both use either nominal or ordinal measurement scales.

Sometimes an observer may decide to record his/her observation using categories. 
The type and number of categories depend upon the type of interaction and the observer’s choice about how to classify the observation. For example, passive/active (two categories); introvert/extrovert (two categories); always/sometimes/never (three categories); strongly agree/agree/uncertain/disagree/strongly disagree (five categories). 

> Categorical data is displayed graphically by bar charts and pie charts.

> Categorical data can take numerical values, but those numbers don’t have any mathematical meaning.

> Chi-square test of goodness of fit can be applied to categorical variables


### Classification

Statistical classification of data is the process of identifying which category (discussed in the previous section) a data point, observation, or variable should be grouped into. The data science process that carries out a classification process is known as a classifier.

Determining whether a book is fiction or non-fiction is a simple example
classification. An analysis of data about restaurants might lead to the classification of them among several genres.

> Accuracy metric is not a good idea for imbalanced class problems.

> Precision and recall metrics are good for imbalanced class problems.

> Linear Classifiers: Logistic Regression, Naive Bayes Classifier, Nearest Neighbor, Support Vector Machines,Decision Trees,Boosted Trees,Random Forest,Neural Networks

### Clustering

Clustering is the process of dividing up the data occurrences into groups or homogeneous subsets of the dataset, not a predetermined set of groups as in classification but groups identified by the execution of the data science process based upon similarities that it found among the occurrences.

Objects in the same group (a group is also referred to as a cluster) are found to be more analogous (in some sense or another) to each other than to those objects found in other groups (or found in other clusters). The process of clustering is found to be very common in exploratory data mining and is also a common technique for statistical data analysis

There are two standard clustering strategies: partitioning methods and hierarchical clustering. K-means clustering is well-known and commonly used partitioning algorithm.

Cluster analysis is popular in many fields:

> If you want to invest in housing market, you can use clustering for identifying groups of houses according to their type, value and location.

> In cancer research for classifying patients into subgroups according their gene expression profile. This can be useful for identifying the molecular profile of patients with good or bad prognostic, as well as for understanding the disease.

> In marketing for market segmentation by identifying subgroups of customers with similar profiles and who might be receptive to a particular form of advertising.


### Statistical comparison

A process of analysis to view the similarities or variances of two or more groups
or populations (or recordsets).

In statistics (data science), this process of comparing is a statistical technique to compare populations or recordsets. 

The four major ways of comparing means from data that is assumed to be normally distributed are:

1. Independent Samples T-Test. Use the independent samples t-test when you want to compare means for two data sets that are independent from each other. 


2. One sample T-Test. Choose this when you want to compare means between one data set and a specified constant (like the mean from a hypothetical normal distribution). 

3. Paired Samples T-Test. Use this test if you have one group tested at two different times. In other words, you have two measurements on the same item, person, or thing.The groups are “paired” because there intrinsic connections between them (i.e. they are not independent). This comparison of means is often used for groups of patients before treatment and after treatment, or for students tested before remediation and after remediation. 


4. One way Analysis of Variance (ANOVA). Although not really a test for comparison of means, ANOVA is the main option when you have more than two levels of independent variable. For example, if your independent variable was “brand of coffee” your levels might be Starbucks, Peets and Trader Joe’s. Use this test when you have a group of individuals randomly split into smaller groups and completing different tasks (like drinking different coffee).


If you have non-normal data (or if you don’t know what distribution your data comes from), you can’t use any of the above tests for comparison of means. You must use a non-parametric test (non-parametric basically means that you don’t know the distributions’s parameters) for ex. Mann-Whitney U test and Wilcoxon Signed Rank

### Coding

Coding or statistical coding is again a process that a data scientist will use to prepare data for analysis. In this process, both quantitative data values (such as income or years of education) and qualitative data (such as race or gender) are categorized or coded in a consistent way.

Coding is performed by a data scientist for various reasons such as follows:

- More effective for running statistical models 

- Computers understand the variables

- Accountability--so the data scientist can run models blind, or without knowing what variables stand for, to reduce programming bias


### Distributions

The distribution of a statistical recordset (or of a population) is a visualization showing all the possible values (or sometimes referred to as intervals) of the data and how often they occur. When a distribution of categorical data is created by a data scientist, it attempts to show the number or percentage of individuals in each group or category.

It is a visualization showing the probability of occurrence of different possible outcomes in an experiment.

Types of Distributions:

1. Bernoulli Distribution
2. Uniform Distribution
3. Binomial Distribution
4. Normal Distribution
5. Poisson Distribution
6. Exponential Distribution


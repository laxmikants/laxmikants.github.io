---
title: "Data Science Terminology"
author: Laxmi K Soni 
description: "Data Science Terminology"
slug: Data Science Terminology
date: 2018-02-05
lastmod: 2018-02-05
categories: ["Posts and Resources on Data Science | Laxmikant Soni", " Statistical population", "Probability", "False positives", "Statistical inference", "Regression", "Fitting", "Categorical data", "Classification", "Clustering", "Statistical comparison", "CodingDistributions", "Data mining", "Decision trees", "Machine learning", "Munging and wrangling", "Visualization", "D3", "Regularization", "Assessment", "Cross-validation", "Neural networks", "Boosting", "Lift", "Mode", "Outlier", "Predictive modeling", "Big data", "Confidence interval", "Python", " R", " Jupyter Notebook", " Tensorflow", " Javascript", " ReactJS", " NodeJS", " Posts and Resources on Data Science", " Data Science", " Hadoop", " Java", " Spring", " Hibernate", " Struts", " MySQL", " Oracle", " DB2", " Websphere", " Weblogic","AWS"]
tags: ["Data Science","Statistical Population","Probability","False Positives","Statistical Inference"]
summary: Data Science Terminology
subtitle: Data Science Terminology
lua:
  image:
    url: "/img/cover.png"
    width: 800
    height: 600
  author: "Laxmi K Soni"
  output:
  html_document:
    df_print: kable
    toc: true
---

<!--more-->


```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

## Data Science Terminology

Based upon personal experience, research, and various industry experts' advice, someone delving
into the art of data science should take every opportunity to understand and gain experience as well as proficiency with the following list of common data science terms:

  
### Statistical population

Statistical population as a recordset (or a set of records). This set or group of records will be of similar items or events that are of interest to the data scientist for some experiment. 

If a sample of a population is chosen accurately, characteristics of the entire population (that the sample is drawn from) can be estimated from the corresponding characteristics of the sample.


### Probability

This is possible upcoming events and the likelihood of them actually occurring. 
This compares to a statistical thought process that involves analyzing the
frequency of past events in an attempt to explain or make sense of the observations. 

In addition, the data scientist will associate various individual events, studying the relationship of these events. How these different events relate to each other governs the methods and rules that will need to be followed when we're studying their probabilities.


### False Positives

The idea of false positives is a very important statistical (data science) concept. A false positive is a mistake or an errored result. That is, it is a scenario where the results of a process or experiment indicate a fulfilled or true condition when, in fact, the condition is not true (not fulfilled). This situation is also referred to by some data scientists as a false alarm and is most easily understood by considering the idea of a recordset or statistical population that is determined not only by the accuracy of the processing but by the characteristics of the sampled population. In other words, the data scientist has made errors during the statistical process, or the recordset is a population that does not have an appropriate sample (or characteristics) for what is being investigated. The idea of false positives closely relates to confusion matrix.

> sklearn.metrics.confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None)

### Statistical Inference

Given a sample from a population, Statistical inference makes estimates about population characteristics. This can be either point estimate or interval estimate.
It includes testing various hypotheses and deriving estimates.

> statsmodels in a python library for difference statistical models

### Regression

Regression is a process or method (selected by the data scientist as the best fit technique for the experiment at hand) used for determining the relationships among variables. 

More precisely, regression is the process that helps the data scientist comprehend how the typical value of the dependent variable (or criterion variable) changes when any one or more of the independent variables is varied while the other independent variables are held fixed


### Fitting

Fitting is the process of measuring how well a statistical model or process describes a data scientist's observations pertaining to a recordset or experiment. These measures will attempt to point out the discrepancy between observed values and probable values. The probable values of a model or process are known as a distribution or a probability distribution. Therefore, a probability distribution fitting (or distribution fitting) is when the data scientist fits a
probability distribution to a series of data concerning the repeated measurement of a variable phenomenon.

The object of a data scientist performing a distribution fitting is to predict the probability or to forecast the frequency of, the occurrence of the phenomenon at a certain interval.


### Categorical data

Categorical variables are measured on nominal or ordinal measurement scales, In many ways qualitative variables are similar to categorical variables as both use either nominal or ordinal measurement scales.

Sometimes an observer may decide to record his/her observation using categories. 
The type and number of categories depend upon the type of interaction and the observer’s choice about how to classify the observation. For example, passive/active (two categories); introvert/extrovert (two categories); always/sometimes/never (three categories); strongly agree/agree/uncertain/disagree/strongly disagree (five categories). 

> Categorical data is displayed graphically by bar charts and pie charts.

> Categorical data can take numerical values, but those numbers don’t have any mathematical meaning.

> Chi-square test of goodness of fit can be applied to categorical variables


### Classification

Statistical classification of data is the process of identifying which category (discussed in the previous section) a data point, observation, or variable should be grouped into. The data science process that carries out a classification process is known as a classifier. Determining whether a book is fiction or non-fiction is a simple example
classification. An analysis of data about restaurants might lead to the classification of them among several genres.

Classification is a very broad (and very popular) category of supervised learning algorithms,with the goal of trying to identify a data point as belonging to some classification (spam or ham; male or female; animal, mineral or vegetable, and so on). 
A multitude of algorithms for classification exists, including:

- k-nearest neighbor
- Logistic regression
- Naive Bayes classifier
- Support Vector Machines
- neural networks
- Decision trees
- Random forests


> Accuracy metric is not a good idea for imbalanced class problems.

> Precision and recall metrics are good for imbalanced class problems.

> Linear Classifiers: Logistic Regression, Naive Bayes Classifier, Nearest Neighbor, Support Vector Machines,Decision Trees,Boosted Trees,Random Forest,Neural Networks

### Clustering

Clustering is the process of dividing up the data occurrences into groups or homogeneous subsets of the dataset, not a predetermined set of groups as in classification but groups identified by the execution of the data science process based upon similarities that it found among the occurrences.

Objects in the same group (a group is also referred to as a cluster) are found to be more analogous (in some sense or another) to each other than to those objects found in other groups (or found in other clusters). The process of clustering is found to be very common in exploratory data mining and is also a common technique for statistical data analysis

There are two standard clustering strategies: partitioning methods and hierarchical clustering. K-means clustering is well-known and commonly used partitioning algorithm.

Cluster analysis is popular in many fields:

> If you want to invest in housing market, you can use clustering for identifying groups of houses according to their type, value and location.

> In cancer research for classifying patients into subgroups according their gene expression profile. This can be useful for identifying the molecular profile of patients with good or bad prognostic, as well as for understanding the disease.

> In marketing for market segmentation by identifying subgroups of customers with similar profiles and who might be receptive to a particular form of advertising.


### Statistical comparison

A process of analysis to view the similarities or variances of two or more groups
or populations (or recordsets).

In statistics (data science), this process of comparing is a statistical technique to compare populations or recordsets. 

The four major ways of comparing means from data that is assumed to be normally distributed are:

1. Independent Samples T-Test. Use the independent samples t-test when you want to compare means for two data sets that are independent from each other. 


2. One sample T-Test. Choose this when you want to compare means between one data set and a specified constant (like the mean from a hypothetical normal distribution). 

3. Paired Samples T-Test. Use this test if you have one group tested at two different times. In other words, you have two measurements on the same item, person, or thing.The groups are “paired” because there intrinsic connections between them (i.e. they are not independent). This comparison of means is often used for groups of patients before treatment and after treatment, or for students tested before remediation and after remediation. 


4. One way Analysis of Variance (ANOVA). Although not really a test for comparison of means, ANOVA is the main option when you have more than two levels of independent variable. For example, if your independent variable was “brand of coffee” your levels might be Starbucks, Peets and Trader Joe’s. Use this test when you have a group of individuals randomly split into smaller groups and completing different tasks (like drinking different coffee).


If you have non-normal data (or if you don’t know what distribution your data comes from), you can’t use any of the above tests for comparison of means. You must use a non-parametric test (non-parametric basically means that you don’t know the distributions’s parameters) for ex. Mann-Whitney U test and Wilcoxon Signed Rank

### Coding

Coding or statistical coding is again a process that a data scientist will use to prepare data for analysis. In this process, both quantitative data values (such as income or years of education) and qualitative data (such as race or gender) are categorized or coded in a consistent way.

Coding is performed by a data scientist for various reasons such as follows:

- More effective for running statistical models 

- Computers understand the variables

- Accountability--so the data scientist can run models blind, or without knowing what variables stand for, to reduce programming bias


### Distributions

The distribution of a statistical recordset (or of a population) is a visualization showing all the possible values (or sometimes referred to as intervals) of the data and how often they occur. When a distribution of categorical data is created by a data scientist, it attempts to show the number or percentage of individuals in each group or category.

It is a visualization showing the probability of occurrence of different possible outcomes in an experiment.

Types of Distributions:

1. Bernoulli Distribution
2. Uniform Distribution
3. Binomial Distribution
4. Normal Distribution
5. Poisson Distribution
6. Exponential Distribution

### Data mining

Data minin, usually concerned with  data relationships (or the potential relationships between points of data,
sometimes referred to as variables) and cognitive analysis.

To further define this term, we can mention that data mining is sometimes more simply referred to as
knowledge discovery or even just discovery, based upon processing through or analyzing data from
new or different viewpoints and summarizing it into valuable insights that can be used to increase
revenue, cuts costs, or both.

Using software dedicated to data mining is just one of several analytical approaches to data mining.
Although there are tools dedicated to this purpose (such as IBM Cognos BI and Planning Analytics,
Tableau, SAS, and so on.), data mining is all about the analysis process finding correlations or
patterns among dozens of fields in the data and that can be effectively accomplished using tools such
as MS Excel or any number of open source technologies.

### Decision trees

A statistical decision tree uses a diagram that looks like a tree. This structure attempts to represent
optional decision paths and a predicted outcome for each path selected. A data scientist will use a
decision tree to support, track, and model decision making and their possible consequences, including
chance event outcomes, resource costs, and utility. It is a common way to display the logic of a data
science process.


### Machine learning

Machine learning is one of the most intriguing and exciting areas of data science. It conjures all forms
of images around artificial intelligence which includes Neural Networks, Support Vector Machines
(SVMs), and so on.Fundamentally, we can describe the term machine learning as a method of training a computer to make
or improve predictions or behaviors based on data or, specifically, relationships within that data.
Continuing, machine learning is a process by which predictions are made based upon recognized
patterns identified within data, and additionally, it is the ability to continuously learn from the data's
patterns, therefore continuingly making better predictions.

It is not uncommon for someone to mistake the process of machine learning for data mining, but data
mining focuses more on exploratory data analysis and is known as unsupervised learning.

Machine learning can be used to learn and establish baseline behavioral profiles for various entities
and then to find meaningful anomalies. Here is the exciting part: the process of machine learning (using data relationships to make
predictions) is known as predictive analytics.

Predictive analytics allow the data scientists to produce reliable, repeatable decisions and results
and uncover hidden insights through learning from historical relationships and trends in the data.

### Munging and wrangling

The terms munging and wrangling are buzzwords or jargon meant to describe one's efforts to affect
the format of data, recordset, or file in some way in an effort to prepare the data for continued or
otherwise processing and/or evaluations.

### Dimensionality reduction

Dimensionality reduction is a family of techniques whose purpose is to convert data with a high number of dimensions into data with a lower number of dimensions. Used as a general term, this can mean either discarding dimensions entirely (such as feature selection), or to
create new individual dimensions that simultaneously represent multiple original dimensions,with some loss of resolution (such as feature extraction). Some algorithms that can be used for dimensionality reduction include:

- Various types of regressions
- PCA
- Image transformations (for example, converting an image to grayscale)
- Stemming and lemmatization (in natural language processing)

### Optimization

Optimization algorithms have the goal of selecting a set of parameters, or the values for a set of parameters, such that the cost or error of a system is minimized (alternatively, such that the reward of a system is maximized). Feature selection and feature extraction is actually a form of optimization; you are modifying parameters with the purpose of reducing dimensionality while preserving important data. In the most basic optimization technique, a brute-force search, you simply try every possible combination of parameters and select the
combination with the best results. In practice, most problems are complex enough that a brute-force search may take an unreasonable amount of time (that is, millions of years on a modern computer). Some optimization techniques include:

- A brute force search (also known as an exhaustive search)
- Gradient descent
- Simulated annealing
- Genetic algorithms

### Natural language processing

Natural language processing (NLP) is an entire field on its own and contains many techniques that are not considered in machine learning. However, NLP is often used in concert with ML algorithms, as the two fields combined are necessary to achieve generalized
artificial intelligence. Many ML classification algorithms operate on text rather than numbers (such as our spam filter), and in those situations, we rely on techniques from the field of NLP:stemming, in particular, is a quick and easy dimensionality reduction technique for text classifiers. Some NLP techniques relevant to ML include:

- Tokenization
- String distance
- Stemming or lemmatization
- TF-IDF

### Image processing

Like NLP, image processing is its own field of study that has overlapped with ML but is not fully encompassed by ML. As with NLP, we may often use image processing techniques to reduce dimensionality before applying an ML algorithm to an image. Some image processing
techniques relevant to machine learning include:

- Edge detection
- Scale invariant transformations
- Color space transformations
- Object detection
- Recurrent neural networks


<!doctype html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
  <style>

    .nocopy {
      -webkit-user-select: none;   
      -moz-user-select: none;      
      -ms-user-select: none;       
      user-select: none;           
    }  

</style>

<script data-ad-client="ca-pub-3804322353139756" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script async src = "https://www.googletagmanager.com/gtag/js?id=UA-155379268-1"></script>
<script>
 window.dataLayer = window.dataLayer || [];
 function gtag(){dataLayer.push(arguments);}
 gtag('js',new Date());
 
 gtag('config','UA-155379268-1');

</script>  
  <meta charset="utf-8">
<title>Classification - Data Science Posts and Resources | Laxmikant Soni</title>
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name=keywords content="Articles and Posts on Python, R, Data Science, Machine Learning and Analytics | Laxmikant Soni, Predictive Analytics, Business, Data, Analytics, Machine Learning, Mining, Python, Intelligence, Big, Modeling, Data Science, Integration, Visualization,Statistical population,Probability,False positives,Statistical inference,Regression,Fitting,Categorical data,Classification,Clustering,Statistical comparison,CodingDistributions,Data mining,Decision trees,Machine learning,Munging and wrangling,Visualization,D3,Regularization,Assessment,Cross-validation,Neural networks,Boosting,Lift,Mode,Outlier,Predictive modeling,Big data,Confidence interval,Python,R,Jupyter Notebook,Tensorflow,Javascript,ReactJS,NodeJS,Posts and Resources on Data Science,Data Science,Hadoop,Java,Spring,Hibernate,Struts,MySQL,Oracle,DB2,Websphere,Weblogic">


<meta name=robots content=index>


<meta name=description content="Articles and Posts on Python, R, Data Science, Machine Learning and Analytics | Laxmikant Soni">


<title>Laxmikant Soni</title>




<meta name="robots" content="index">




<meta name="generator" content="Hugo 0.74.3" /><meta itemprop="name" content="Classification">
<meta itemprop="description" content="Classifcation models a function to predict a specific outcome out of mulitple possible outcomes.">
<meta itemprop="datePublished" content="2020-03-17T00:00:00+00:00" />
<meta itemprop="dateModified" content="2020-03-17T00:00:00+00:00" />
<meta itemprop="wordCount" content="1301">



<meta itemprop="keywords" content="Classification," />
<meta property="og:title" content="Classification" />
<meta property="og:description" content="Classifcation models a function to predict a specific outcome out of mulitple possible outcomes." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://laxmikants.github.io/blog/classification/" />
<meta property="article:published_time" content="2020-03-17T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-03-17T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Classification"/>
<meta name="twitter:description" content="Classifcation models a function to predict a specific outcome out of mulitple possible outcomes."/>
<meta name="twitter:site" content="@laxmikantsoni09"/>
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/monokai.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css">
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway:400,800,900|Source+Sans+Pro:400,700">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css">
        <script src="https://kit.fontawesome.com/be54eb011a.js" crossorigin="anonymous"></script>
      <script async   src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="/css/main.min.1f17d6560afe48677306aa7452f1c061799e35dbd1cc697cb7d6d22eed0b6b9b.css" integrity="sha256-HxfWVgr&#43;SGdzBqp0UvHAYXmeNdvRzGl8t9bSLu0La5s="><link rel="stylesheet" href="/css/add-on.css">

<title>Classification : Data Science Posts and Resources</title>

<meta property="og:title" content="Classification">
<meta property="og:site_name" content="Data Science Posts and Resources">
<meta property="og:url" content="https://laxmikants.github.io/blog/classification/">
<meta property="og:type" content="article">
<meta property="og:locale" content="en">
<meta property="og:description" content="Classifcation models a function to predict a specific outcome out of mulitple possible outcomes.">
<meta name="description" content="Classifcation models a function to predict a specific outcome out of mulitple possible outcomes.">
<meta property="og:updated_time" content="2020-03-17T00:00:00Z">
<meta property="fb:app_id" content="428818034507005">
<meta name="author" content="Laxmikant Soni">
<meta property="article:author" content="https://laxmikants.github.io">
<meta property="article:published_time" content="2020-03-17T00:00:00Z">
<meta property="article:modified_time" content="2020-03-17T00:00:00Z">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "Classification",
  "alternativeHeadline": "Classification",
  "url": "https://laxmikants.github.io/blog/classification/",
  "image": "https://laxmikants.github.io/",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://laxmikants.github.io/blog/classification/"
  },
  "description": "Classifcation models a function to predict a specific outcome out of mulitple possible outcomes.",
  "author": {
    "@type": "Person",
    "name": "Laxmikant Soni"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data Science Posts and Resources",
    "logo": {
      "@type": "ImageObject",
      "url": "https://laxmikants.github.io/"
    }
  },
  "datePublished": "2020-03-17T00:00:00Z",
  "dateModified": "2020-03-17T00:00:00Z",
  "articleBody": "\r\n\u003cscript src=\"/rmarkdown-libs/header-attrs/header-attrs.js\"\u003e\u003c/script\u003e\r\n\u003clink href=\"/rmarkdown-libs/anchor-sections/anchor-sections.css\" rel=\"stylesheet\" /\u003e\r\n\u003cscript src=\"/rmarkdown-libs/anchor-sections/anchor-sections.js\"\u003e\u003c/script\u003e\r\n\r\n\r\n\u003cdiv id=\"classification\" class=\"section level2\"\u003e\r\n\u003ch2\u003eClassification\u003c/h2\u003e\r\n\u003cp\u003eWith regression we now predicted specific output-values for certain given input-values. Sometimes, however, we are not trying to predict outputs but to categorize or classify our elements. For this, we use classification algorithms\u003c/p\u003e\r\n\u003cp\u003e\u003cimg src=\"/img/main/KNN.png\" /\u003e\u003c/p\u003e\r\n\u003cp\u003eIn the figure above, we see one specific kind of classification algorithm, namely the K-Nearest-Neighbor classifier. Here we already have a decent amount of classified elements. We then add a new one (represented by the stars) and try to predict its class by looking at its nearest neighbors.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"classification-algorithms\" class=\"section level2\"\u003e\r\n\u003ch2\u003eCLASSIFICATION ALGORITHMS\u003c/h2\u003e\r\n\u003cp\u003eThere are various different classification algorithms and they are often used for predicting medical data or other real life use-cases. For example, by providing a large amount of tumor samples, we can classify if a tumor is benign or malignant with a pretty high certainty.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"k-nearest-neighbors\" class=\"section level2\"\u003e\r\n\u003ch2\u003eK-NEAREST-NEIGHBORS\u003c/h2\u003e\r\n\u003cp\u003eAs already mentioned, by using the K-Nearest-Neighbors classifier, we assign the class of the new object, based on its nearest neighbors. The K specifies the amount of neighbors to look at. For example, we could say that we only want to look at the one neighbor who is nearest but we could also say that we want to factor in 100 neighbors.\r\nNotice that K shouldn’t be a multiple of the number of classes since it might cause conflicts when we have an equal amount of elements from one class as from the other.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"naive-bayes\" class=\"section level2\"\u003e\r\n\u003ch2\u003eNAIVE-BAYES\u003c/h2\u003e\r\n\u003cp\u003eThe Naive Bayes algorithm might be a bit confusing when you encounter it the first time. However, we are only going to discuss the basics and focus more on the implementation in Python later on.\u003c/p\u003e\r\n\u003ctable\u003e\r\n\u003cthead\u003e\r\n\u003ctr class=\"header\"\u003e\r\n\u003cth\u003eOutlook\u003c/th\u003e\r\n\u003cth\u003eTemperture\u003c/th\u003e\r\n\u003cth\u003eHumidity\u003c/th\u003e\r\n\u003cth\u003eWindy\u003c/th\u003e\r\n\u003cth\u003ePlay\u003c/th\u003e\r\n\u003c/tr\u003e\r\n\u003c/thead\u003e\r\n\u003ctbody\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd\u003eSunny\u003c/td\u003e\r\n\u003ctd\u003eHot\u003c/td\u003e\r\n\u003ctd\u003eHigh\u003c/td\u003e\r\n\u003ctd\u003eFALSE\u003c/td\u003e\r\n\u003ctd\u003eNo\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd\u003eSunny\u003c/td\u003e\r\n\u003ctd\u003eHot\u003c/td\u003e\r\n\u003ctd\u003eHigh\u003c/td\u003e\r\n\u003ctd\u003eTRUE\u003c/td\u003e\r\n\u003ctd\u003eNo\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd\u003eRainy\u003c/td\u003e\r\n\u003ctd\u003eMild\u003c/td\u003e\r\n\u003ctd\u003eHigh\u003c/td\u003e\r\n\u003ctd\u003eFALSE\u003c/td\u003e\r\n\u003ctd\u003eNo\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd\u003eRainy\u003c/td\u003e\r\n\u003ctd\u003eHot\u003c/td\u003e\r\n\u003ctd\u003eHigh\u003c/td\u003e\r\n\u003ctd\u003eTRUE\u003c/td\u003e\r\n\u003ctd\u003eNo\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd\u003eOvercast\u003c/td\u003e\r\n\u003ctd\u003eHot\u003c/td\u003e\r\n\u003ctd\u003eNormal\u003c/td\u003e\r\n\u003ctd\u003eTRUE\u003c/td\u003e\r\n\u003ctd\u003eYes\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd\u003eSunny\u003c/td\u003e\r\n\u003ctd\u003eHot\u003c/td\u003e\r\n\u003ctd\u003eNormal\u003c/td\u003e\r\n\u003ctd\u003eTRUE\u003c/td\u003e\r\n\u003ctd\u003eYes\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd\u003eSunny\u003c/td\u003e\r\n\u003ctd\u003eMild\u003c/td\u003e\r\n\u003ctd\u003eHigh\u003c/td\u003e\r\n\u003ctd\u003eTRUE\u003c/td\u003e\r\n\u003ctd\u003eYes\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd\u003eOvercast\u003c/td\u003e\r\n\u003ctd\u003eCold\u003c/td\u003e\r\n\u003ctd\u003eNormal\u003c/td\u003e\r\n\u003ctd\u003eTRUE\u003c/td\u003e\r\n\u003ctd\u003eNo\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003c/tbody\u003e\r\n\u003c/table\u003e\r\n\u003cp\u003eImagine that we have a table like the one above. We have four input values (which we would have to make numerical of course) and one label or output. The two classes are Yes and No and they indicate if we are going to play outside or not.\u003c/p\u003e\r\n\u003cp\u003eWhat Naive Bayes now does is to write down all the probabilities for the individual scenarios. So we would start by writing the general probability of playing and not playing. In this case, we only play three out of eight times and thus our probability of playing will be 3/8 and the probability of not playing will be 5/8.\u003c/p\u003e\r\n\u003cp\u003eAlso, out of the five times we had a high humidity we only played once, whereas out of the three times it was normal, we played twice. So our probability for playing when we have a high humidity is 1/5 and for playing when we have a medium humidity is 2/3. We go on like that and note all the probabilities we have in our table. To then get the classification for a new entry, we multiply the probabilities together and end up with a prediction.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"logistic-regression\" class=\"section level2\"\u003e\r\n\u003ch2\u003eLOGISTIC REGRESSION\u003c/h2\u003e\r\n\u003cp\u003eAnother popular classification algorithm is called logistic regression . Even though the name says regression , this is actually a classification algorithm. It looks at probabilities and determines how likely it is that a certain event happens (or a certain class is the right one), given the input data. This is done by plotting something similar to a logistic growth curve and splitting the data into two.\u003c/p\u003e\r\n\u003cp\u003e\u003cimg src=\"/img/main/LogisticRegg.png\" /\u003e\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"decision-trees\" class=\"section level2\"\u003e\r\n\u003ch2\u003eDECISION TREES\u003c/h2\u003e\r\n\u003cp\u003eWith decision tree classifiers, we construct a decision tree out of our training data and use it to predict the classes of new elements.\u003c/p\u003e\r\n\u003cp\u003e\u003cimg src=\"/img/main/dtreeweather.png\" /\u003e\u003c/p\u003e\r\n\u003cp\u003eSince we are not using a line (and thus our model is not linear), we are also preventing mistakes caused by outliers.\u003c/p\u003e\r\n\u003cp\u003eThis classification algorithm requires very little data preparation and it is also very easy to understand and visualize. On the other hand, it is very easy to be overfitting the model. Here, the model is very closely matched to the training data and thus has worse chances to make a correct prediction on new data.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"random-forest\" class=\"section level2\"\u003e\r\n\u003ch2\u003eRANDOM FOREST\u003c/h2\u003e\r\n\u003cp\u003eRndom forest classifier is based on decision trees. What it does is creating a forest of multiple decision trees. To classify a new object, all the various trees determine a class and the most frequent result gets chosen. This makes the result more accurate and it also prevents overfitting. It is also more suited to handle data sets with higher dimensions. On the other hand, since the generation of the forest is random , you have very little control over your model.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"loading-data\" class=\"section level2\"\u003e\r\n\u003ch2\u003eLOADING DATA\u003c/h2\u003e\r\n\u003cp\u003eNow let us get into the code. In this example, we will get our data directly from the sklearn module. For the program we need the following imports:\u003c/p\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003eimport numpy as np\r\nimport pandas as pd\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nfrom sklearn.datasets import load_breast_cancer \u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eAt the last import, we import a dataset containing data on breast cancer. Also notice that we are only importing the KNeighborsClassifier for now.\u003c/p\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003edata = load_breast_cancer()\r\nprint (data.feature_names)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e## [\u0026#39;mean radius\u0026#39; \u0026#39;mean texture\u0026#39; \u0026#39;mean perimeter\u0026#39; \u0026#39;mean area\u0026#39;\r\n##  \u0026#39;mean smoothness\u0026#39; \u0026#39;mean compactness\u0026#39; \u0026#39;mean concavity\u0026#39;\r\n##  \u0026#39;mean concave points\u0026#39; \u0026#39;mean symmetry\u0026#39; \u0026#39;mean fractal dimension\u0026#39;\r\n##  \u0026#39;radius error\u0026#39; \u0026#39;texture error\u0026#39; \u0026#39;perimeter error\u0026#39; \u0026#39;area error\u0026#39;\r\n##  \u0026#39;smoothness error\u0026#39; \u0026#39;compactness error\u0026#39; \u0026#39;concavity error\u0026#39;\r\n##  \u0026#39;concave points error\u0026#39; \u0026#39;symmetry error\u0026#39; \u0026#39;fractal dimension error\u0026#39;\r\n##  \u0026#39;worst radius\u0026#39; \u0026#39;worst texture\u0026#39; \u0026#39;worst perimeter\u0026#39; \u0026#39;worst area\u0026#39;\r\n##  \u0026#39;worst smoothness\u0026#39; \u0026#39;worst compactness\u0026#39; \u0026#39;worst concavity\u0026#39;\r\n##  \u0026#39;worst concave points\u0026#39; \u0026#39;worst symmetry\u0026#39; \u0026#39;worst fractal dimension\u0026#39;]\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003eprint (data.target_names)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e## [\u0026#39;malignant\u0026#39; \u0026#39;benign\u0026#39;]\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003etargets, we have two options in this dataset: malignant and benign .\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"preparing-data\" class=\"section level2\"\u003e\r\n\u003ch2\u003ePREPARING DATA\u003c/h2\u003e\r\n\u003cp\u003eAgain, we convert our data back into NumPy arrays and split them into training and test data.\u003c/p\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003eX = np.array(data.data)\r\nY = np.array(data.target)\r\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.1 )\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eThe data attribute refers to our features and the target attribute points to the classes or labels. We again choose a test size of ten percent.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"training-and-testing\" class=\"section level2\"\u003e\r\n\u003ch2\u003eTRAINING AND TESTING\u003c/h2\u003e\r\n\u003cp\u003eWe start by first defining our K-Nearest-Neighbors classifier and then training it.\u003c/p\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003eknn = KNeighborsClassifier( n_neighbors = 5 )\r\nknn.fit(X_train, Y_train)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e## KNeighborsClassifier()\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eThe n_neighbors parameter specifies how many neighbor points we want to consider. In this case, we take five. Then we test our model again for its accuracy.\u003c/p\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003eaccuracy = knn.score(X_test, Y_test)\r\nprint (accuracy)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e## 0.8947368421052632\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eWe get a pretty decent accuracy for such a complex task.\u003c/p\u003e\r\n\u003cp\u003e0.9649122807017544\u003c/p\u003e\r\n\u003cdiv id=\"the-best-algorithm\" class=\"section level3\"\u003e\r\n\u003ch3\u003eTHE BEST ALGORITHM\u003c/h3\u003e\r\n\u003cp\u003eNow let’s put all the classification algorithms that we’ve discussed up until now to use and see which one performs best.\u003c/p\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003efrom sklearn.neighbors import KNeighborsClassifier\r\nfrom sklearn.naive_bayes import GaussianNB\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.ensemble import RandomForestClassifier \u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003eclf1 = KNeighborsClassifier( n_neighbors = 5 )\r\nclf2 = GaussianNB()\r\nclf3 = LogisticRegression()\r\nclf4 = DecisionTreeClassifier()\r\nclf5 = RandomForestClassifier()\r\n\r\nclf1.fit(X_train, Y_train)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e## KNeighborsClassifier()\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003eclf2.fit(X_train, Y_train)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e## GaussianNB()\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003eclf3.fit(X_train, Y_train)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e## LogisticRegression()\r\n## \r\n## C:\\Users\\slaxm\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\r\n## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\r\n## \r\n## Increase the number of iterations (max_iter) or scale the data as shown in:\r\n##     https://scikit-learn.org/stable/modules/preprocessing.html\r\n## Please also refer to the documentation for alternative solver options:\r\n##     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\r\n##   extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003eclf4.fit(X_train, Y_train)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e## DecisionTreeClassifier()\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003eclf5.fit(X_train, Y_train)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e## RandomForestClassifier()\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003eprint (clf1.score(X_test, Y_test))\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e## 0.8947368421052632\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003eprint (clf2.score(X_test, Y_test))\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e## 0.9298245614035088\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003eprint (clf3.score(X_test, Y_test))\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e## 0.9473684210526315\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003eprint (clf4.score(X_test, Y_test))\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e## 0.9473684210526315\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003eprint (clf5.score(X_test, Y_test))\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e## 0.9824561403508771\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eWhen you run this program a couple of times, you will notice that we can’t really say which algorithm is the best. Every time we run this script, we will see different results, at least for this specific data set.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"predicting-labels\" class=\"section level3\"\u003e\r\n\u003ch3\u003ePREDICTING LABELS\u003c/h3\u003e\r\n\u003cp\u003eAgain, we can again make predictions for new, unknown data. The chance of success in the classification is even very high. We just need to pass an array of input values and use the predict function .\u003c/p\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003eX_new = np.array([[...]])\r\nY_new = clf.predict(X_new)\u003c/code\u003e\u003c/pre\u003e\r\n\u003c/div\u003e\r\n\u003c/div\u003e"
}
</script>

  

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-155379268-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>  
</head>

  <body>
    
      
    

<header id="site-header">
  <nav id="site-nav">
    <h1 class="nav-title">
      <a href="/" class="nav">
        
          Blog
        
      </a>
    </h1>
    <menu id="site-nav-menu" class="flyout-menu menu">
      
        
          
          <a href="/" class="nav link"><i class='fa fa-home'></i> Home</a>
        
      
        
          
          <a href="/blog/" class="nav link"><i class='far fa-newspaper'></i> Blog</a>
        
      
        
          
          <a href="/categories/" class="nav link"><i class='fas fa-sitemap'></i> Categories</a>
        
      
      <a href="#share-menu" class="nav link share-toggle"><i class="fas fa-share-alt">&nbsp;</i>Share</a>
      <a href="#search-input" class="nav link search-toggle"><i class="fas fa-search">&nbsp;</i>Search</a>
    </menu>
    <a href="#search-input" class="nav search-toggle"><i class="fas fa-search fa-2x">&nbsp;</i></a>
    <a href="#share-menu" class="nav share-toggle"><i class="fas fa-share-alt fa-2x">&nbsp;</i></a>
    <a href="#lang-menu" class="nav lang-toggle" lang="en">en</a>
    <a href="#site-nav" class="nav nav-toggle"><i class="fas fa-bars fa-2x"></i></a>
  </nav>
  <menu id="search" class="menu"><input id="search-input" class="search-input menu"></input><div id="search-results" class="search-results menu"></div></menu>
  <menu id="lang-menu" class="flyout-menu menu">
  <a href="#" lang="en" class="nav link active">English (en)</a>
  
    
      
    
  
</menu>

  
    <menu id="share-menu" class="flyout-menu menu">
      <h1>Share Post</h1>
      




  
    
    <a href="//twitter.com/share?text=Classification&amp;url=https%3a%2f%2flaxmikants.github.io%2fblog%2fclassification%2f" target="_blank" rel="noopener" class="nav share-btn twitter">
        <p>Twitter</p>
      </a>
  

  
      <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2flaxmikants.github.io%2fblog%2fclassification%2f" target="_blank" rel="noopener" class="nav share-btn facebook">
        <p>Facebook</p>
        </a>
  


    </menu>
  
</header>

    <div id="wrapper">
      <section id="site-intro" >
  
  <header>
    <h1>Data Science Posts and Resources</h1>
  </header>
  <main>
    
  </main>
  
    <footer>
      <ul class="socnet-icons">
        

        <li><a href="//github.com/laxmikants" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>











<li><a href="//linkedin.com/in/laxmikantsoni09" target="_blank" rel="noopener" title="LinkedIn" class="fab fa-linkedin"></a></li>




<li><a href="//facebook.com/laxmikantsoni09" target="_blank" rel="noopener" title="Facebook" class="fab fa-facebook"></a></li>










<li><a href="//twitter.com/laxmikantsoni09" target="_blank" rel="noopener" title="Twitter" class="fab fa-twitter"></a></li>













      </ul>
    </footer>
  
</section>

      <main id="site-main">
        
  <article class="post">
    <header>
  <div class="title">
    
      <h2><a href="/blog/classification/">Classification</a></h2>
    
    
      <p>Classifcation models a function to predict a specific outcome out of mulitple possible outcomes.</p>
    
  </div>
  <div class="meta">
    <time datetime="2020-03-17 00:00:00 &#43;0000 UTC">March 17, 2020</time>
    <p>Laxmi K Soni</p>
    <p>7-Minute Read</p>
  </div>
</header>

    <div id="socnet-share">
      




  
    
    <a href="//twitter.com/share?text=Classification&amp;url=https%3a%2f%2flaxmikants.github.io%2fblog%2fclassification%2f" target="_blank" rel="noopener" class="nav share-btn twitter">
        <p>Twitter</p>
      </a>
  

  
      <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2flaxmikants.github.io%2fblog%2fclassification%2f" target="_blank" rel="noopener" class="nav share-btn facebook">
        <p>Facebook</p>
        </a>
  


    </div>
    <div class="content">
      <a href="/blog/classification/" class="image" style="--bg-image: url('https://laxmikants.github.io/img/main/classification-17.jpg');">
    <img src="https://laxmikants.github.io/img/main/classification-17.jpg" alt="">
  </a>
      
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<div id="classification" class="section level2">
<h2>Classification</h2>
<p>With regression we now predicted specific output-values for certain given input-values. Sometimes, however, we are not trying to predict outputs but to categorize or classify our elements. For this, we use classification algorithms</p>
<p><img src="/img/main/KNN.png" /></p>
<p>In the figure above, we see one specific kind of classification algorithm, namely the K-Nearest-Neighbor classifier. Here we already have a decent amount of classified elements. We then add a new one (represented by the stars) and try to predict its class by looking at its nearest neighbors.</p>
</div>
<div id="classification-algorithms" class="section level2">
<h2>CLASSIFICATION ALGORITHMS</h2>
<p>There are various different classification algorithms and they are often used for predicting medical data or other real life use-cases. For example, by providing a large amount of tumor samples, we can classify if a tumor is benign or malignant with a pretty high certainty.</p>
</div>
<div id="k-nearest-neighbors" class="section level2">
<h2>K-NEAREST-NEIGHBORS</h2>
<p>As already mentioned, by using the K-Nearest-Neighbors classifier, we assign the class of the new object, based on its nearest neighbors. The K specifies the amount of neighbors to look at. For example, we could say that we only want to look at the one neighbor who is nearest but we could also say that we want to factor in 100 neighbors.
Notice that K shouldn’t be a multiple of the number of classes since it might cause conflicts when we have an equal amount of elements from one class as from the other.</p>
</div>
<div id="naive-bayes" class="section level2">
<h2>NAIVE-BAYES</h2>
<p>The Naive Bayes algorithm might be a bit confusing when you encounter it the first time. However, we are only going to discuss the basics and focus more on the implementation in Python later on.</p>
<table>
<thead>
<tr class="header">
<th>Outlook</th>
<th>Temperture</th>
<th>Humidity</th>
<th>Windy</th>
<th>Play</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sunny</td>
<td>Hot</td>
<td>High</td>
<td>FALSE</td>
<td>No</td>
</tr>
<tr class="even">
<td>Sunny</td>
<td>Hot</td>
<td>High</td>
<td>TRUE</td>
<td>No</td>
</tr>
<tr class="odd">
<td>Rainy</td>
<td>Mild</td>
<td>High</td>
<td>FALSE</td>
<td>No</td>
</tr>
<tr class="even">
<td>Rainy</td>
<td>Hot</td>
<td>High</td>
<td>TRUE</td>
<td>No</td>
</tr>
<tr class="odd">
<td>Overcast</td>
<td>Hot</td>
<td>Normal</td>
<td>TRUE</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>Sunny</td>
<td>Hot</td>
<td>Normal</td>
<td>TRUE</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td>Sunny</td>
<td>Mild</td>
<td>High</td>
<td>TRUE</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>Overcast</td>
<td>Cold</td>
<td>Normal</td>
<td>TRUE</td>
<td>No</td>
</tr>
</tbody>
</table>
<p>Imagine that we have a table like the one above. We have four input values (which we would have to make numerical of course) and one label or output. The two classes are Yes and No and they indicate if we are going to play outside or not.</p>
<p>What Naive Bayes now does is to write down all the probabilities for the individual scenarios. So we would start by writing the general probability of playing and not playing. In this case, we only play three out of eight times and thus our probability of playing will be 3/8 and the probability of not playing will be 5/8.</p>
<p>Also, out of the five times we had a high humidity we only played once, whereas out of the three times it was normal, we played twice. So our probability for playing when we have a high humidity is 1/5 and for playing when we have a medium humidity is 2/3. We go on like that and note all the probabilities we have in our table. To then get the classification for a new entry, we multiply the probabilities together and end up with a prediction.</p>
</div>
<div id="logistic-regression" class="section level2">
<h2>LOGISTIC REGRESSION</h2>
<p>Another popular classification algorithm is called logistic regression . Even though the name says regression , this is actually a classification algorithm. It looks at probabilities and determines how likely it is that a certain event happens (or a certain class is the right one), given the input data. This is done by plotting something similar to a logistic growth curve and splitting the data into two.</p>
<p><img src="/img/main/LogisticRegg.png" /></p>
</div>
<div id="decision-trees" class="section level2">
<h2>DECISION TREES</h2>
<p>With decision tree classifiers, we construct a decision tree out of our training data and use it to predict the classes of new elements.</p>
<p><img src="/img/main/dtreeweather.png" /></p>
<p>Since we are not using a line (and thus our model is not linear), we are also preventing mistakes caused by outliers.</p>
<p>This classification algorithm requires very little data preparation and it is also very easy to understand and visualize. On the other hand, it is very easy to be overfitting the model. Here, the model is very closely matched to the training data and thus has worse chances to make a correct prediction on new data.</p>
</div>
<div id="random-forest" class="section level2">
<h2>RANDOM FOREST</h2>
<p>Rndom forest classifier is based on decision trees. What it does is creating a forest of multiple decision trees. To classify a new object, all the various trees determine a class and the most frequent result gets chosen. This makes the result more accurate and it also prevents overfitting. It is also more suited to handle data sets with higher dimensions. On the other hand, since the generation of the forest is random , you have very little control over your model.</p>
</div>
<div id="loading-data" class="section level2">
<h2>LOADING DATA</h2>
<p>Now let us get into the code. In this example, we will get our data directly from the sklearn module. For the program we need the following imports:</p>
<pre class="python"><code>import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_breast_cancer </code></pre>
<p>At the last import, we import a dataset containing data on breast cancer. Also notice that we are only importing the KNeighborsClassifier for now.</p>
<pre class="python"><code>data = load_breast_cancer()
print (data.feature_names)</code></pre>
<pre><code>## [&#39;mean radius&#39; &#39;mean texture&#39; &#39;mean perimeter&#39; &#39;mean area&#39;
##  &#39;mean smoothness&#39; &#39;mean compactness&#39; &#39;mean concavity&#39;
##  &#39;mean concave points&#39; &#39;mean symmetry&#39; &#39;mean fractal dimension&#39;
##  &#39;radius error&#39; &#39;texture error&#39; &#39;perimeter error&#39; &#39;area error&#39;
##  &#39;smoothness error&#39; &#39;compactness error&#39; &#39;concavity error&#39;
##  &#39;concave points error&#39; &#39;symmetry error&#39; &#39;fractal dimension error&#39;
##  &#39;worst radius&#39; &#39;worst texture&#39; &#39;worst perimeter&#39; &#39;worst area&#39;
##  &#39;worst smoothness&#39; &#39;worst compactness&#39; &#39;worst concavity&#39;
##  &#39;worst concave points&#39; &#39;worst symmetry&#39; &#39;worst fractal dimension&#39;]</code></pre>
<pre class="python"><code>print (data.target_names)</code></pre>
<pre><code>## [&#39;malignant&#39; &#39;benign&#39;]</code></pre>
<p>targets, we have two options in this dataset: malignant and benign .</p>
</div>
<div id="preparing-data" class="section level2">
<h2>PREPARING DATA</h2>
<p>Again, we convert our data back into NumPy arrays and split them into training and test data.</p>
<pre class="python"><code>X = np.array(data.data)
Y = np.array(data.target)
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.1 )</code></pre>
<p>The data attribute refers to our features and the target attribute points to the classes or labels. We again choose a test size of ten percent.</p>
</div>
<div id="training-and-testing" class="section level2">
<h2>TRAINING AND TESTING</h2>
<p>We start by first defining our K-Nearest-Neighbors classifier and then training it.</p>
<pre class="python"><code>knn = KNeighborsClassifier( n_neighbors = 5 )
knn.fit(X_train, Y_train)</code></pre>
<pre><code>## KNeighborsClassifier()</code></pre>
<p>The n_neighbors parameter specifies how many neighbor points we want to consider. In this case, we take five. Then we test our model again for its accuracy.</p>
<pre class="python"><code>accuracy = knn.score(X_test, Y_test)
print (accuracy)</code></pre>
<pre><code>## 0.8947368421052632</code></pre>
<p>We get a pretty decent accuracy for such a complex task.</p>
<p>0.9649122807017544</p>
<div id="the-best-algorithm" class="section level3">
<h3>THE BEST ALGORITHM</h3>
<p>Now let’s put all the classification algorithms that we’ve discussed up until now to use and see which one performs best.</p>
<pre class="python"><code>from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier </code></pre>
<pre class="python"><code>clf1 = KNeighborsClassifier( n_neighbors = 5 )
clf2 = GaussianNB()
clf3 = LogisticRegression()
clf4 = DecisionTreeClassifier()
clf5 = RandomForestClassifier()

clf1.fit(X_train, Y_train)</code></pre>
<pre><code>## KNeighborsClassifier()</code></pre>
<pre class="python"><code>clf2.fit(X_train, Y_train)</code></pre>
<pre><code>## GaussianNB()</code></pre>
<pre class="python"><code>clf3.fit(X_train, Y_train)</code></pre>
<pre><code>## LogisticRegression()
## 
## C:\Users\slaxm\AppData\Local\R-MINI~1\envs\R-RETI~1\lib\site-packages\sklearn\linear_model\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):
## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
## 
## Increase the number of iterations (max_iter) or scale the data as shown in:
##     https://scikit-learn.org/stable/modules/preprocessing.html
## Please also refer to the documentation for alternative solver options:
##     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
##   extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)</code></pre>
<pre class="python"><code>clf4.fit(X_train, Y_train)</code></pre>
<pre><code>## DecisionTreeClassifier()</code></pre>
<pre class="python"><code>clf5.fit(X_train, Y_train)</code></pre>
<pre><code>## RandomForestClassifier()</code></pre>
<pre class="python"><code>print (clf1.score(X_test, Y_test))</code></pre>
<pre><code>## 0.8947368421052632</code></pre>
<pre class="python"><code>print (clf2.score(X_test, Y_test))</code></pre>
<pre><code>## 0.9298245614035088</code></pre>
<pre class="python"><code>print (clf3.score(X_test, Y_test))</code></pre>
<pre><code>## 0.9473684210526315</code></pre>
<pre class="python"><code>print (clf4.score(X_test, Y_test))</code></pre>
<pre><code>## 0.9473684210526315</code></pre>
<pre class="python"><code>print (clf5.score(X_test, Y_test))</code></pre>
<pre><code>## 0.9824561403508771</code></pre>
<p>When you run this program a couple of times, you will notice that we can’t really say which algorithm is the best. Every time we run this script, we will see different results, at least for this specific data set.</p>
</div>
<div id="predicting-labels" class="section level3">
<h3>PREDICTING LABELS</h3>
<p>Again, we can again make predictions for new, unknown data. The chance of success in the classification is even very high. We just need to pass an array of input values and use the predict function .</p>
<pre class="python"><code>X_new = np.array([[...]])
Y_new = clf.predict(X_new)</code></pre>
</div>
</div>

    </div>
    <footer>
      <div class="stats">
  <ul class="categories">
    
      
        
          <li><a class="article-terms-link" href="/categories/classification/">Classification</a></li>
        
      
    
  </ul>
  <ul class="tags">
    
      
        
          <li><a class="article-terms-link" href="/tags/classification/">Classification</a></li>
        
      
    
  </ul>
</div>

    </footer>
  </article>
  
    


  <article class="post">
    
    <div>
      <img src="/img/main/logo.png" alt="img">
      <h2 id="say-something">Say Something</h2>
        <form id="comment-form" class="new-comment" method="POST">
          
          <h3 class='reply-notice hidden'>
            <span class='reply-name'></span>
          </h3>

          
          <input type="hidden" name="options[entryId]" value="d6b53757aa31703d682acb966e8edc3e">
          <input type='hidden' name='fields[replyThread]' value=''>
          <input type='hidden' name='fields[replyID]' value=''>
          <input type='hidden' name='fields[replyName]' value=''>

          
          <input required name='fields[name]' type='text' placeholder='Your Name'>
          <input name='fields[website]' type='text' placeholder='Your Website'>
          <input required name='fields[email]' type='email' placeholder='Your Email'>
          <textarea required name='fields[body]' placeholder='Your Message' rows='10'></textarea>

          
          

          
          <div class='submit-notice'>
            <strong class='submit-notice-text submit-success hidden'>Thanks for your comment! It will be shown on the site once it has been approved.</strong>
            <strong class='submit-notice-text submit-failed hidden'>Sorry, there was an error with your submission. Please make sure all required fields have been completed and try again.</strong>
          </div>

          
          <input type='submit' value='Submit' class='button'>
          <input type='submit' value='Submitted' class='hidden button' disabled>
          <input type='reset' value='Reset' class='button'>
        </form>
    </div>

    
    <div>
      <h2>Comments</h2><p>Nothing yet.</p>
      
    </div>
  </article>


  
  <div class="pagination">
    
      <a href="/blog/logistic-regression/" class="button left"><span>Logistic Regression in ML</span></a>
    
    
      <a href="/blog/student-grade-prediction/" class="button right"><span>Student Grade Prediction</span></a>
    
  </div>

      </main>
      <section id="site-sidebar">
  
    <section id="recent-posts">
      <header>
        <h1>Recent Posts</h1>
      </header>
      
      <article class="mini-post">
          <a href="/blog/neural-network-using-make-moons-dataset/" class="image" style="--bg-image: url('https://laxmikants.github.io/img/main/nn_makemoons_dataset.jpg');">
    <img src="https://laxmikants.github.io/img/main/nn_makemoons_dataset.jpg" alt="">
  </a>
        <header>
          <h2><a href="/blog/neural-network-using-make-moons-dataset/">Neural Network using Make Moons dataset</a></h2>
          <time class="published" datetime="2020-12-10 00:00:00 &#43;0000 UTC">December 10, 2020</time>
        </header>
      </article>
      
      <article class="mini-post">
          <a href="/blog/single-layer-perceptron/" class="image" style="--bg-image: url('https://laxmikants.github.io/img/main/Single_Layer_Perceptron.jpg');">
    <img src="https://laxmikants.github.io/img/main/Single_Layer_Perceptron.jpg" alt="">
  </a>
        <header>
          <h2><a href="/blog/single-layer-perceptron/">Single Layer Perceptron</a></h2>
          <time class="published" datetime="2020-12-10 00:00:00 &#43;0000 UTC">December 10, 2020</time>
        </header>
      </article>
      
      <article class="mini-post">
          <a href="/blog/neural-networks/" class="image" style="--bg-image: url('https://laxmikants.github.io/img/main/Neural-Networks-32.jpg');">
    <img src="https://laxmikants.github.io/img/main/Neural-Networks-32.jpg" alt="">
  </a>
        <header>
          <h2><a href="/blog/neural-networks/">Neural Networks</a></h2>
          <time class="published" datetime="2020-12-05 00:00:00 &#43;0000 UTC">December 5, 2020</time>
        </header>
      </article>
      
      
        <footer>
          <a href="/blog/" class="button">See More</a>
        </footer>
      
    </section>
  

  
    
      <section id="categories">
        <header>
          <h1><a href="/categories">categories</a></h1>
        </header>
        <ul>
          
          
          <li>
              <a href="/categories/python/">python<span class="count">14</span></a>
          
          <li>
              <a href="/categories/linear-regression/">linear-regression<span class="count">2</span></a>
          
          <li>
              <a href="/categories/logistic-regression/">logistic-regression<span class="count">2</span></a>
          
          <li>
              <a href="/categories/big-data/">big-data<span class="count">1</span></a>
          
          <li>
              <a href="/categories/bigdata/">bigdata<span class="count">1</span></a>
          
          <li>
              <a href="/categories/classification/">classification<span class="count">1</span></a>
          
          <li>
              <a href="/categories/clustering/">clustering<span class="count">1</span></a>
          
          <li>
              <a href="/categories/data-science/">data-science<span class="count">1</span></a>
          
          <li>
              <a href="/categories/decision-trees/">decision-trees<span class="count">1</span></a>
          
          <li>
              <a href="/categories/eda/">eda<span class="count">1</span></a>
          
          <li>
              <a href="/categories/exploring-dataset/">exploring-dataset<span class="count">1</span></a>
          
          <li>
              <a href="/categories/frequency-tables/">frequency-tables<span class="count">1</span></a>
          
          <li>
              <a href="/categories/hadoop/">hadoop<span class="count">1</span></a>
          
          <li>
              <a href="/categories/k-nearest-neighbours/">k-nearest-neighbours<span class="count">1</span></a>
          
          <li>
              <a href="/categories/machine-learning/">machine-learning<span class="count">1</span></a>
          
          <li>
              <a href="/categories/natural-language-processing/">natural-language-processing<span class="count">1</span></a>
          
          <li>
              <a href="/categories/neural-network/">neural-network<span class="count">1</span></a>
          
          <li>
              <a href="/categories/neural-networks/">neural-networks<span class="count">1</span></a>
          
          <li>
              <a href="/categories/numeric-data/">numeric-data<span class="count">1</span></a>
          
          <li>
              <a href="/categories/pandas/">pandas<span class="count">1</span></a>
          
          <li>
              <a href="/categories/react-native/">react-native<span class="count">1</span></a>
          
          <li>
              <a href="/categories/reactjs/">reactjs<span class="count">1</span></a>
          
          <li>
              <a href="/categories/single-layer-perceptron/">single-layer-perceptron<span class="count">1</span></a>
          
          <li>
              <a href="/categories/stock-market/">stock-market<span class="count">1</span></a>
          
          <li>
              <a href="/categories/support-vector-machines/">support-vector-machines<span class="count">1</span></a>
          
          <li>
              <a href="/categories/text-analytics/">text-analytics<span class="count">1</span></a>
          
          <li>
              <a href="/categories/time-series/">time-series<span class="count">1</span></a>
          
          <li>
              <a href="/categories/web-scrapping/">web-scrapping<span class="count">1</span></a>
          
          </li>
        </ul>
      </section>
    
  

  
</section>

      <footer id="site-footer">
  
      <ul class="socnet-icons">
        

        <li><a href="//github.com/laxmikants" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>











<li><a href="//linkedin.com/in/laxmikantsoni09" target="_blank" rel="noopener" title="LinkedIn" class="fab fa-linkedin"></a></li>




<li><a href="//facebook.com/laxmikantsoni09" target="_blank" rel="noopener" title="Facebook" class="fab fa-facebook"></a></li>










<li><a href="//twitter.com/laxmikantsoni09" target="_blank" rel="noopener" title="Twitter" class="fab fa-twitter"></a></li>













      </ul>
  
  <p class="copyright">
    © 2020 Data Science Posts and Resources
      <br>
  </p>
</footer>
<a id="back-to-top" href="#" class="fas fa-arrow-up fa-2x"></a>

      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/highlight.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/python.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script><script src="//code.jquery.com/jquery-3.5.1.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.js"></script>
    <script src="//unpkg.com/lunr/lunr.js"></script><script src="/js/bundlecdn.min.f2add8c14b1f0b4ba32b812ada31b9a7ae32730960754ccf519f6d49f5da77a1.js" integrity="sha256-8q3YwUsfC0ujK4Eq2jG5p64ycwlgdUzPUZ9tSfXad6E="></script>
    <script src="/js/add-on.js"></script>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-155379268-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

    </div>
  </body>
</html>

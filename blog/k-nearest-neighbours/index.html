<!doctype html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
  <style>

    .nocopy {
      -webkit-user-select: none;   
      -moz-user-select: none;      
      -ms-user-select: none;       
      user-select: none;           
    }  

</style>

<script data-ad-client="ca-pub-3804322353139756" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script async src = "https://www.googletagmanager.com/gtag/js?id=UA-155379268-1"></script>
<script>
 window.dataLayer = window.dataLayer || [];
 function gtag(){dataLayer.push(arguments);}
 gtag('js',new Date());
 
 gtag('config','UA-155379268-1');

</script>  
  <meta charset="utf-8">
<title>K nearest Neighbours - Data Science Posts and Resources :: Laxmikant Soni</title>

<meta name="viewport" content="width=device-width" />

<meta name="google-site-verification" content="MeRcFEBEyWiTb3NfY4THWxbV_fx3rKOJnvr_Jk398wY" />

<meta name=keywords content="Articles and Posts on Python, R, Data Science, Machine Learning and Analytics | Laxmikant Soni, Predictive Analytics, Business, Data, Analytics, Machine Learning, Mining, Python, Intelligence, Big, Modeling, Data Science, Integration, Visualization,Statistical population,Probability,False positives,Statistical inference,Regression,Fitting,Categorical data,Classification,Clustering,Statistical comparison,CodingDistributions,Data mining,Decision trees,Machine learning,Munging and wrangling,Visualization,D3,Regularization,Assessment,Cross-validation,Neural networks,Boosting,Lift,Mode,Outlier,Predictive modeling,Big data,Confidence interval,Python,R,Jupyter Notebook,Tensorflow,Javascript,ReactJS,NodeJS,Posts and Resources on Data Science,Data Science,Hadoop,Java,Spring,Hibernate,Struts,MySQL,Oracle,DB2,Websphere,Weblogic">

<meta name=description content="Articles and Posts on Python, R, Data Science, Machine Learning and Analytics :: Laxmikant Soni">

<meta name="robots" content="index">


<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>




<meta name="generator" content="Hugo 0.80.0" /><meta itemprop="name" content="K nearest Neighbours">
<meta itemprop="description" content="K nearest Neighbours">
<meta itemprop="datePublished" content="2020-04-30T00:00:00+00:00" />
<meta itemprop="dateModified" content="2020-04-30T00:00:00+00:00" />
<meta itemprop="wordCount" content="1762">



<meta itemprop="keywords" content="K nearest Neighbours," />
<meta property="og:title" content="K nearest Neighbours" />
<meta property="og:description" content="K nearest Neighbours" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://laxmikants.github.io/blog/k-nearest-neighbours/" />
<meta property="article:published_time" content="2020-04-30T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-04-30T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="K nearest Neighbours"/>
<meta name="twitter:description" content="K nearest Neighbours"/>
<meta name="twitter:site" content="@laxmikantsoni09"/>
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/monokai.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css">
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway:400,800,900|Source+Sans+Pro:400,700">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css">
        <script src="https://kit.fontawesome.com/be54eb011a.js" crossorigin="anonymous"></script>
      <script async   src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="/css/main.min.aa4e8165f5b2a16460fcb21582ad412bed8e48e9c5dc49f3b412d1703be4d75d.css" integrity="sha256-qk6BZfWyoWRg/LIVgq1BK&#43;2OSOnF3EnztBLRcDvk110="><link rel="stylesheet" href="/css/add-on.css">
        <link rel="stylesheet" href="/css/main.css">

<title>K nearest Neighbours : Data Science Posts and Resources</title>

<meta property="og:title" content="K nearest Neighbours">
<meta property="og:site_name" content="Data Science Posts and Resources">
<meta property="og:url" content="https://laxmikants.github.io/blog/k-nearest-neighbours/">
<meta property="og:type" content="article">
<meta property="og:locale" content="en">
<meta property="og:description" content="K nearest Neighbours">
<meta name="description" content="K nearest Neighbours">
<meta property="og:updated_time" content="2020-04-30T00:00:00Z">
<meta property="fb:app_id" content="428818034507005">
<meta name="author" content="Laxmikant Soni">

<meta property="article:author" content="https://laxmikants.github.io">
<meta property="article:published_time" content="2020-04-30T00:00:00Z">
<meta property="article:modified_time" content="2020-04-30T00:00:00Z">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "K nearest Neighbours",
  "alternativeHeadline": "K nearest Neighbours",
  "name" : "Data Science Posts and Resources",
  "url": "https://laxmikants.github.io/blog/k-nearest-neighbours/",
  "image": "https://laxmikants.github.io/",
  "sameAs":
      [ "https://www.facebook.com/laxmikantsoni09",
        "https://instagram.com/laxmikantsoni09",
        "https://www.linkedin.com/in/laxmikantsoni09",
        "https://twitter.com/laxmikantsoni09",
        "https://github.com/laxmikants",
        "https://www.kaggle.com/laxmikantsoni"
    ],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://laxmikants.github.io/blog/k-nearest-neighbours/"
  },
  "description": "K nearest Neighbours",
  "author": {
    "@type": "Person",
    "name": "Laxmikant Soni"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data Science Posts and Resources",
    "logo": {
      "@type": "ImageObject",
      "url": "https://laxmikants.github.io/"
    }
  },
  "datePublished": "2020-04-30T00:00:00Z",
  "dateModified": "2020-04-30T00:00:00Z",
  "articleBody": "\r\n\u003cscript src=\"/rmarkdown-libs/header-attrs/header-attrs.js\"\u003e\u003c/script\u003e\r\n\u003clink href=\"/rmarkdown-libs/anchor-sections/anchor-sections.css\" rel=\"stylesheet\" /\u003e\r\n\u003cscript src=\"/rmarkdown-libs/anchor-sections/anchor-sections.js\"\u003e\u003c/script\u003e\r\n\r\n\r\n\u003cdiv id=\"k-nearest-neighbours\" class=\"section level2\"\u003e\r\n\u003ch2\u003eK nearest Neighbours\u003c/h2\u003e\r\n\u003cp\u003eIf there are several groups of labeled samples and the items present in the groups are homogeneous in nature.\r\nNow, assume that we have an unlabeled example which needs to be classified into one of those several labeled groups.\r\nHow do we do that? Using kNN Algorithm. k nearest neighbors is an algorithm that knows all the available cases and classify the new cases by a majority vote of its k neighbors. This algorithms segregates unlabeled data points into well defined groups. KNN classifies a categorical value using the majority votes of nearest neighbors.\u003c/p\u003e\r\n\u003cdiv id=\"inputoutputparameter\" class=\"section level4\"\u003e\r\n\u003ch4\u003eInput/Output/parameter\u003c/h4\u003e\r\n\u003cp\u003eThe input to a KNN classifier can be both quantitative and qualitative. The output from a KNN classifer are categorical values which typically are the class of the data.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"distance-metric\" class=\"section level4\"\u003e\r\n\u003ch4\u003eDistance metric\u003c/h4\u003e\r\n\u003cp\u003eA distance metric is used to define proximity between ant two data points. The examples are Euclidean distance, Manhattan distance or Hamming distance.\u003c/p\u003e\r\n\u003cp\u003eThe Euclidean distance is used to be applied to categorical data.\u003c/p\u003e\r\n\u003cp\u003eManhattan distance is used over the Euclidean distance when there is high dimensionality in the data.\r\nManhattan distance (or block distance) is appropriate when we have a discrete data set and the Euclidean distance is appropriate when we have continuous numerical variables.\u003c/p\u003e\r\n\u003cp\u003eBoth Euclidean and Manhattan distances are used in case of discrete/continuous variables, whereas hamming distance is used in case of categorical variable.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"example\" class=\"section level4\"\u003e\r\n\u003ch4\u003eExample\u003c/h4\u003e\r\n\u003cp\u003eLet us assume that we have a blind tasting experience, in which we need to classify what food we tasted as fruit, protein, or vegetable. Suppose that prior to eating the mystery item, we created a taste dataset in which we recorded two features of each ingredient: A measure from 1 to 10 on how crunchy the food is and second a score from 1 to 10 of how sweet the ingredient tastes. Then we labeled each ingredient as one of the three types of food: fruits,vegetables or proteins. We could have a table such as:\u003c/p\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003eimport pandas as pd\r\ningredient = [\u0026#39;apple\u0026#39;, \u0026#39;bacon\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;carrot\u0026#39;, \u0026#39;celery\u0026#39;, \u0026#39;cheese\u0026#39;,\u0026#39;grape\u0026#39;,\u0026#39;green bean\u0026#39;,\u0026#39;nuts\u0026#39;,\u0026#39;orange\u0026#39;]\r\nsweetness = [10,1,10,7,3,1,8,3,3,7]\r\ncrunchiness = [9,4,1,10,10,1,5,7,6,3]\r\nfood_type = [\u0026#39;fruit\u0026#39;,\u0026#39;protein\u0026#39;,\u0026#39;fruit\u0026#39;,\u0026#39;vegetable\u0026#39;,\u0026#39;vegetable\u0026#39;,\u0026#39;protein\u0026#39;,\u0026#39;fruit\u0026#39;,\u0026#39;vegetable\u0026#39;,\u0026#39;protein\u0026#39;,\u0026#39;fruit\u0026#39;]\r\ndf = pd.DataFrame(list(zip(ingredient,sweetness,crunchiness,food_type)), columns =[\u0026#39;ingredient\u0026#39;, \u0026#39;sweetness\u0026#39;,\u0026#39;crunchiness\u0026#39;,\u0026#39;food_type\u0026#39;])\r\nprint(df)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e##    ingredient  sweetness  crunchiness  food_type\r\n## 0       apple         10            9      fruit\r\n## 1       bacon          1            4    protein\r\n## 2      banana         10            1      fruit\r\n## 3      carrot          7           10  vegetable\r\n## 4      celery          3           10  vegetable\r\n## 5      cheese          1            1    protein\r\n## 6       grape          8            5      fruit\r\n## 7  green bean          3            7  vegetable\r\n## 8        nuts          3            6    protein\r\n## 9      orange          7            3      fruit\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eThe kNN algorithm treats the features as coordinates in a multidimensional feature space. As our dataset includes only two\r\nfeatures, the feature space is 2 dimensional, with the x dimension indicating the ingredient sweetness and the y dimension indicating the crunchiness.\u003c/p\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003eimport matplotlib.pyplot as plt\r\nx = sweetness\r\ny = crunchiness\r\nlabs = food_type\r\nplt.scatter(x, y)\r\nfor i, txt in enumerate(labs):\r\n    plt.annotate(txt, (x[i]+.25, y[i]), fontsize=12)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003e\u003cimg src=\"/img/main/fruitscatter.png\" /\u003e\u003c/p\u003e\r\n\u003cp\u003eSimilar types of food tend to be grouped closely together.As illustrated in the next figure, vegetables tend to be crunchy but not sweet, fruits tend to be sweet and either crunchy or not crunchy, while proteins tend to be neither crunchy nor sweet\u003c/p\u003e\r\n\u003cp\u003eSuppose that after constructing this dataset we need to find if tomato a fruit or a vegetable. We can use a nearest neighbor approach to determine which class is a better fit as shown in the following figure:\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"calculating-distance\" class=\"section level4\"\u003e\r\n\u003ch4\u003eCalculating distance:\u003c/h4\u003e\r\n\u003cp\u003eLocating an object nearest neighbors requires a distance function or a formula that will measure the similarity between two instances.\r\nTraditionally the kNN algorithm uses the Euclidian distance. Euclidian distance is specified by the following formula, where p\r\nand q are instances to be compared, each having n features\u003c/p\u003e\r\n\u003cp\u003eThe term p1 refers to the value of the first feature in p, while q1 refers to\r\nthe first feature of q.\u003c/p\u003e\r\n\u003cp\u003e\u003cspan class=\"math display\"\u003e\\[dist (p, q) = ‚àö(ùëù1 ‚àí ùëû1)^2 + (ùëù2 ‚àí ùëû2)^2 + ‚ãØ (ùëù^ n ‚àí ùëû^ùëõ)\\]\u003c/span\u003e\u003c/p\u003e\r\n\u003cp\u003eThe distance formula involves comparing the values of each feature. For example, to\r\ncalculate the distance between the tomato (sweetness = 6, crunchiness = 4), and the\r\ngreen bean (sweetness = 3, crunchiness = 7), we can use the formula as follows:\u003c/p\u003e\r\n\u003cp\u003e\u003cspan class=\"math display\"\u003e\\[dist(tomato, green bean) = \\sqrt(6-3)^2 + (4‚àí7)^2 = 4.2\\]\u003c/span\u003e\u003c/p\u003e\r\n\u003cp\u003eIn a similar vein, we can calculate the distance between the tomato and several of its\r\nclosest neighbors as follows:\u003c/p\u003e\r\n\u003ctable\u003e\r\n\u003cthead\u003e\r\n\u003ctr class=\"header\"\u003e\r\n\u003cth\u003eingradient\u003c/th\u003e\r\n\u003cth\u003esweetness\u003c/th\u003e\r\n\u003cth\u003ecrunchness\u003c/th\u003e\r\n\u003cth\u003efood type\u003c/th\u003e\r\n\u003cth\u003edistanct to the tomato\u003c/th\u003e\r\n\u003c/tr\u003e\r\n\u003c/thead\u003e\r\n\u003ctbody\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd\u003egrape\u003c/td\u003e\r\n\u003ctd\u003e8\u003c/td\u003e\r\n\u003ctd\u003e5\u003c/td\u003e\r\n\u003ctd\u003efruit\u003c/td\u003e\r\n\u003ctd\u003e\u003cspan class=\"math display\"\u003e\\[\\sqrt((6 - 8)^2 + (4 - 5)^2) = 2.2 \\]\u003c/span\u003e\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd\u003egreen bean\u003c/td\u003e\r\n\u003ctd\u003e3\u003c/td\u003e\r\n\u003ctd\u003e7\u003c/td\u003e\r\n\u003ctd\u003evegetable\u003c/td\u003e\r\n\u003ctd\u003e\u003cspan class=\"math display\"\u003e\\[\\sqrt((6 - 3)^2 + (4 - 7)^2) = 4.2 \\]\u003c/span\u003e\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd\u003enuts\u003c/td\u003e\r\n\u003ctd\u003e3\u003c/td\u003e\r\n\u003ctd\u003e6\u003c/td\u003e\r\n\u003ctd\u003eprotein\u003c/td\u003e\r\n\u003ctd\u003e\u003cspan class=\"math display\"\u003e\\[\\sqrt((6 - 3)^2 + (4 - 6)^2) = 3.6 \\]\u003c/span\u003e\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd\u003eorange\u003c/td\u003e\r\n\u003ctd\u003e7\u003c/td\u003e\r\n\u003ctd\u003e3\u003c/td\u003e\r\n\u003ctd\u003efruit\u003c/td\u003e\r\n\u003ctd\u003e\u003cspan class=\"math display\"\u003e\\[\\sqrt((6 - 7)^2 + (4 - 3)^2) = 1.4 \\]\u003c/span\u003e\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003c/tbody\u003e\r\n\u003c/table\u003e\r\n\u003cp\u003eTo classify the tomato as vegetable, fruit or protein, we‚Äôll begin by assigning the tomato the food type of its single nearest neighbor.\r\nThis is called 1NN because k =1. The orange is the nearest neighbor to the tomato, with a distance of 1.4. As orange is a fruit,\r\nthe 1NN algorithm would classify tomato as a fruit. If we use the kNN algorithm with k=3 instead, it performs a vote among the three nearest neighbors: orange, grape, and nuts.\u003c/p\u003e\r\n\u003cp\u003e\u003ccode\u003eApplying KNNClassifier:\u003c/code\u003e\u003c/p\u003e\r\n\u003cp\u003eThe features list consists of the sweetness and crunchiness values. The target which is a categorical value is\r\nencoded using LabelEncoder.\u003c/p\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003efrom sklearn.preprocessing import LabelEncoder as le\r\nfeatures = list(zip (sweetness, crunchiness))\r\nfood_type = [\u0026#39;fruit\u0026#39;,\u0026#39;protein\u0026#39;,\u0026#39;fruit\u0026#39;,\u0026#39;vegetable\u0026#39;,\u0026#39;vegetable\u0026#39;,\u0026#39;protein\u0026#39;,\u0026#39;fruit\u0026#39;,\u0026#39;vegetable\u0026#39;,\u0026#39;protein\u0026#39;,\u0026#39;fruit\u0026#39;]\r\ntarget = le.fit_transform(food_type)\r\ntarget_lables = food_type\r\ntarget_df = pd.DataFrame(food_type,target)\r\ntarget_df\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003e0:fruit,\r\n1:protein,\r\n2:vegetable\u003c/p\u003e\r\n\u003cp\u003e\u003ccode\u003eKNeighborsClassifier\u003c/code\u003e created with neighbors = 2. Inside the model we fit the data with the features and encoded target\u003c/p\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003emodel = KNeighborsClassifier(n_neighbors=2)\r\nmodel.fit(features,target)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eAfter that we using the model we predict the food_type for new data (‚ÄòOrange‚Äô,sweetness = 6, crunchines = 4)\u003c/p\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003epredicted= model.predict([[6,4]]) \r\nprint(predicted)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003e[0]\u003c/p\u003e\r\n\u003cp\u003eThe predicted class 0 belongs to the food type of ‚Äòfruit‚Äô.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"choosing-an-appropriate-k\" class=\"section level4\"\u003e\r\n\u003ch4\u003eChoosing an appropriate k\u003c/h4\u003e\r\n\u003cp\u003eDeciding how many neighbrs to be used for kNN determines how well the model will generalize to future data. The balance between\r\noverfitting or underfitting the training data is a problem called as the bias/variance tradeoff. Choosing greater k reduce the variance caused by noisy data, but it can bias the learner and can run the risk of ignoring small but important patterns.\u003c/p\u003e\r\n\u003cp\u003eSmaller k values cause complex decision boundaries that more carefully fit the training data and can overfit.\u003c/p\u003e\r\n\u003cp\u003eIn practice, choosing k-value depends on the complexity of the concept to be learned and the number of records in the training data.\u003c/p\u003e\r\n\u003cp\u003eGenerally, k is set between 3 and 10. One common practice is setting the k equal to the square root of the number of training example.\u003c/p\u003e\r\n\u003cp\u003eIn the food classifier, we can set it to 4 assuming that there are 15 examples in the training data set.\r\nAn alternative value is to test multiple values of k on different test datasets and select the one that delivers the best classification performance.\u003c/p\u003e\r\n\u003cp\u003eIn the example above because the majority class among these neighbors is fruit (2 out of 3 votes), the tomato again is classified as a fruit.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"algorithm\" class=\"section level4\"\u003e\r\n\u003ch4\u003eAlgorithm\u003c/h4\u003e\r\n\u003cp\u003eThe KNN classification is performed using the following four steps\u003c/p\u003e\r\n\u003cul\u003e\r\n\u003cli\u003e\u003cp\u003eCompute the distance metric between the test data point and all the labeled data points\u003c/p\u003e\u003c/li\u003e\r\n\u003cli\u003e\u003cp\u003eOrder the labeled data points in the increasing order of this distance metric\u003c/p\u003e\u003c/li\u003e\r\n\u003cli\u003e\u003cp\u003eSelect the top k labeled data points and look at the class labels\u003c/p\u003e\u003c/li\u003e\r\n\u003cli\u003e\u003cp\u003eFind the class label that the majority of these k labeled data points have and assign it to the test data point.\u003c/p\u003e\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003cp\u003eThere are various things like Parameter selection, Presence of noise, Feature selection and scaling, Curse of dimensionality we need to consider before applying KNN algorithm\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"k-fold-cross-validation\" class=\"section level4\"\u003e\r\n\u003ch4\u003eK fold cross validation\u003c/h4\u003e\r\n\u003cp\u003eIn order to evaluate the performance of KNN classifier there are different approaches.\r\nOne approach is separating the data have into a training and test set. but it can be difficult to decide what the ratio should be between the\r\nsets. Outliers and other factor will greatly influence the results of how the classifier is built and the\r\nreported accuracy.\u003c/p\u003e\r\n\u003cp\u003eAnother approach is called as k-fold cross validation. In this approach, the data is divided up in to k equally sized pieces. One of these pieces is saved for test data and the remaining k ‚àí 1 pieces are used for training. We then buiid the model k times, rotating the piece we use for testing. We can judge the performance of the classifier on its average accuracy and standard dev.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"k-fold-example\" class=\"section level4\"\u003e\r\n\u003ch4\u003ek fold example\u003c/h4\u003e\r\n\u003cp\u003eSplit the dataset into K equal partitions (or ‚Äúfolds‚Äù) therefore if k = 4 and dataset has 120 observations then each of the 4 folds will have 30 observations. After that make use of the fold 1 as the testing set and the union of the other folds as the training set. Therefore in this case we will have testing set = 30 observations (fold 1), Training set = 90 observations (folds 2-4). After that calculate the testing accuracy. There after by repeating above steps K times, using a different fold as the testing set each time. We will need to repeat the process 4 times. In 2nd iteration fold 2 will be testing set and union of 1,3,4 will be training set and so on..Finally use the mean testing accuracy as the estimate of out-of-sample accuracy.\u003c/p\u003e\r\n\u003cp\u003e\u003cimg src=\"/img/main/cross_validation_diagram.png\" /\u003e\u003c/p\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003e# simulate splitting a dataset of 25 observations into 5 folds\r\nfrom sklearn.model_selection import KFold\r\nimport numpy as np\r\nkf = KFold(n_splits=5, random_state=None, shuffle=False)\r\nVec = np.arange(0,25)\r\n# print the contents of each training and testing set\r\nprint(\u0026#39;{} {:^61} {}\u0026#39;.format(\u0026#39;Iteration\u0026#39;, \r\n                            \u0026#39;Training set observations\u0026#39;, \r\n                            \u0026#39;Testing set observations\u0026#39;))\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e## Iteration                   Training set observations                   Testing set observations\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre class=\"python\"\u003e\u003ccode\u003efor iteration, data in enumerate(kf.split(Vec), start=1):\r\n   print(\u0026#39;{:^9} {} {!s:^25}\u0026#39;.format(iteration, data[0], data[1]))\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e##     1     [ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]        [0 1 2 3 4]       \r\n##     2     [ 0  1  2  3  4 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]        [5 6 7 8 9]       \r\n##     3     [ 0  1  2  3  4  5  6  7  8  9 15 16 17 18 19 20 21 22 23 24]     [10 11 12 13 14]     \r\n##     4     [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 20 21 22 23 24]     [15 16 17 18 19]     \r\n##     5     [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]     [20 21 22 23 24]\u003c/code\u003e\u003c/pre\u003e\r\n\u003c/div\u003e\r\n\u003c/div\u003e"
}
</script>

  

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-155379268-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>  
</head>
  <body>
    
    
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T5KFS4C"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    
      
    


<header id="site-header">
  <nav id="site-nav">
    <h1 class="nav-title">
      <a href="/" class="nav">
        
          Blog
        
      </a>
    </h1>
    <menu id="site-nav-menu" class="flyout-menu menu">
      
        
          
          <a href="/" class="nav link"><i class='fa fa-home'></i> Home</a>
        
      
        
          
          <a href="/about/" class="nav link"><i class='far fa-id-card'></i> About</a>
        
      
        
          
          <a href="/blog/" class="nav link"><i class='far fa-newspaper'></i> Blog</a>
        
      
        
          
          <a href="/categories/" class="nav link"><i class='fas fa-sitemap'></i> Categories</a>
        
      
        
          
          <a href="/contact/" class="nav link"><i class='far fa-envelope'></i> Contact</a>
        
      
      <a href="#share-menu" class="nav link share-toggle"><i class="fas fa-share-alt">&nbsp;</i>Share</a>
      <a href="#search-input" class="nav link search-toggle"><i class="fas fa-search">&nbsp;</i>Search</a>
    </menu>
 
    <a href="#search-input" class="nav search-toggle"><i class="fas fa-search fa-2x">&nbsp;</i></a>
    <a href="#share-menu" class="nav share-toggle"><i class="fas fa-share-alt fa-2x">&nbsp;</i></a>
    <a href="#lang-menu" class="nav lang-toggle" lang="en">en</a>
    <a href="#site-nav" class="nav nav-toggle"><i class="fas fa-bars fa-2x"></i></a>
  </nav>
  
  
  <menu id="search" class="menu"><input id="search-input" class="search-input menu"></input><div id="search-results" class="search-results menu">

  </div></menu>
  
  <menu id="lang-menu" class="flyout-menu menu">
  <a href="#" lang="en" class="nav link active">English (en)</a>
  
    
      
    
  
</menu>

  
    <menu id="share-menu" class="flyout-menu menu">
      <h1>Share Post</h1>
      




  
    
    <a href="//twitter.com/share?text=K%20nearest%20Neighbours&amp;url=https%3a%2f%2flaxmikants.github.io%2fblog%2fk-nearest-neighbours%2f" target="_blank" rel="noopener" class="nav share-btn twitter">
        <p>Twitter</p>
      </a>
  

  
      <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2flaxmikants.github.io%2fblog%2fk-nearest-neighbours%2f" target="_blank" rel="noopener" class="nav share-btn facebook">
        <p>Facebook</p>
        </a>
  

  
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2flaxmikants.github.io%2fblog%2fk-nearest-neighbours%2f&amp;title=K%20nearest%20Neighbours" target="_blank" rel="noopener" class="nav share-btn linkedin">
            <p>LinkedIn</p>
          </a>
  


    </menu>
  
</header>

    <div id="wrapper">
      <section id="site-intro" >
  
  <header>
    <h1>Data Science Posts and Resources</h1>
  </header>
  <main>
    <p>Articles on Data Science</p>
  </main>
  
    <footer>
      <ul class="socnet-icons">
        

        <li><a href="//github.com/laxmikants" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>











<li><a href="//linkedin.com/in/laxmikantsoni09" target="_blank" rel="noopener" title="LinkedIn" class="fab fa-linkedin"></a></li>




<li><a href="//facebook.com/laxmikantsoni09" target="_blank" rel="noopener" title="Facebook" class="fab fa-facebook"></a></li>










<li><a href="//twitter.com/laxmikantsoni09" target="_blank" rel="noopener" title="Twitter" class="fab fa-twitter"></a></li>













      </ul>
    </footer>
  
</section>



      <main id="site-main">
        
  <article class="post">
    <header>
  <div class="title">
    
      <h2><a href="/blog/k-nearest-neighbours/">K nearest Neighbours</a></h2>
    
    
      <p>K nearest Neighbours</p>
    
  </div>
  <div class="meta">
    <time datetime="2020-04-30 00:00:00 &#43;0000 UTC">April 30, 2020</time>
    <p>Laxmi K Soni</p>
    <p>9-Minute Read</p>
  </div>
</header>

    <div id="socnet-share">
      




  
    
    <a href="//twitter.com/share?text=K%20nearest%20Neighbours&amp;url=https%3a%2f%2flaxmikants.github.io%2fblog%2fk-nearest-neighbours%2f" target="_blank" rel="noopener" class="nav share-btn twitter">
        <p>Twitter</p>
      </a>
  

  
      <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2flaxmikants.github.io%2fblog%2fk-nearest-neighbours%2f" target="_blank" rel="noopener" class="nav share-btn facebook">
        <p>Facebook</p>
        </a>
  

  
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2flaxmikants.github.io%2fblog%2fk-nearest-neighbours%2f&amp;title=K%20nearest%20Neighbours" target="_blank" rel="noopener" class="nav share-btn linkedin">
            <p>LinkedIn</p>
          </a>
  


    </div>
  
    <div class="content">
      <a href="/blog/k-nearest-neighbours/" class="image" style="--bg-image: url('https://laxmikants.github.io/img/main/knnclassifier.jpg');">
    <img src="https://laxmikants.github.io/img/main/knnclassifier.jpg" alt="">
  </a>
      
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<div id="k-nearest-neighbours" class="section level2">
<h2>K nearest Neighbours</h2>
<p>If there are several groups of labeled samples and the items present in the groups are homogeneous in nature.
Now, assume that we have an unlabeled example which needs to be classified into one of those several labeled groups.
How do we do that? Using kNN Algorithm. k nearest neighbors is an algorithm that knows all the available cases and classify the new cases by a majority vote of its k neighbors. This algorithms segregates unlabeled data points into well defined groups. KNN classifies a categorical value using the majority votes of nearest neighbors.</p>
<div id="inputoutputparameter" class="section level4">
<h4>Input/Output/parameter</h4>
<p>The input to a KNN classifier can be both quantitative and qualitative. The output from a KNN classifer are categorical values which typically are the class of the data.</p>
</div>
<div id="distance-metric" class="section level4">
<h4>Distance metric</h4>
<p>A distance metric is used to define proximity between ant two data points. The examples are Euclidean distance, Manhattan distance or Hamming distance.</p>
<p>The Euclidean distance is used to be applied to categorical data.</p>
<p>Manhattan distance is used over the Euclidean distance when there is high dimensionality in the data.
Manhattan distance (or block distance) is appropriate when we have a discrete data set and the Euclidean distance is appropriate when we have continuous numerical variables.</p>
<p>Both Euclidean and Manhattan distances are used in case of discrete/continuous variables, whereas hamming distance is used in case of categorical variable.</p>
</div>
<div id="example" class="section level4">
<h4>Example</h4>
<p>Let us assume that we have a blind tasting experience, in which we need to classify what food we tasted as fruit, protein, or vegetable. Suppose that prior to eating the mystery item, we created a taste dataset in which we recorded two features of each ingredient: A measure from 1 to 10 on how crunchy the food is and second a score from 1 to 10 of how sweet the ingredient tastes. Then we labeled each ingredient as one of the three types of food: fruits,vegetables or proteins. We could have a table such as:</p>
<pre class="python"><code>import pandas as pd
ingredient = [&#39;apple&#39;, &#39;bacon&#39;, &#39;banana&#39;, &#39;carrot&#39;, &#39;celery&#39;, &#39;cheese&#39;,&#39;grape&#39;,&#39;green bean&#39;,&#39;nuts&#39;,&#39;orange&#39;]
sweetness = [10,1,10,7,3,1,8,3,3,7]
crunchiness = [9,4,1,10,10,1,5,7,6,3]
food_type = [&#39;fruit&#39;,&#39;protein&#39;,&#39;fruit&#39;,&#39;vegetable&#39;,&#39;vegetable&#39;,&#39;protein&#39;,&#39;fruit&#39;,&#39;vegetable&#39;,&#39;protein&#39;,&#39;fruit&#39;]
df = pd.DataFrame(list(zip(ingredient,sweetness,crunchiness,food_type)), columns =[&#39;ingredient&#39;, &#39;sweetness&#39;,&#39;crunchiness&#39;,&#39;food_type&#39;])
print(df)</code></pre>
<pre><code>##    ingredient  sweetness  crunchiness  food_type
## 0       apple         10            9      fruit
## 1       bacon          1            4    protein
## 2      banana         10            1      fruit
## 3      carrot          7           10  vegetable
## 4      celery          3           10  vegetable
## 5      cheese          1            1    protein
## 6       grape          8            5      fruit
## 7  green bean          3            7  vegetable
## 8        nuts          3            6    protein
## 9      orange          7            3      fruit</code></pre>
<p>The kNN algorithm treats the features as coordinates in a multidimensional feature space. As our dataset includes only two
features, the feature space is 2 dimensional, with the x dimension indicating the ingredient sweetness and the y dimension indicating the crunchiness.</p>
<pre class="python"><code>import matplotlib.pyplot as plt
x = sweetness
y = crunchiness
labs = food_type
plt.scatter(x, y)
for i, txt in enumerate(labs):
    plt.annotate(txt, (x[i]+.25, y[i]), fontsize=12)</code></pre>
<p><img src="/img/main/fruitscatter.png" /></p>
<p>Similar types of food tend to be grouped closely together.As illustrated in the next figure, vegetables tend to be crunchy but not sweet, fruits tend to be sweet and either crunchy or not crunchy, while proteins tend to be neither crunchy nor sweet</p>
<p>Suppose that after constructing this dataset we need to find if tomato a fruit or a vegetable. We can use a nearest neighbor approach to determine which class is a better fit as shown in the following figure:</p>
</div>
<div id="calculating-distance" class="section level4">
<h4>Calculating distance:</h4>
<p>Locating an object nearest neighbors requires a distance function or a formula that will measure the similarity between two instances.
Traditionally the kNN algorithm uses the Euclidian distance. Euclidian distance is specified by the following formula, where p
and q are instances to be compared, each having n features</p>
<p>The term p1 refers to the value of the first feature in p, while q1 refers to
the first feature of q.</p>
<p><span class="math display">\[dist (p, q) = ‚àö(ùëù1 ‚àí ùëû1)^2 + (ùëù2 ‚àí ùëû2)^2 + ‚ãØ (ùëù^ n ‚àí ùëû^ùëõ)\]</span></p>
<p>The distance formula involves comparing the values of each feature. For example, to
calculate the distance between the tomato (sweetness = 6, crunchiness = 4), and the
green bean (sweetness = 3, crunchiness = 7), we can use the formula as follows:</p>
<p><span class="math display">\[dist(tomato, green bean) = \sqrt(6-3)^2 + (4‚àí7)^2 = 4.2\]</span></p>
<p>In a similar vein, we can calculate the distance between the tomato and several of its
closest neighbors as follows:</p>
<table>
<thead>
<tr class="header">
<th>ingradient</th>
<th>sweetness</th>
<th>crunchness</th>
<th>food type</th>
<th>distanct to the tomato</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>grape</td>
<td>8</td>
<td>5</td>
<td>fruit</td>
<td><span class="math display">\[\sqrt((6 - 8)^2 + (4 - 5)^2) = 2.2 \]</span></td>
</tr>
<tr class="even">
<td>green bean</td>
<td>3</td>
<td>7</td>
<td>vegetable</td>
<td><span class="math display">\[\sqrt((6 - 3)^2 + (4 - 7)^2) = 4.2 \]</span></td>
</tr>
<tr class="odd">
<td>nuts</td>
<td>3</td>
<td>6</td>
<td>protein</td>
<td><span class="math display">\[\sqrt((6 - 3)^2 + (4 - 6)^2) = 3.6 \]</span></td>
</tr>
<tr class="even">
<td>orange</td>
<td>7</td>
<td>3</td>
<td>fruit</td>
<td><span class="math display">\[\sqrt((6 - 7)^2 + (4 - 3)^2) = 1.4 \]</span></td>
</tr>
</tbody>
</table>
<p>To classify the tomato as vegetable, fruit or protein, we‚Äôll begin by assigning the tomato the food type of its single nearest neighbor.
This is called 1NN because k =1. The orange is the nearest neighbor to the tomato, with a distance of 1.4. As orange is a fruit,
the 1NN algorithm would classify tomato as a fruit. If we use the kNN algorithm with k=3 instead, it performs a vote among the three nearest neighbors: orange, grape, and nuts.</p>
<p><code>Applying KNNClassifier:</code></p>
<p>The features list consists of the sweetness and crunchiness values. The target which is a categorical value is
encoded using LabelEncoder.</p>
<pre class="python"><code>from sklearn.preprocessing import LabelEncoder as le
features = list(zip (sweetness, crunchiness))
food_type = [&#39;fruit&#39;,&#39;protein&#39;,&#39;fruit&#39;,&#39;vegetable&#39;,&#39;vegetable&#39;,&#39;protein&#39;,&#39;fruit&#39;,&#39;vegetable&#39;,&#39;protein&#39;,&#39;fruit&#39;]
target = le.fit_transform(food_type)
target_lables = food_type
target_df = pd.DataFrame(food_type,target)
target_df</code></pre>
<p>0:fruit,
1:protein,
2:vegetable</p>
<p><code>KNeighborsClassifier</code> created with neighbors = 2. Inside the model we fit the data with the features and encoded target</p>
<pre class="python"><code>model = KNeighborsClassifier(n_neighbors=2)
model.fit(features,target)</code></pre>
<p>After that we using the model we predict the food_type for new data (‚ÄòOrange‚Äô,sweetness = 6, crunchines = 4)</p>
<pre class="python"><code>predicted= model.predict([[6,4]]) 
print(predicted)</code></pre>
<p>[0]</p>
<p>The predicted class 0 belongs to the food type of ‚Äòfruit‚Äô.</p>
</div>
<div id="choosing-an-appropriate-k" class="section level4">
<h4>Choosing an appropriate k</h4>
<p>Deciding how many neighbrs to be used for kNN determines how well the model will generalize to future data. The balance between
overfitting or underfitting the training data is a problem called as the bias/variance tradeoff. Choosing greater k reduce the variance caused by noisy data, but it can bias the learner and can run the risk of ignoring small but important patterns.</p>
<p>Smaller k values cause complex decision boundaries that more carefully fit the training data and can overfit.</p>
<p>In practice, choosing k-value depends on the complexity of the concept to be learned and the number of records in the training data.</p>
<p>Generally, k is set between 3 and 10. One common practice is setting the k equal to the square root of the number of training example.</p>
<p>In the food classifier, we can set it to 4 assuming that there are 15 examples in the training data set.
An alternative value is to test multiple values of k on different test datasets and select the one that delivers the best classification performance.</p>
<p>In the example above because the majority class among these neighbors is fruit (2 out of 3 votes), the tomato again is classified as a fruit.</p>
</div>
<div id="algorithm" class="section level4">
<h4>Algorithm</h4>
<p>The KNN classification is performed using the following four steps</p>
<ul>
<li><p>Compute the distance metric between the test data point and all the labeled data points</p></li>
<li><p>Order the labeled data points in the increasing order of this distance metric</p></li>
<li><p>Select the top k labeled data points and look at the class labels</p></li>
<li><p>Find the class label that the majority of these k labeled data points have and assign it to the test data point.</p></li>
</ul>
<p>There are various things like Parameter selection, Presence of noise, Feature selection and scaling, Curse of dimensionality we need to consider before applying KNN algorithm</p>
</div>
<div id="k-fold-cross-validation" class="section level4">
<h4>K fold cross validation</h4>
<p>In order to evaluate the performance of KNN classifier there are different approaches.
One approach is separating the data have into a training and test set. but it can be difficult to decide what the ratio should be between the
sets. Outliers and other factor will greatly influence the results of how the classifier is built and the
reported accuracy.</p>
<p>Another approach is called as k-fold cross validation. In this approach, the data is divided up in to k equally sized pieces. One of these pieces is saved for test data and the remaining k ‚àí 1 pieces are used for training. We then buiid the model k times, rotating the piece we use for testing. We can judge the performance of the classifier on its average accuracy and standard dev.</p>
</div>
<div id="k-fold-example" class="section level4">
<h4>k fold example</h4>
<p>Split the dataset into K equal partitions (or ‚Äúfolds‚Äù) therefore if k = 4 and dataset has 120 observations then each of the 4 folds will have 30 observations. After that make use of the fold 1 as the testing set and the union of the other folds as the training set. Therefore in this case we will have testing set = 30 observations (fold 1), Training set = 90 observations (folds 2-4). After that calculate the testing accuracy. There after by repeating above steps K times, using a different fold as the testing set each time. We will need to repeat the process 4 times. In 2nd iteration fold 2 will be testing set and union of 1,3,4 will be training set and so on..Finally use the mean testing accuracy as the estimate of out-of-sample accuracy.</p>
<p><img src="/img/main/cross_validation_diagram.png" /></p>
<pre class="python"><code># simulate splitting a dataset of 25 observations into 5 folds
from sklearn.model_selection import KFold
import numpy as np
kf = KFold(n_splits=5, random_state=None, shuffle=False)
Vec = np.arange(0,25)
# print the contents of each training and testing set
print(&#39;{} {:^61} {}&#39;.format(&#39;Iteration&#39;, 
                            &#39;Training set observations&#39;, 
                            &#39;Testing set observations&#39;))</code></pre>
<pre><code>## Iteration                   Training set observations                   Testing set observations</code></pre>
<pre class="python"><code>for iteration, data in enumerate(kf.split(Vec), start=1):
   print(&#39;{:^9} {} {!s:^25}&#39;.format(iteration, data[0], data[1]))</code></pre>
<pre><code>##     1     [ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]        [0 1 2 3 4]       
##     2     [ 0  1  2  3  4 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]        [5 6 7 8 9]       
##     3     [ 0  1  2  3  4  5  6  7  8  9 15 16 17 18 19 20 21 22 23 24]     [10 11 12 13 14]     
##     4     [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 20 21 22 23 24]     [15 16 17 18 19]     
##     5     [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]     [20 21 22 23 24]</code></pre>
</div>
</div>

      <div align = "center">
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        
        <ins class="adsbygoogle"
         style="display:block"
         data-ad-client="ca-pub-3804322353139756"
         data-ad-slot="3387213493"
         data-ad-format="auto"
         data-full-width-responsive="true"></ins>
        <script>
            $(document).ready(function(){(adsbygoogle = window.adsbygoogle || []).push({})})
        </script>
    </div>            
    </div>
    <footer>
      <div class="stats">
  <ul class="categories">
    
      
        
          <li><a class="article-terms-link" href="/categories/k-nearest-neighbours/">K nearest Neighbours</a></li>
        
      
    
  </ul>
  <ul class="tags">
    
      
        
          <li><a class="article-terms-link" href="/tags/k-nearest-neighbours/">K nearest Neighbours</a></li>
        
      
    
  </ul>
</div>

    </footer>
  </article>
  
    

  <article class="post">
    
    <div>
      <h2 id="say-something">Say Something</h2>
        <form id="comment-form" class="new-comment" method="POST">
          
          <h3 class='reply-notice hidden'>
            <span class='reply-name'></span>
          </h3>

          
          <input type="hidden" name="options[entryId]" value="87a7eb59de7202d41458f6956f1d7c72">
          <input type='hidden' name='fields[replyThread]' value=''>
          <input type='hidden' name='fields[replyID]' value=''>
          <input type='hidden' name='fields[replyName]' value=''>

          
          <input required name='fields[name]' type='text' placeholder='Your Name'>
          <input name='fields[website]' type='text' placeholder='Your Website'>
          <input required name='fields[email]' type='email' placeholder='Your Email'>
          <textarea required name='fields[body]' placeholder='Your Message' rows='10'></textarea>

          
          

          
          <div class='submit-notice'>
            <strong class='submit-notice-text submit-success hidden'>Thanks for your comment! It will be shown on the site once it has been approved.</strong>
            <strong class='submit-notice-text submit-failed hidden'>Sorry, there was an error with your submission. Please make sure all required fields have been completed and try again.</strong>
          </div>

          
          <input type='submit' value='Submit' class='button'>
          <input type='submit' value='Submitted' class='hidden button' disabled>
          <input type='reset' value='Reset' class='button'>
        </form>
    </div>

    
    <div>
      <h2>Comments</h2><p>Nothing yet.</p>
      
    </div>
  </article>


  
  <div class="pagination">
    
      <a href="/blog/clustering/" class="button left"><span>Clustering</span></a>
    
    
      <a href="/blog/support-vector-machines/" class="button right"><span>Support Vector Machines</span></a>
    
  </div>
  

      </main>
      
<section id="site-sidebar">
 
  <div align="center">
      
      <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
      
      <ins class="adsbygoogle"
           style="display:block"
           data-ad-client="ca-pub-3804322353139756"
           data-ad-slot="9101315425"
           data-ad-format="auto"
           data-full-width-responsive="true"></ins>
      <script>
           $(document).ready(function(){(adsbygoogle = window.adsbygoogle || []).push({})})
      </script>
  
  </div>

  
    <section id="recent-posts">
      <header>
        <h1>Recent Posts</h1>
      </header>
      
      <article class="mini-post">
          <a href="/blog/convolutional-neural-networks/" class="image" style="--bg-image: url('https://laxmikants.github.io/img/main/cnn-neural-network.jpg');">
    <img src="https://laxmikants.github.io/img/main/cnn-neural-network.jpg" alt="">
  </a>
        <header>
          <h2><a href="/blog/convolutional-neural-networks/">Convolutional Neural Networks</a></h2>
          <time class="published" datetime="2021-01-29 00:00:00 &#43;0000 UTC">January 29, 2021</time>
        </header>
      </article>
      
      <article class="mini-post">
          <a href="/blog/neural-network-using-make-moons-dataset/" class="image" style="--bg-image: url('https://laxmikants.github.io/img/main/nn_makemoons_dataset.jpg');">
    <img src="https://laxmikants.github.io/img/main/nn_makemoons_dataset.jpg" alt="">
  </a>
        <header>
          <h2><a href="/blog/neural-network-using-make-moons-dataset/">Neural Network using Make Moons dataset</a></h2>
          <time class="published" datetime="2020-12-10 00:00:00 &#43;0000 UTC">December 10, 2020</time>
        </header>
      </article>
      
      <article class="mini-post">
          <a href="/blog/single-layer-perceptron/" class="image" style="--bg-image: url('https://laxmikants.github.io/img/main/Single_Layer_Perceptron.jpg');">
    <img src="https://laxmikants.github.io/img/main/Single_Layer_Perceptron.jpg" alt="">
  </a>
        <header>
          <h2><a href="/blog/single-layer-perceptron/">Single Layer Perceptron</a></h2>
          <time class="published" datetime="2020-12-10 00:00:00 &#43;0000 UTC">December 10, 2020</time>
        </header>
      </article>
      
      
        <footer>
          <a href="/blog/" class="button">See More</a>
        </footer>
      

      
    </section>
  

  <div align="center">
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
      
    <ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-3804322353139756"
     data-ad-slot="3115024406"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
    <script>
        $(document).ready(function(){(adsbygoogle = window.adsbygoogle || []).push({})})
    </script>
  </div>
  
  
    
      <section id="categories">
        
   
        <header>
          <h1><a href="/categories">Categories</a></h1>
        </header>
        <ul>
          
          
          <li>
              <a href="/categories/python/">python<span class="count">14</span></a>
          
          <li>
              <a href="/categories/linear-regression/">linear-regression<span class="count">2</span></a>
          
          <li>
              <a href="/categories/logistic-regression/">logistic-regression<span class="count">2</span></a>
          
          <li>
              <a href="/categories/android/">android<span class="count">1</span></a>
          
          <li>
              <a href="/categories/big-data/">big-data<span class="count">1</span></a>
          
          <li>
              <a href="/categories/bigdata/">bigdata<span class="count">1</span></a>
          
          <li>
              <a href="/categories/classification/">classification<span class="count">1</span></a>
          
          <li>
              <a href="/categories/clustering/">clustering<span class="count">1</span></a>
          
          <li>
              <a href="/categories/convolutional-neural-networks/">convolutional-neural-networks<span class="count">1</span></a>
          
          <li>
              <a href="/categories/data-science/">data-science<span class="count">1</span></a>
          
          <li>
              <a href="/categories/decision-trees/">decision-trees<span class="count">1</span></a>
          
          <li>
              <a href="/categories/eda/">eda<span class="count">1</span></a>
          
          <li>
              <a href="/categories/exploring-dataset/">exploring-dataset<span class="count">1</span></a>
          
          <li>
              <a href="/categories/frequency-tables/">frequency-tables<span class="count">1</span></a>
          
          <li>
              <a href="/categories/hadoop/">hadoop<span class="count">1</span></a>
          
          <li>
              <a href="/categories/k-nearest-neighbours/">k-nearest-neighbours<span class="count">1</span></a>
          
          <li>
              <a href="/categories/machine-learning/">machine-learning<span class="count">1</span></a>
          
          <li>
              <a href="/categories/natural-language-processing/">natural-language-processing<span class="count">1</span></a>
          
          <li>
              <a href="/categories/neural-network/">neural-network<span class="count">1</span></a>
          
          <li>
              <a href="/categories/neural-networks/">neural-networks<span class="count">1</span></a>
          
          <li>
              <a href="/categories/numeric-data/">numeric-data<span class="count">1</span></a>
          
          <li>
              <a href="/categories/pandas/">pandas<span class="count">1</span></a>
          
          <li>
              <a href="/categories/react-native/">react-native<span class="count">1</span></a>
          
          <li>
              <a href="/categories/reactjs/">reactjs<span class="count">1</span></a>
          
          <li>
              <a href="/categories/single-layer-perceptron/">single-layer-perceptron<span class="count">1</span></a>
          
          <li>
              <a href="/categories/stock-market/">stock-market<span class="count">1</span></a>
          
          <li>
              <a href="/categories/support-vector-machines/">support-vector-machines<span class="count">1</span></a>
          
          <li>
              <a href="/categories/text-analytics/">text-analytics<span class="count">1</span></a>
          
          <li>
              <a href="/categories/time-series/">time-series<span class="count">1</span></a>
          
          <li>
              <a href="/categories/web-scrapping/">web-scrapping<span class="count">1</span></a>
          
          </li>
        </ul>
        <div align="center">
          <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
          
            <ins class="adsbygoogle"
             style="display:block"
             data-ad-client="ca-pub-3804322353139756"
             data-ad-slot="5736252654"
             data-ad-format="auto"
             data-full-width-responsive="true"></ins>
          <script>
            $(document).ready(function(){(adsbygoogle = window.adsbygoogle || []).push({})})
          </script>             
        </div>             
      </section>
    
  

  
    <section id="mini-bio">
      <header>
        <h1>About</h1>
      </header>
      <p>about</p>
      <footer>
        <a href="/about" class="button">Learn More</a>
      </footer>
    </section>
  
</section>

      <footer id="site-footer">
  
  
      <ul class="socnet-icons">
        

        <li><a href="//github.com/laxmikants" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>











<li><a href="//linkedin.com/in/laxmikantsoni09" target="_blank" rel="noopener" title="LinkedIn" class="fab fa-linkedin"></a></li>




<li><a href="//facebook.com/laxmikantsoni09" target="_blank" rel="noopener" title="Facebook" class="fab fa-facebook"></a></li>










<li><a href="//twitter.com/laxmikantsoni09" target="_blank" rel="noopener" title="Twitter" class="fab fa-twitter"></a></li>













      </ul>
  
  <p class="copyright">
    ¬© 2021 Data Science Posts and Resources
      <br>
  </p>
</footer>

      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/highlight.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/python.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script><script src="//code.jquery.com/jquery-3.5.1.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.js"></script>
    <script src="//unpkg.com/lunr/lunr.js"></script><script src="/js/bundlecdn.min.daca826145adfcf5004b4b16d74010eff217a0382fad56b874f5630927a20c4e.js" integrity="sha256-2sqCYUWt/PUAS0sW10AQ7/IXoDgvrVa4dPVjCSeiDE4="></script>
    <script src="/js/add-on.js"></script>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-155379268-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

    </div>
  </body>
  
</html>

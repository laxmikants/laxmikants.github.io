---
title: "Web Scrapping Basics"
author: Laxmi K Soni 
description: "A beginer level introduction to Web Scrapping using Python"
slug: Web Scrapping
date: 2020-06-15
lastmod: 2020-06-15
categories: ["Web Scrapping"]
tags: ["Web Scrapping"]
Summary: A beginer level introduction to Web Scrapping using Python
subtitle: A beginer level introduction to Web Scrapping using Python
featured: "img/main/web_scrapping02.jpg"
output:
  html_document:
    highlight: tango
    theme: flatly
    toc: no
    toc_float: no

---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<div id="what-is-web-scrapping" class="section level3">
<h3>What is web scrapping</h3>
<p>Webscrapping also known as the the automated gathering of data from the Internet.
itself. This is usually accomplished by writing an automated program
that queries a web server, requests data , and then parses that data to extract needed informa‐
tion. BeautifulSoup, Python requests, LXML, Mechanical Soup and Scrappy are most common libraries used for web scraping.</p>
<p>Features:</p>
<ul>
<li><p>Large amounts of data can be extracted from websites</p></li>
<li><p>Convert unstructred data to structured data</p></li>
<li><p>There is a Crawler in webscrapping to index and search content</p></li>
<li><p>There is a Scrapper in webscrapping which extracts the data from the web page</p></li>
<li><p>Websites, News resources, RSS feeds, Pricing Websites, Social Media, Company information, Schemas/charts/tables/graphs are the main sources of unstructured data which a webscrapper uses to get data.</p></li>
</ul>
<div id="why-web-scraping" class="section level4">
<h4>Why Web Scraping?</h4>
<p>Browsers displays the contents in the human readable format but it can not answers to specific queries that can be used to integrate with other systems.
For example if a program requires information such as cheapest flights to newyork then the browser will provide lots of information containing images,
advertisements etc. but a web scrapping program will get the specif answer to our query. Practically web scrapping involves a wide variety of programming techniques
and technologies, such as data analysis and information security.</p>
</div>
</div>
<div id="how-it-works" class="section level3">
<h3>How it works</h3>
<ul>
<li><p>Get Request is sent using http protocol to the site the scrapper is targetting</p></li>
<li><p>Web server processes the request and then allowed to read and extract the html of the web page</p></li>
<li><p>The data is retrieved in html format after which it is carefully parsed to extricate the raw data we want from th noise surrounding it.</p></li>
<li><p>Finally the data is stored in the format to exact the specifications of the project.</p></li>
</ul>
</div>
<div id="flow" class="section level3">
<h3>Flow</h3>
<ul>
<li><p>Request</p></li>
<li><p>Check Response</p></li>
<li><p>Parse</p></li>
<li><p>Filter</p></li>
<li><p>Download</p></li>
</ul>
</div>
<div id="common-python-libraries-for-web-scrapping" class="section level3">
<h3>Common python libraries for web-scrapping</h3>
<div id="beautifulsoup" class="section level4">
<h4>BeautifulSoup</h4>
<p>The most common library is BeautifulSoup.</p>
<ul>
<li><p>It parses html document</p></li>
<li><p>It extracts text from it</p></li>
<li><p>It searches tags by their attributes</p></li>
<li><p>It has findAll and find functions are commonly used to find all attributes.</p></li>
</ul>
<p>Example#1: Getting covid-19 data from the web</p>
<pre class="python"><code>import requests
from bs4 import BeautifulSoup

url = &quot;https://www.worldometers.info/coronavirus/&quot;

page = requests.get(url)

soup = BeautifulSoup(page.text,&#39;html.parser&#39;)

total = soup.find(&quot;div&quot;,class_ = &quot;maincounter-number&quot;).text

total = total[1:len(total)-1]

other = soup.find_all(&quot;span&quot;,class_=&quot;number-table&quot;)

recovered = other[2].text

deaths = other[3].text

deaths = deaths[1:]

ans = {&quot;total cases&quot;: total, &quot;recovered&quot;: recovered, &quot;deaths&quot;: deaths}

print(ans)</code></pre>
<pre><code>## {&#39;total cases&#39;: &#39;72,194,514 &#39;, &#39;recovered&#39;: &#39;50,575,106&#39;, &#39;deaths&#39;: &#39;1,613,344&#39;}</code></pre>
<p>Example#2: Scrapping yourdictionary.com</p>
<pre class="python"><code>import requests
from bs4 import BeautifulSoup

url = &quot;https://examples.yourdictionary.com/20-words-to-avoid-on-your-resume.html&quot;

page = requests.get(url)

soup = BeautifulSoup(page.text,&#39;html.parser&#39;)

paras = soup.findAll(&#39;p&#39;)

for p in paras:
  print(p.text + &#39;\n&#39;)</code></pre>
<p>Example#2: Getting top mathematicians from web</p>
<pre class="python"><code>from urllib.request import urlopen as uReq
from bs4 import BeautifulSoup as BeautifulSoup, Tag
import pandas as pd

html = uReq(&quot;http://www.fabpedigree.com/james/gmat200.htm&quot;).read()

soup = BeautifulSoup(html,&#39;html5lib&#39;)

names = []
for item in soup.find_all(&#39;li&#39;):
  if isinstance(item, Tag):
    names.append(item.text.rstrip())

names_df = pd.DataFrame(names)    


print(names_df.head())    </code></pre>
<pre><code>##                    0
## 0       Isaac Newton
## 1         Archimedes
## 2      Carl F. Gauss
## 3     Leonhard Euler
## 4   Bernhard Riemann</code></pre>
</div>
<div id="lxml" class="section level4">
<h4>LXML</h4>
<p>Python provides lxml library which is easier to use and has lots of features. lxml and Beautiful soup have similarity. It allows to parse XML and HTML documents easily. Ease of use and performance are the key features of lxml library.</p>
<p>Example:</p>
<pre class="python"><code>from lxml import html
import requests

page = requests.get(&#39;https://projecteuler.net/problem=1&#39;)
tree = html.fromstring(page.content)
text=tree.xpath(&#39;//div[@role=&quot;problem&quot;]/p/text()&#39;)
print (text)</code></pre>
<pre><code>## [&#39;If we list all the natural numbers below 10 that are multiples of 3 or 5, we get 3, 5, 6 and 9. The sum of these multiples is 23.&#39;, &#39;Find the sum of all the multiples of 3 or 5 below 1000.&#39;]</code></pre>
</div>
<div id="machenical-soup" class="section level4">
<h4>Machenical Soup</h4>
<p>It is a library for automating interaction with the websites. User can login and logout of the website, submit forms etc.</p>
</div>
<div id="python-requests" class="section level4">
<h4>Python Requests</h4>
<p>Submitting a form with the Requests library can be done in four lines, including the
import and the instruction to print the content (yes, it’s that easy):</p>
<p>Example#3 Getting exchange rates</p>
<pre class="python"><code>
import requests
import pandas as pd
 # base_url variable store base url  
base_url = &quot;https://www.alphavantage.co/query?function=CURRENCY_EXCHANGE_RATE&quot;

from_currency = &quot;USD&quot;

to_currency = &quot;INR&quot;

main_url = base_url + &quot;&amp;from_currency=&quot; + from_currency + &quot;&amp;to_currency=&quot; + to_currency + &quot;&amp;apikey=4RSKNH0KOBS5TMP1&quot;

req_ob = requests.get(main_url) 

result = req_ob.json() 


oneusdequals = result[&quot;Realtime Currency Exchange Rate&quot;][&#39;5. Exchange Rate&#39;]


print(float(oneusdequals))
  </code></pre>
<pre><code>## 73.645</code></pre>
</div>
<div id="summary" class="section level4">
<h4>Summary</h4>
<p>There are different ways to scrape data from the internet.
Regular expressions can be useful for a one-off scrape or to avoid the overhead of parsing the entire web page.
BeautifulSoup provides a high-level interface while avoiding any difficult dependencies.
Web scraping services provide an essential service at a low cost.
It is used to scrape Price and Products for Comparison Sites and many such use cases to provide useful data for further processing.</p>
</div>
</div>

---
title: "Logistic Regression in ML"
author: Laxmi K Soni 
description: "Applying Logistic Regression to datasets"
slug: Logistic Regression
date: 2020-03-19
lastmod: 2020-03-19
categories: ["Logistic Regression"]
tags: ["Logistic Regression"]
Summary: Logistic regression is a classification algorithm used to assign observations to a discrete set of classes.
subtitle: Logistic Regression Basics
featured: "img/main/logistic-regression07.jpg"
output:
  html_document:
    highlight: tango
    theme: flatly
    toc: no
    toc_float: no
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<div id="logistic-regression" class="section level1">
<h1>Logistic Regression</h1>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Logistic regression is a machine learning classification algorithm. The algorithm assigns observations to a set of classes. The logistic regration is used to predict a categorical variabble.</p>
<div id="comparison-to-linear-regression" class="section level3">
<h3>Comparison to linear regression</h3>
<p>If we are given dataset which contains study time and exam scrores. Then</p>
<blockquote>
<ul>
<li><strong>Linear Regression</strong> helps to predic the exam scroes which is a continuous
variable.</li>
<li><strong>Logistic Regression</strong> can predict if the student
passed or failed which is a discrete categorical variable</li>
</ul>
</blockquote>
</div>
<div id="types-of-logistic-regression" class="section level3">
<h3>Types of logistic regression</h3>
<blockquote>
<ul>
<li>Binary (Pass/Fail)</li>
<li>Multi (Cats, Dogs, Sheep)</li>
<li>Ordinal (Low, Medium, High)</li>
</ul>
</blockquote>
</div>
</div>
<div id="binary-logistic-regression" class="section level2">
<h2>Binary logistic regression</h2>
<p>If we are having dataset of student exam results and the objective is to predict student will pass or faile based on hours slept and hours spent studying.</p>
<table>
<tbody>
<tr class="odd">
<td align="left"><strong>Studied</strong></td>
<td align="left"><strong>Slept</strong></td>
<td align="left"><strong>Passed</strong></td>
</tr>
<tr class="even">
<td align="left">4.85</td>
<td align="left">9.63</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">8.62</td>
<td align="left">3.23</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="left">5.43</td>
<td align="left">8.23</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">9.21</td>
<td align="left">6.34</td>
<td align="left">0</td>
</tr>
</tbody>
</table>
<p>Graph of the data</p>
<div class="figure">
<img src="/img/main/logistic_regression_exam_scores_scatter.png" alt="" />
<p class="caption">image</p>
</div>
<div id="sigmoid-activation" class="section level3">
<h3>Sigmoid activation</h3>
<p>Generally probability is assigned to predicted values. For this we use sigmoid function which maps predictions to probabilities.</p>
<p><strong>Math</strong></p>
<p><span class="math display">\[S(z) = \frac{1} {1 + e^{-z}}\]</span></p>
<blockquote>
<p><strong>note</strong></p>
<ul>
<li><span class="math inline">\(s(z)\)</span> = output between 0 and 1 (probability estimate)</li>
<li><span class="math inline">\(z\)</span> = input to the function (your algorithm’s prediction e.g. mx +
<ol start="2" style="list-style-type: lower-alpha">
<li></li>
</ol></li>
<li><span class="math inline">\(e\)</span> = base of natural log</li>
</ul>
</blockquote>
<p><strong>Graph</strong></p>
<div class="figure">
<img src="/img/main/sigmoid.png" alt="" />
<p class="caption">image</p>
</div>
<p><strong>Code</strong></p>
<pre class="r"><code>sigmoid &lt;- function(z) {
  #SIGMOID Compute sigmoid functoon
  #   J &lt;- SIGMOID(z) computes the sigmoid of z.
  
  # You need to return the following variables correctly
  z &lt;- as.matrix(z)
  g &lt;- matrix(0,dim(z)[1],dim(z)[2])
  
  # ----------------------- YOUR CODE HERE -----------------------
  # Instructions: Compute the sigmoid of each value of z (z can be a matrix,
  #               vector or scalar).
  
  g &lt;- 1 / (1 + exp(-1 * z))
  g
  # ----------------------------------------------------
}</code></pre>
</div>
<div id="decision-boundary" class="section level3">
<h3>Decision boundary</h3>
<p>The prediction function returns probability value between 0
and 1. Map this to a discrete class (true/false) based on some threshold value.</p>
<p><span class="math display">\[p \geq 0.5, class=1 \\ p &lt; 0.5, class=0\]</span></p>
<p>For example, if our threshold is .5 and our prediction function
returned .7, we will classify this observation as positive. If our
prediction is .1 we would classify the observation as negative.</p>
<div class="figure">
<img src="/img/main/logistic_regression_sigmoid_w_threshold.png" alt="" />
<p class="caption">image</p>
</div>
</div>
<div id="making-predictions" class="section level3">
<h3>Making predictions</h3>
<p>To make predictions we need to find the probability of our observations.</p>
<p><strong>Math</strong></p>
<p><span class="math display">\[z = W_0 + W_1 Studied \]</span></p>
<p>We can transform the output using the sigmoid function to return a probability value between 0 and 1.</p>
<p><span class="math display">\[P(class=1) = \frac{1} {1 + e^{-z}}\]</span></p>
<p>If the model returns .3 it believes there is only a 30% chance of
passing and this would be classified as fail.</p>
<p><strong>Code</strong></p>
<pre class="r"><code>predict &lt;- function(theta, X) {
  
  m &lt;- dim(X)[1] # Number of training examples
  
  p &lt;- rep(0,m)
  
  p[sigmoid(X %*% theta) &gt;= 0.5] &lt;- 1
  
  p
  # ----------------------------------------------------
}</code></pre>
<p>A group of 20 students spend between 0 and 6 hours studying for an exam. How does the number of hours spent studying affect the probability that the student will pass the exam?</p>
<pre class="r"><code>Hours &lt;- c(0.50, 0.75, 1.00, 1.25, 1.50, 1.75, 1.75, 2.00, 2.25,
           2.50, 2.75, 3.00, 3.25, 3.50,    4.00,   4.25,   4.50,   4.75,
           5.00, 5.50)
Pass    &lt;- c(0, 0, 0, 0, 0, 0, 1,   0, 1, 0, 1, 0, 1, 0, 1, 1, 1,   1, 1, 1)

HrsStudying &lt;- data.frame(Hours, Pass)</code></pre>
<p>The table shows the number of hours each student spent studying, and whether they passed (1) or failed (0).</p>
<pre class="r"><code>HrsStudying_Table &lt;- t(HrsStudying); HrsStudying_Table</code></pre>
<pre><code>##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
## Hours  0.5 0.75    1 1.25  1.5 1.75 1.75    2 2.25   2.5  2.75     3  3.25
## Pass   0.0 0.00    0 0.00  0.0 0.00 1.00    0 1.00   0.0  1.00     0  1.00
##       [,14] [,15] [,16] [,17] [,18] [,19] [,20]
## Hours   3.5     4  4.25   4.5  4.75     5   5.5
## Pass    0.0     1  1.00   1.0  1.00     1   1.0</code></pre>
<p>The graph shows the probability of passing the exam versus the number of hours studying, with the logistic regression curve fitted to the data.</p>
<pre class="r"><code>library(ggplot2)
ggplot(HrsStudying, aes(Hours, Pass)) +
  geom_point(aes()) +
  geom_smooth(method=&#39;glm&#39;, family=&quot;binomial&quot;, se=FALSE) +
  labs (x=&quot;Hours Studying&quot;, y=&quot;Probability of Passing Exam&quot;,
        title=&quot;Probability of Passing Exam vs Hours Studying&quot;)</code></pre>
<pre><code>## Warning: Ignoring unknown parameters: family</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="public/blog/2020-03-19-logistic-regression-28_files/figure-html/unnamed-chunk-5-1.png" width="672" />
The logistic regression analysis gives the following output.</p>
<pre class="r"><code>model &lt;- glm(Pass ~.,family=binomial(link=&#39;logit&#39;),data=HrsStudying)
model$coefficients</code></pre>
<pre><code>## (Intercept)       Hours 
##   -4.077713    1.504645</code></pre>
<p>Coefficient Std.Error z-value P-value (Wald)</p>
<p>Intercept -4.0777 1.7610 -2.316 0.0206</p>
<p>Hours 1.5046 0.6287 2.393 0.0167</p>
<p>The output indicates that hours studying is significantly associated with the probability of passing the exam (p=0.0167, Wald test). The output also provides the coefficients for Intercept = -4.0777 and Hours = 1.5046. These coefficients are entered in the logistic regression equation to estimate the probability of passing the exam:</p>
<p><span class="math display">\[P(class=1) = \frac{1} {1 + e^{-(-4.0777+1.5046* Hours)}}\]</span></p>
<p>For example, for a student who studies 3 hours, entering the value Hours = 3 in the equation gives the estimated probability of passing the exam of p = 0.60</p>
<pre class="r"><code>StudentHours &lt;- 3
ProbabilityOfPassingExam &lt;- 1/(1+exp(-(-4.0777+1.5046*StudentHours)))
ProbabilityOfPassingExam</code></pre>
<pre><code>## [1] 0.6073293</code></pre>
<p>This table shows the probability of passing the exam for several values of hours studying.</p>
<pre class="r"><code>ExamPassTable &lt;- data.frame(column1=c(1, 2, 3, 4, 5),
                            column2=c(1/(1+exp(-(-4.0777+1.5046*1))),
                                      1/(1+exp(-(-4.0777+1.5046*2))),
                                      1/(1+exp(-(-4.0777+1.5046*3))),
                                      1/(1+exp(-(-4.0777+1.5046*4))),
                                      1/(1+exp(-(-4.0777+1.5046*5)))))
names(ExamPassTable) &lt;- c(&quot;Hours of study&quot;, &quot;Probability of passing exam&quot;)
ExamPassTable</code></pre>
<pre><code>##   Hours of study Probability of passing exam
## 1              1                  0.07088985
## 2              2                  0.25568845
## 3              3                  0.60732935
## 4              4                  0.87442903
## 5              5                  0.96909067</code></pre>
</div>
<div id="cost-function" class="section level3">
<h3>Cost function</h3>
<p>Instead of Mean Squared Error, we use a cost function called Cross-entropy loss can be
divided into two separate cost functions: one for <span class="math inline">\(y=1\)</span> and one for
<span class="math inline">\(y=0\)</span>.</p>
<div class="figure">
<img src="/img/main/ng_cost_function_logistic.png" alt="" />
<p class="caption">image</p>
</div>
<p>The benefits of taking the logarithm reveal themselves when you look at
the cost function graphs for y=1 and y=0. These smooth monotonic
functions [^2] (always increasing or always decreasing) make it easy to
calculate the gradient and minimize cost. Image from Andrew Ng’s slides
on logistic regression [^3].</p>
<div class="figure">
<img src="/img/main/y1andy2_logistic_function.png" alt="" />
<p class="caption">image</p>
</div>
<p><strong>Above functions compressed into one</strong></p>
<div class="figure">
<img src="/img/main/logistic_cost_function_joined.png" alt="" />
<p class="caption">image</p>
</div>
<p>Multiplying by <span class="math inline">\(y\)</span> and <span class="math inline">\((1-y)\)</span> in the above equation is a sneaky trick
that let’s us use the same equation to solve for both y=1 and y=0 cases.
If y=0, the first side cancels out. If y=1, the second side cancels out.
In both cases we only perform the operation we need to perform.</p>
<p><strong>Vectorized cost function</strong></p>
<div class="figure">
<img src="/img/main/logistic_cost_function_vectorized.png" alt="" />
<p class="caption">image</p>
</div>
<p><strong>Code</strong></p>
<pre class="r"><code>costFunction  &lt;- function(X, y) {
  
  #COSTFUNCTION Compute cost for logistic regression
  #   J &lt;- COSTFUNCTION(theta, X, y) computes the cost of using theta as the
  #   parameter for logistic regression.
  
  function(theta) {
    # Initialize some useful values
    m &lt;- length(y) # number of training examples
    
    # You need to return the following variable correctly
    J &lt;- 0
    
  
    h &lt;- sigmoid(X %*% theta)
    J &lt;- (t(-y) %*% log(h) - t(1 - y) %*% log(1 - h)) / m
    J
    # ----------------------------------------------------
  }
}</code></pre>
</div>
<div id="gradient-descent" class="section level3">
<h3>Gradient descent</h3>
<p>Remember that the general form of gradient descent is:</p>
<p><span class="math display">\[\begin{align*}&amp; Repeat \; \lbrace \newline &amp; \; \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta) \newline &amp; \rbrace\end{align*}\]</span></p>
<p>We can do the derivative using calculus to get:</p>
<p><span class="math display">\[\begin{align*} &amp; Repeat \; \lbrace \newline &amp; \; \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \newline &amp; \rbrace \end{align*}\]</span></p>
<p>A vectorized implementation is:</p>
<p><span class="math display">\[\begin{align*} \newline &amp; \; \theta := \theta - \frac{\alpha}{m} {X^T (g(X\theta}) - y^ \rightarrow  )\end{align*}\]</span></p>
<pre class="r"><code>grad &lt;- function(X, y) {
  #COSTFUNCTION Compute gradient for logistic regression
    #   J &lt;- COSTFUNCTION(theta, X, y) computes the gradient of the cost
    #   w.r.t. to the parameters.
  function(theta) {

    # You need to return the following variable correctly
    grad &lt;- matrix(0,dim(as.matrix(theta)))
    m &lt;- length(y)

    h &lt;- sigmoid(X %*% theta)

    # calculate grads
    
    grad &lt;- (t(X) %*% (h - y)) / m
    
    grad
    # ----------------------------------------------------
    
  }
}</code></pre>
<p><strong>Pseudocode</strong></p>
<pre><code>Repeat {

  1. Calculate gradient average
  2. Multiply by learning rate
  3. Subtract from weights

}</code></pre>
<p><strong>Cost history</strong></p>
<div class="figure">
<img src="/img/main/logistic_regression_loss_history.png" alt="" />
<p class="caption">image</p>
</div>
<p><strong>Accuracy</strong></p>
<p>Accuracy measures how correct our predictions
were. In this case we simply compare predicted labels to true labels and
divide by the total.</p>
<p><strong>Decision boundary</strong></p>
<p>Another helpful technique is to plot the decision boundary on top of our
predictions to see how our labels compare to the actual labels. This
involves plotting our predicted probabilities and coloring them with
their true labels.</p>
<div class="figure">
<img src="/img/main/logistic_regression_final_decision_boundary.png" alt="" />
<p class="caption">image</p>
</div>
</div>
</div>
<div id="multiclass-logistic-regression" class="section level2">
<h2>Multiclass logistic regression</h2>
<p>Instead of <span class="math inline">\(y = {0,1}\)</span> we will expand our definition so that
<span class="math inline">\(y = {0,1...n}\)</span>. Basically we re-run binary classification multiple
times, once for each class.</p>
<div id="procedure" class="section level3">
<h3>Procedure</h3>
<blockquote>
<ol style="list-style-type: decimal">
<li>Divide the problem into n+1 binary classification problems (+1
because the index starts at 0?).</li>
<li>For each class…</li>
<li>Predict the probability the observations are in that single class.</li>
<li>prediction = &lt;math&gt;max(probability of the classes)</li>
</ol>
</blockquote>
<p>For each sub-problem, we select one class (YES) and lump all the others
into a second class (NO). Then we take the class with the highest
predicted value.</p>
<p>Since y = {0,1…n}, we divide our problem into n+1 (+1 because the index starts at 0) binary classification problems; in each one, we predict the probability that ‘y’ is a member of one of our classes.</p>
<p><span class="math display">\[\begin{align*}&amp; y \in \lbrace0, 1 ... n\rbrace \newline&amp; h_\theta^{(0)}(x) = P(y = 0 | x ; \theta) \newline&amp; h_\theta^{(1)}(x) = P(y = 1 | x ; \theta) \newline&amp; \cdots \newline&amp; h_\theta^{(n)}(x) = P(y = n | x ; \theta) \newline&amp; \mathrm{prediction} = \max_i( h_\theta ^{(i)}(x) )\newline\end{align*}\]</span></p>
<p><strong><em>Slide show</em></strong></p>
<pre class="r"><code>knitr::include_url(&#39;/slides/LogisticRegression.html&#39;)</code></pre>
<iframe src="/slides/LogisticRegression.html" width="672" height="400px">
</iframe>
</div>
</div>
</div>

<!doctype html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
  <style>

    .nocopy {
      -webkit-user-select: none;   
      -moz-user-select: none;      
      -ms-user-select: none;       
      user-select: none;           
    }  

</style>

<script data-ad-client="ca-pub-3804322353139756" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script async src = "https://www.googletagmanager.com/gtag/js?id=UA-155379268-1"></script>
<script>
 window.dataLayer = window.dataLayer || [];
 function gtag(){dataLayer.push(arguments);}
 gtag('js',new Date());
 
 gtag('config','UA-155379268-1');

</script>  
  <meta charset="utf-8">
<title>Logistic Regression in ML - Data Science Posts and Resources :: Laxmikant Soni</title>

<meta name="viewport" content="width=device-width" />

<meta name="google-site-verification" content="MeRcFEBEyWiTb3NfY4THWxbV_fx3rKOJnvr_Jk398wY" />

<meta name=keywords content="Articles and Posts on Python, R, Data Science, Machine Learning and Analytics | Laxmikant Soni, Predictive Analytics, Business, Data, Analytics, Machine Learning, Mining, Python, Intelligence, Big, Modeling, Data Science, Integration, Visualization,Statistical population,Probability,False positives,Statistical inference,Regression,Fitting,Categorical data,Classification,Clustering,Statistical comparison,CodingDistributions,Data mining,Decision trees,Machine learning,Munging and wrangling,Visualization,D3,Regularization,Assessment,Cross-validation,Neural networks,Boosting,Lift,Mode,Outlier,Predictive modeling,Big data,Confidence interval,Python,R,Jupyter Notebook,Tensorflow,Javascript,ReactJS,NodeJS,Posts and Resources on Data Science,Data Science,Hadoop,Java,Spring,Hibernate,Struts,MySQL,Oracle,DB2,Websphere,Weblogic">

<meta name=description content="Articles and Posts on Python, R, Data Science, Machine Learning and Analytics :: Laxmikant Soni">

<meta name="robots" content="index">


<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>




<meta name="generator" content="Hugo 0.80.0" /><meta itemprop="name" content="Logistic Regression in ML">
<meta itemprop="description" content="Applying Logistic Regression to datasets">
<meta itemprop="datePublished" content="2020-03-19T00:00:00+00:00" />
<meta itemprop="dateModified" content="2020-03-19T00:00:00+00:00" />
<meta itemprop="wordCount" content="1498">



<meta itemprop="keywords" content="Logistic Regression," />
<meta property="og:title" content="Logistic Regression in ML" />
<meta property="og:description" content="Applying Logistic Regression to datasets" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://laxmikants.github.io/blog/logistic-regression/" />
<meta property="article:published_time" content="2020-03-19T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-03-19T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Logistic Regression in ML"/>
<meta name="twitter:description" content="Applying Logistic Regression to datasets"/>
<meta name="twitter:site" content="@laxmikantsoni09"/>
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/monokai.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css">
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway:400,800,900|Source+Sans+Pro:400,700">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css">
        <script src="https://kit.fontawesome.com/be54eb011a.js" crossorigin="anonymous"></script>
      <script async   src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="/css/main.min.aa4e8165f5b2a16460fcb21582ad412bed8e48e9c5dc49f3b412d1703be4d75d.css" integrity="sha256-qk6BZfWyoWRg/LIVgq1BK&#43;2OSOnF3EnztBLRcDvk110="><link rel="stylesheet" href="/css/add-on.css">
        <link rel="stylesheet" href="/css/main.css">

<title>Logistic Regression in ML : Data Science Posts and Resources</title>

<meta property="og:title" content="Logistic Regression in ML">
<meta property="og:site_name" content="Data Science Posts and Resources">
<meta property="og:url" content="https://laxmikants.github.io/blog/logistic-regression/">
<meta property="og:type" content="article">
<meta property="og:locale" content="en">
<meta property="og:description" content="Applying Logistic Regression to datasets">
<meta name="description" content="Applying Logistic Regression to datasets">
<meta property="og:updated_time" content="2020-03-19T00:00:00Z">
<meta property="fb:app_id" content="428818034507005">
<meta name="author" content="Laxmikant Soni">

<meta property="article:author" content="https://laxmikants.github.io">
<meta property="article:published_time" content="2020-03-19T00:00:00Z">
<meta property="article:modified_time" content="2020-03-19T00:00:00Z">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "Logistic Regression in ML",
  "alternativeHeadline": "Logistic Regression Basics",
  "name" : "Data Science Posts and Resources",
  "url": "https://laxmikants.github.io/blog/logistic-regression/",
  "image": "https://laxmikants.github.io/",
  "sameAs":
      [ "https://www.facebook.com/laxmikantsoni09",
        "https://instagram.com/laxmikantsoni09",
        "https://www.linkedin.com/in/laxmikantsoni09",
        "https://twitter.com/laxmikantsoni09",
        "https://github.com/laxmikants",
        "https://www.kaggle.com/laxmikantsoni"
    ],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://laxmikants.github.io/blog/logistic-regression/"
  },
  "description": "Applying Logistic Regression to datasets",
  "author": {
    "@type": "Person",
    "name": "Laxmikant Soni"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data Science Posts and Resources",
    "logo": {
      "@type": "ImageObject",
      "url": "https://laxmikants.github.io/"
    }
  },
  "datePublished": "2020-03-19T00:00:00Z",
  "dateModified": "2020-03-19T00:00:00Z",
  "articleBody": "\r\n\u003cscript src=\"/rmarkdown-libs/header-attrs/header-attrs.js\"\u003e\u003c/script\u003e\r\n\u003clink href=\"/rmarkdown-libs/anchor-sections/anchor-sections.css\" rel=\"stylesheet\" /\u003e\r\n\u003cscript src=\"/rmarkdown-libs/anchor-sections/anchor-sections.js\"\u003e\u003c/script\u003e\r\n\r\n\r\n\u003cdiv id=\"logistic-regression\" class=\"section level1\"\u003e\r\n\u003ch1\u003eLogistic Regression\u003c/h1\u003e\r\n\u003cdiv id=\"introduction\" class=\"section level2\"\u003e\r\n\u003ch2\u003eIntroduction\u003c/h2\u003e\r\n\u003cp\u003eLogistic regression is a machine learning classification algorithm. The algorithm assigns observations to a set of classes. The logistic regration is used to predict a categorical variabble.\u003c/p\u003e\r\n\u003cdiv id=\"comparison-to-linear-regression\" class=\"section level3\"\u003e\r\n\u003ch3\u003eComparison to linear regression\u003c/h3\u003e\r\n\u003cp\u003eIf we are given dataset which contains study time and exam scrores. Then\u003c/p\u003e\r\n\u003cblockquote\u003e\r\n\u003cul\u003e\r\n\u003cli\u003e\u003cstrong\u003eLinear Regression\u003c/strong\u003e helps to predic the exam scroes which is a continuous\r\nvariable.\u003c/li\u003e\r\n\u003cli\u003e\u003cstrong\u003eLogistic Regression\u003c/strong\u003e can predict if the student\r\npassed or failed which is a discrete categorical variable\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003c/blockquote\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"types-of-logistic-regression\" class=\"section level3\"\u003e\r\n\u003ch3\u003eTypes of logistic regression\u003c/h3\u003e\r\n\u003cblockquote\u003e\r\n\u003cul\u003e\r\n\u003cli\u003eBinary (Pass/Fail)\u003c/li\u003e\r\n\u003cli\u003eMulti (Cats, Dogs, Sheep)\u003c/li\u003e\r\n\u003cli\u003eOrdinal (Low, Medium, High)\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003c/blockquote\u003e\r\n\u003c/div\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"binary-logistic-regression\" class=\"section level2\"\u003e\r\n\u003ch2\u003eBinary logistic regression\u003c/h2\u003e\r\n\u003cp\u003eIf we are having dataset of student exam results and the objective is to predict student will pass or faile based on hours slept and hours spent studying.\u003c/p\u003e\r\n\u003ctable\u003e\r\n\u003ctbody\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd align=\"left\"\u003e\u003cstrong\u003eStudied\u003c/strong\u003e\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e\u003cstrong\u003eSlept\u003c/strong\u003e\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e\u003cstrong\u003ePassed\u003c/strong\u003e\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd align=\"left\"\u003e4.85\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e9.63\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e1\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd align=\"left\"\u003e8.62\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e3.23\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e0\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"even\"\u003e\r\n\u003ctd align=\"left\"\u003e5.43\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e8.23\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e1\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003ctr class=\"odd\"\u003e\r\n\u003ctd align=\"left\"\u003e9.21\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e6.34\u003c/td\u003e\r\n\u003ctd align=\"left\"\u003e0\u003c/td\u003e\r\n\u003c/tr\u003e\r\n\u003c/tbody\u003e\r\n\u003c/table\u003e\r\n\u003cp\u003eGraph of the data\u003c/p\u003e\r\n\u003cdiv class=\"figure\"\u003e\r\n\u003cimg src=\"/img/main/logistic_regression_exam_scores_scatter.png\" alt=\"\" /\u003e\r\n\u003cp class=\"caption\"\u003eimage\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"sigmoid-activation\" class=\"section level3\"\u003e\r\n\u003ch3\u003eSigmoid activation\u003c/h3\u003e\r\n\u003cp\u003eGenerally probability is assigned to predicted values. For this we use sigmoid function which maps predictions to probabilities.\u003c/p\u003e\r\n\u003cp\u003e\u003cstrong\u003eMath\u003c/strong\u003e\u003c/p\u003e\r\n\u003cp\u003e\u003cspan class=\"math display\"\u003e\\[S(z) = \\frac{1} {1 + e^{-z}}\\]\u003c/span\u003e\u003c/p\u003e\r\n\u003cblockquote\u003e\r\n\u003cp\u003e\u003cstrong\u003enote\u003c/strong\u003e\u003c/p\u003e\r\n\u003cul\u003e\r\n\u003cli\u003e\u003cspan class=\"math inline\"\u003e\\(s(z)\\)\u003c/span\u003e = output between 0 and 1 (probability estimate)\u003c/li\u003e\r\n\u003cli\u003e\u003cspan class=\"math inline\"\u003e\\(z\\)\u003c/span\u003e = input to the function (your algorithm’s prediction e.g. mx +\r\n\u003col start=\"2\" style=\"list-style-type: lower-alpha\"\u003e\r\n\u003cli\u003e\u003c/li\u003e\r\n\u003c/ol\u003e\u003c/li\u003e\r\n\u003cli\u003e\u003cspan class=\"math inline\"\u003e\\(e\\)\u003c/span\u003e = base of natural log\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003c/blockquote\u003e\r\n\u003cp\u003e\u003cstrong\u003eGraph\u003c/strong\u003e\u003c/p\u003e\r\n\u003cdiv class=\"figure\"\u003e\r\n\u003cimg src=\"/img/main/sigmoid.png\" alt=\"\" /\u003e\r\n\u003cp class=\"caption\"\u003eimage\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cp\u003e\u003cstrong\u003eCode\u003c/strong\u003e\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003esigmoid \u0026lt;- function(z) {\r\n  #SIGMOID Compute sigmoid functoon\r\n  #   J \u0026lt;- SIGMOID(z) computes the sigmoid of z.\r\n  \r\n  # You need to return the following variables correctly\r\n  z \u0026lt;- as.matrix(z)\r\n  g \u0026lt;- matrix(0,dim(z)[1],dim(z)[2])\r\n  \r\n  # ----------------------- YOUR CODE HERE -----------------------\r\n  # Instructions: Compute the sigmoid of each value of z (z can be a matrix,\r\n  #               vector or scalar).\r\n  \r\n  g \u0026lt;- 1 / (1 + exp(-1 * z))\r\n  g\r\n  # ----------------------------------------------------\r\n}\u003c/code\u003e\u003c/pre\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"decision-boundary\" class=\"section level3\"\u003e\r\n\u003ch3\u003eDecision boundary\u003c/h3\u003e\r\n\u003cp\u003eThe prediction function returns probability value between 0\r\nand 1. Map this to a discrete class (true/false) based on some threshold value.\u003c/p\u003e\r\n\u003cp\u003e\u003cspan class=\"math display\"\u003e\\[p \\geq 0.5, class=1 \\\\ p \u0026lt; 0.5, class=0\\]\u003c/span\u003e\u003c/p\u003e\r\n\u003cp\u003eFor example, if our threshold is .5 and our prediction function\r\nreturned .7, we will classify this observation as positive. If our\r\nprediction is .1 we would classify the observation as negative.\u003c/p\u003e\r\n\u003cdiv class=\"figure\"\u003e\r\n\u003cimg src=\"/img/main/logistic_regression_sigmoid_w_threshold.png\" alt=\"\" /\u003e\r\n\u003cp class=\"caption\"\u003eimage\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"making-predictions\" class=\"section level3\"\u003e\r\n\u003ch3\u003eMaking predictions\u003c/h3\u003e\r\n\u003cp\u003eTo make predictions we need to find the probability of our observations.\u003c/p\u003e\r\n\u003cp\u003e\u003cstrong\u003eMath\u003c/strong\u003e\u003c/p\u003e\r\n\u003cp\u003e\u003cspan class=\"math display\"\u003e\\[z = W_0 + W_1 Studied \\]\u003c/span\u003e\u003c/p\u003e\r\n\u003cp\u003eWe can transform the output using the sigmoid function to return a probability value between 0 and 1.\u003c/p\u003e\r\n\u003cp\u003e\u003cspan class=\"math display\"\u003e\\[P(class=1) = \\frac{1} {1 + e^{-z}}\\]\u003c/span\u003e\u003c/p\u003e\r\n\u003cp\u003eIf the model returns .3 it believes there is only a 30% chance of\r\npassing and this would be classified as fail.\u003c/p\u003e\r\n\u003cp\u003e\u003cstrong\u003eCode\u003c/strong\u003e\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003epredict \u0026lt;- function(theta, X) {\r\n  \r\n  m \u0026lt;- dim(X)[1] # Number of training examples\r\n  \r\n  p \u0026lt;- rep(0,m)\r\n  \r\n  p[sigmoid(X %*% theta) \u0026gt;= 0.5] \u0026lt;- 1\r\n  \r\n  p\r\n  # ----------------------------------------------------\r\n}\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eA group of 20 students spend between 0 and 6 hours studying for an exam. How does the number of hours spent studying affect the probability that the student will pass the exam?\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003eHours \u0026lt;- c(0.50, 0.75, 1.00, 1.25, 1.50, 1.75, 1.75, 2.00, 2.25,\r\n           2.50, 2.75, 3.00, 3.25, 3.50,    4.00,   4.25,   4.50,   4.75,\r\n           5.00, 5.50)\r\nPass    \u0026lt;- c(0, 0, 0, 0, 0, 0, 1,   0, 1, 0, 1, 0, 1, 0, 1, 1, 1,   1, 1, 1)\r\n\r\nHrsStudying \u0026lt;- data.frame(Hours, Pass)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eThe table shows the number of hours each student spent studying, and whether they passed (1) or failed (0).\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003eHrsStudying_Table \u0026lt;- t(HrsStudying); HrsStudying_Table\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\r\n## Hours  0.5 0.75    1 1.25  1.5 1.75 1.75    2 2.25   2.5  2.75     3  3.25\r\n## Pass   0.0 0.00    0 0.00  0.0 0.00 1.00    0 1.00   0.0  1.00     0  1.00\r\n##       [,14] [,15] [,16] [,17] [,18] [,19] [,20]\r\n## Hours   3.5     4  4.25   4.5  4.75     5   5.5\r\n## Pass    0.0     1  1.00   1.0  1.00     1   1.0\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eThe graph shows the probability of passing the exam versus the number of hours studying, with the logistic regression curve fitted to the data.\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003elibrary(ggplot2)\r\nggplot(HrsStudying, aes(Hours, Pass)) +\r\n  geom_point(aes()) +\r\n  geom_smooth(method=\u0026#39;glm\u0026#39;, family=\u0026quot;binomial\u0026quot;, se=FALSE) +\r\n  labs (x=\u0026quot;Hours Studying\u0026quot;, y=\u0026quot;Probability of Passing Exam\u0026quot;,\r\n        title=\u0026quot;Probability of Passing Exam vs Hours Studying\u0026quot;)\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e## Warning: Ignoring unknown parameters: family\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003e\u003cimg src=\"/blog/2020-03-19-logistic-regression-28_files/figure-html/unnamed-chunk-5-1.png\" width=\"672\" /\u003e\r\nThe logistic regression analysis gives the following output.\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003emodel \u0026lt;- glm(Pass ~.,family=binomial(link=\u0026#39;logit\u0026#39;),data=HrsStudying)\r\nmodel$coefficients\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e## (Intercept)       Hours \r\n##   -4.077713    1.504645\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eCoefficient Std.Error z-value P-value (Wald)\u003c/p\u003e\r\n\u003cp\u003eIntercept -4.0777 1.7610 -2.316 0.0206\u003c/p\u003e\r\n\u003cp\u003eHours 1.5046 0.6287 2.393 0.0167\u003c/p\u003e\r\n\u003cp\u003eThe output indicates that hours studying is significantly associated with the probability of passing the exam (p=0.0167, Wald test). The output also provides the coefficients for Intercept = -4.0777 and Hours = 1.5046. These coefficients are entered in the logistic regression equation to estimate the probability of passing the exam:\u003c/p\u003e\r\n\u003cp\u003e\u003cspan class=\"math display\"\u003e\\[P(class=1) = \\frac{1} {1 + e^{-(-4.0777+1.5046* Hours)}}\\]\u003c/span\u003e\u003c/p\u003e\r\n\u003cp\u003eFor example, for a student who studies 3 hours, entering the value Hours = 3 in the equation gives the estimated probability of passing the exam of p = 0.60\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003eStudentHours \u0026lt;- 3\r\nProbabilityOfPassingExam \u0026lt;- 1/(1+exp(-(-4.0777+1.5046*StudentHours)))\r\nProbabilityOfPassingExam\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e## [1] 0.6073293\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eThis table shows the probability of passing the exam for several values of hours studying.\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003eExamPassTable \u0026lt;- data.frame(column1=c(1, 2, 3, 4, 5),\r\n                            column2=c(1/(1+exp(-(-4.0777+1.5046*1))),\r\n                                      1/(1+exp(-(-4.0777+1.5046*2))),\r\n                                      1/(1+exp(-(-4.0777+1.5046*3))),\r\n                                      1/(1+exp(-(-4.0777+1.5046*4))),\r\n                                      1/(1+exp(-(-4.0777+1.5046*5)))))\r\nnames(ExamPassTable) \u0026lt;- c(\u0026quot;Hours of study\u0026quot;, \u0026quot;Probability of passing exam\u0026quot;)\r\nExamPassTable\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e##   Hours of study Probability of passing exam\r\n## 1              1                  0.07088985\r\n## 2              2                  0.25568845\r\n## 3              3                  0.60732935\r\n## 4              4                  0.87442903\r\n## 5              5                  0.96909067\u003c/code\u003e\u003c/pre\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"cost-function\" class=\"section level3\"\u003e\r\n\u003ch3\u003eCost function\u003c/h3\u003e\r\n\u003cp\u003eInstead of Mean Squared Error, we use a cost function called Cross-entropy loss can be\r\ndivided into two separate cost functions: one for \u003cspan class=\"math inline\"\u003e\\(y=1\\)\u003c/span\u003e and one for\r\n\u003cspan class=\"math inline\"\u003e\\(y=0\\)\u003c/span\u003e.\u003c/p\u003e\r\n\u003cdiv class=\"figure\"\u003e\r\n\u003cimg src=\"/img/main/ng_cost_function_logistic.png\" alt=\"\" /\u003e\r\n\u003cp class=\"caption\"\u003eimage\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cp\u003eThe benefits of taking the logarithm reveal themselves when you look at\r\nthe cost function graphs for y=1 and y=0. These smooth monotonic\r\nfunctions [^2] (always increasing or always decreasing) make it easy to\r\ncalculate the gradient and minimize cost. Image from Andrew Ng’s slides\r\non logistic regression [^3].\u003c/p\u003e\r\n\u003cdiv class=\"figure\"\u003e\r\n\u003cimg src=\"/img/main/y1andy2_logistic_function.png\" alt=\"\" /\u003e\r\n\u003cp class=\"caption\"\u003eimage\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cp\u003e\u003cstrong\u003eAbove functions compressed into one\u003c/strong\u003e\u003c/p\u003e\r\n\u003cdiv class=\"figure\"\u003e\r\n\u003cimg src=\"/img/main/logistic_cost_function_joined.png\" alt=\"\" /\u003e\r\n\u003cp class=\"caption\"\u003eimage\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cp\u003eMultiplying by \u003cspan class=\"math inline\"\u003e\\(y\\)\u003c/span\u003e and \u003cspan class=\"math inline\"\u003e\\((1-y)\\)\u003c/span\u003e in the above equation is a sneaky trick\r\nthat let’s us use the same equation to solve for both y=1 and y=0 cases.\r\nIf y=0, the first side cancels out. If y=1, the second side cancels out.\r\nIn both cases we only perform the operation we need to perform.\u003c/p\u003e\r\n\u003cp\u003e\u003cstrong\u003eVectorized cost function\u003c/strong\u003e\u003c/p\u003e\r\n\u003cdiv class=\"figure\"\u003e\r\n\u003cimg src=\"/img/main/logistic_cost_function_vectorized.png\" alt=\"\" /\u003e\r\n\u003cp class=\"caption\"\u003eimage\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cp\u003e\u003cstrong\u003eCode\u003c/strong\u003e\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003ecostFunction  \u0026lt;- function(X, y) {\r\n  \r\n  #COSTFUNCTION Compute cost for logistic regression\r\n  #   J \u0026lt;- COSTFUNCTION(theta, X, y) computes the cost of using theta as the\r\n  #   parameter for logistic regression.\r\n  \r\n  function(theta) {\r\n    # Initialize some useful values\r\n    m \u0026lt;- length(y) # number of training examples\r\n    \r\n    # You need to return the following variable correctly\r\n    J \u0026lt;- 0\r\n    \r\n  \r\n    h \u0026lt;- sigmoid(X %*% theta)\r\n    J \u0026lt;- (t(-y) %*% log(h) - t(1 - y) %*% log(1 - h)) / m\r\n    J\r\n    # ----------------------------------------------------\r\n  }\r\n}\u003c/code\u003e\u003c/pre\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"gradient-descent\" class=\"section level3\"\u003e\r\n\u003ch3\u003eGradient descent\u003c/h3\u003e\r\n\u003cp\u003eRemember that the general form of gradient descent is:\u003c/p\u003e\r\n\u003cp\u003e\u003cspan class=\"math display\"\u003e\\[\\begin{align*}\u0026amp; Repeat \\; \\lbrace \\newline \u0026amp; \\; \\theta_j := \\theta_j - \\alpha \\dfrac{\\partial}{\\partial \\theta_j}J(\\theta) \\newline \u0026amp; \\rbrace\\end{align*}\\]\u003c/span\u003e\u003c/p\u003e\r\n\u003cp\u003eWe can do the derivative using calculus to get:\u003c/p\u003e\r\n\u003cp\u003e\u003cspan class=\"math display\"\u003e\\[\\begin{align*} \u0026amp; Repeat \\; \\lbrace \\newline \u0026amp; \\; \\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\newline \u0026amp; \\rbrace \\end{align*}\\]\u003c/span\u003e\u003c/p\u003e\r\n\u003cp\u003eA vectorized implementation is:\u003c/p\u003e\r\n\u003cp\u003e\u003cspan class=\"math display\"\u003e\\[\\begin{align*} \\newline \u0026amp; \\; \\theta := \\theta - \\frac{\\alpha}{m} {X^T (g(X\\theta}) - y^ \\rightarrow  )\\end{align*}\\]\u003c/span\u003e\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003egrad \u0026lt;- function(X, y) {\r\n  #COSTFUNCTION Compute gradient for logistic regression\r\n    #   J \u0026lt;- COSTFUNCTION(theta, X, y) computes the gradient of the cost\r\n    #   w.r.t. to the parameters.\r\n  function(theta) {\r\n\r\n    # You need to return the following variable correctly\r\n    grad \u0026lt;- matrix(0,dim(as.matrix(theta)))\r\n    m \u0026lt;- length(y)\r\n\r\n    h \u0026lt;- sigmoid(X %*% theta)\r\n\r\n    # calculate grads\r\n    \r\n    grad \u0026lt;- (t(X) %*% (h - y)) / m\r\n    \r\n    grad\r\n    # ----------------------------------------------------\r\n    \r\n  }\r\n}\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003e\u003cstrong\u003ePseudocode\u003c/strong\u003e\u003c/p\u003e\r\n\u003cpre\u003e\u003ccode\u003eRepeat {\r\n\r\n  1. Calculate gradient average\r\n  2. Multiply by learning rate\r\n  3. Subtract from weights\r\n\r\n}\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003e\u003cstrong\u003eCost history\u003c/strong\u003e\u003c/p\u003e\r\n\u003cdiv class=\"figure\"\u003e\r\n\u003cimg src=\"/img/main/logistic_regression_loss_history.png\" alt=\"\" /\u003e\r\n\u003cp class=\"caption\"\u003eimage\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cp\u003e\u003cstrong\u003eAccuracy\u003c/strong\u003e\u003c/p\u003e\r\n\u003cp\u003eAccuracy measures how correct our predictions\r\nwere. In this case we simply compare predicted labels to true labels and\r\ndivide by the total.\u003c/p\u003e\r\n\u003cp\u003e\u003cstrong\u003eDecision boundary\u003c/strong\u003e\u003c/p\u003e\r\n\u003cp\u003eAnother helpful technique is to plot the decision boundary on top of our\r\npredictions to see how our labels compare to the actual labels. This\r\ninvolves plotting our predicted probabilities and coloring them with\r\ntheir true labels.\u003c/p\u003e\r\n\u003cdiv class=\"figure\"\u003e\r\n\u003cimg src=\"/img/main/logistic_regression_final_decision_boundary.png\" alt=\"\" /\u003e\r\n\u003cp class=\"caption\"\u003eimage\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003c/div\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"multiclass-logistic-regression\" class=\"section level2\"\u003e\r\n\u003ch2\u003eMulticlass logistic regression\u003c/h2\u003e\r\n\u003cp\u003eInstead of \u003cspan class=\"math inline\"\u003e\\(y = {0,1}\\)\u003c/span\u003e we will expand our definition so that\r\n\u003cspan class=\"math inline\"\u003e\\(y = {0,1...n}\\)\u003c/span\u003e. Basically we re-run binary classification multiple\r\ntimes, once for each class.\u003c/p\u003e\r\n\u003cdiv id=\"procedure\" class=\"section level3\"\u003e\r\n\u003ch3\u003eProcedure\u003c/h3\u003e\r\n\u003cblockquote\u003e\r\n\u003col style=\"list-style-type: decimal\"\u003e\r\n\u003cli\u003eDivide the problem into n+1 binary classification problems (+1\r\nbecause the index starts at 0?).\u003c/li\u003e\r\n\u003cli\u003eFor each class…\u003c/li\u003e\r\n\u003cli\u003ePredict the probability the observations are in that single class.\u003c/li\u003e\r\n\u003cli\u003eprediction = \u0026lt;math\u0026gt;max(probability of the classes)\u003c/li\u003e\r\n\u003c/ol\u003e\r\n\u003c/blockquote\u003e\r\n\u003cp\u003eFor each sub-problem, we select one class (YES) and lump all the others\r\ninto a second class (NO). Then we take the class with the highest\r\npredicted value.\u003c/p\u003e\r\n\u003cp\u003eSince y = {0,1…n}, we divide our problem into n+1 (+1 because the index starts at 0) binary classification problems; in each one, we predict the probability that ‘y’ is a member of one of our classes.\u003c/p\u003e\r\n\u003cp\u003e\u003cspan class=\"math display\"\u003e\\[\\begin{align*}\u0026amp; y \\in \\lbrace0, 1 ... n\\rbrace \\newline\u0026amp; h_\\theta^{(0)}(x) = P(y = 0 | x ; \\theta) \\newline\u0026amp; h_\\theta^{(1)}(x) = P(y = 1 | x ; \\theta) \\newline\u0026amp; \\cdots \\newline\u0026amp; h_\\theta^{(n)}(x) = P(y = n | x ; \\theta) \\newline\u0026amp; \\mathrm{prediction} = \\max_i( h_\\theta ^{(i)}(x) )\\newline\\end{align*}\\]\u003c/span\u003e\u003c/p\u003e\r\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eSlide show\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\r\n\u003cpre class=\"r\"\u003e\u003ccode\u003eknitr::include_url(\u0026#39;/slides/LogisticRegression.html\u0026#39;)\u003c/code\u003e\u003c/pre\u003e\r\n\u003ciframe src=\"/slides/LogisticRegression.html\" width=\"672\" height=\"400px\"\u003e\r\n\u003c/iframe\u003e\r\n\u003c/div\u003e\r\n\u003c/div\u003e\r\n\u003c/div\u003e"
}
</script>

  

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-155379268-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>  
</head>
  <body>
    
    
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T5KFS4C"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    
      
    


<header id="site-header">
  <nav id="site-nav">
    <h1 class="nav-title">
      <a href="/" class="nav">
        
          Blog
        
      </a>
    </h1>
    <menu id="site-nav-menu" class="flyout-menu menu">
      
        
          
          <a href="/" class="nav link"><i class='fa fa-home'></i> Home</a>
        
      
        
          
          <a href="/about/" class="nav link"><i class='far fa-id-card'></i> About</a>
        
      
        
          
          <a href="/blog/" class="nav link"><i class='far fa-newspaper'></i> Blog</a>
        
      
        
          
          <a href="/categories/" class="nav link"><i class='fas fa-sitemap'></i> Categories</a>
        
      
        
          
          <a href="/contact/" class="nav link"><i class='far fa-envelope'></i> Contact</a>
        
      
      <a href="#share-menu" class="nav link share-toggle"><i class="fas fa-share-alt">&nbsp;</i>Share</a>
      <a href="#search-input" class="nav link search-toggle"><i class="fas fa-search">&nbsp;</i>Search</a>
    </menu>
 
    <a href="#search-input" class="nav search-toggle"><i class="fas fa-search fa-2x">&nbsp;</i></a>
    <a href="#share-menu" class="nav share-toggle"><i class="fas fa-share-alt fa-2x">&nbsp;</i></a>
    <a href="#lang-menu" class="nav lang-toggle" lang="en">en</a>
    <a href="#site-nav" class="nav nav-toggle"><i class="fas fa-bars fa-2x"></i></a>
  </nav>
  
  
  <menu id="search" class="menu"><input id="search-input" class="search-input menu"></input><div id="search-results" class="search-results menu">

  </div></menu>
  
  <menu id="lang-menu" class="flyout-menu menu">
  <a href="#" lang="en" class="nav link active">English (en)</a>
  
    
      
    
  
</menu>

  
    <menu id="share-menu" class="flyout-menu menu">
      <h1>Share Post</h1>
      




  
    
    <a href="//twitter.com/share?text=Logistic%20Regression%20in%20ML&amp;url=https%3a%2f%2flaxmikants.github.io%2fblog%2flogistic-regression%2f" target="_blank" rel="noopener" class="nav share-btn twitter">
        <p>Twitter</p>
      </a>
  

  
      <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2flaxmikants.github.io%2fblog%2flogistic-regression%2f" target="_blank" rel="noopener" class="nav share-btn facebook">
        <p>Facebook</p>
        </a>
  

  
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2flaxmikants.github.io%2fblog%2flogistic-regression%2f&amp;title=Logistic%20Regression%20in%20ML" target="_blank" rel="noopener" class="nav share-btn linkedin">
            <p>LinkedIn</p>
          </a>
  


    </menu>
  
</header>

    <div id="wrapper">
      <section id="site-intro" >
  
  <header>
    <h1>Data Science Posts and Resources</h1>
  </header>
  <main>
    <p>Articles on Data Science</p>
  </main>
  
    <footer>
      <ul class="socnet-icons">
        

        <li><a href="//github.com/laxmikants" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>











<li><a href="//linkedin.com/in/laxmikantsoni09" target="_blank" rel="noopener" title="LinkedIn" class="fab fa-linkedin"></a></li>




<li><a href="//facebook.com/laxmikantsoni09" target="_blank" rel="noopener" title="Facebook" class="fab fa-facebook"></a></li>










<li><a href="//twitter.com/laxmikantsoni09" target="_blank" rel="noopener" title="Twitter" class="fab fa-twitter"></a></li>













      </ul>
    </footer>
  
</section>



      <main id="site-main">
        
  <article class="post">
    <header>
  <div class="title">
    
      <h2><a href="/blog/logistic-regression/">Logistic Regression in ML</a></h2>
    
    
      <p>Applying Logistic Regression to datasets</p>
    
  </div>
  <div class="meta">
    <time datetime="2020-03-19 00:00:00 &#43;0000 UTC">March 19, 2020</time>
    <p>Laxmi K Soni</p>
    <p>8-Minute Read</p>
  </div>
</header>

    <div id="socnet-share">
      




  
    
    <a href="//twitter.com/share?text=Logistic%20Regression%20in%20ML&amp;url=https%3a%2f%2flaxmikants.github.io%2fblog%2flogistic-regression%2f" target="_blank" rel="noopener" class="nav share-btn twitter">
        <p>Twitter</p>
      </a>
  

  
      <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2flaxmikants.github.io%2fblog%2flogistic-regression%2f" target="_blank" rel="noopener" class="nav share-btn facebook">
        <p>Facebook</p>
        </a>
  

  
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2flaxmikants.github.io%2fblog%2flogistic-regression%2f&amp;title=Logistic%20Regression%20in%20ML" target="_blank" rel="noopener" class="nav share-btn linkedin">
            <p>LinkedIn</p>
          </a>
  


    </div>
  
    <div class="content">
      <a href="/blog/logistic-regression/" class="image" style="--bg-image: url('https://laxmikants.github.io/img/main/logistic-regression07.jpg');">
    <img src="https://laxmikants.github.io/img/main/logistic-regression07.jpg" alt="">
  </a>
      
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<div id="logistic-regression" class="section level1">
<h1>Logistic Regression</h1>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Logistic regression is a machine learning classification algorithm. The algorithm assigns observations to a set of classes. The logistic regration is used to predict a categorical variabble.</p>
<div id="comparison-to-linear-regression" class="section level3">
<h3>Comparison to linear regression</h3>
<p>If we are given dataset which contains study time and exam scrores. Then</p>
<blockquote>
<ul>
<li><strong>Linear Regression</strong> helps to predic the exam scroes which is a continuous
variable.</li>
<li><strong>Logistic Regression</strong> can predict if the student
passed or failed which is a discrete categorical variable</li>
</ul>
</blockquote>
</div>
<div id="types-of-logistic-regression" class="section level3">
<h3>Types of logistic regression</h3>
<blockquote>
<ul>
<li>Binary (Pass/Fail)</li>
<li>Multi (Cats, Dogs, Sheep)</li>
<li>Ordinal (Low, Medium, High)</li>
</ul>
</blockquote>
</div>
</div>
<div id="binary-logistic-regression" class="section level2">
<h2>Binary logistic regression</h2>
<p>If we are having dataset of student exam results and the objective is to predict student will pass or faile based on hours slept and hours spent studying.</p>
<table>
<tbody>
<tr class="odd">
<td align="left"><strong>Studied</strong></td>
<td align="left"><strong>Slept</strong></td>
<td align="left"><strong>Passed</strong></td>
</tr>
<tr class="even">
<td align="left">4.85</td>
<td align="left">9.63</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">8.62</td>
<td align="left">3.23</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="left">5.43</td>
<td align="left">8.23</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">9.21</td>
<td align="left">6.34</td>
<td align="left">0</td>
</tr>
</tbody>
</table>
<p>Graph of the data</p>
<div class="figure">
<img src="/img/main/logistic_regression_exam_scores_scatter.png" alt="" />
<p class="caption">image</p>
</div>
<div id="sigmoid-activation" class="section level3">
<h3>Sigmoid activation</h3>
<p>Generally probability is assigned to predicted values. For this we use sigmoid function which maps predictions to probabilities.</p>
<p><strong>Math</strong></p>
<p><span class="math display">\[S(z) = \frac{1} {1 + e^{-z}}\]</span></p>
<blockquote>
<p><strong>note</strong></p>
<ul>
<li><span class="math inline">\(s(z)\)</span> = output between 0 and 1 (probability estimate)</li>
<li><span class="math inline">\(z\)</span> = input to the function (your algorithm’s prediction e.g. mx +
<ol start="2" style="list-style-type: lower-alpha">
<li></li>
</ol></li>
<li><span class="math inline">\(e\)</span> = base of natural log</li>
</ul>
</blockquote>
<p><strong>Graph</strong></p>
<div class="figure">
<img src="/img/main/sigmoid.png" alt="" />
<p class="caption">image</p>
</div>
<p><strong>Code</strong></p>
<pre class="r"><code>sigmoid &lt;- function(z) {
  #SIGMOID Compute sigmoid functoon
  #   J &lt;- SIGMOID(z) computes the sigmoid of z.
  
  # You need to return the following variables correctly
  z &lt;- as.matrix(z)
  g &lt;- matrix(0,dim(z)[1],dim(z)[2])
  
  # ----------------------- YOUR CODE HERE -----------------------
  # Instructions: Compute the sigmoid of each value of z (z can be a matrix,
  #               vector or scalar).
  
  g &lt;- 1 / (1 + exp(-1 * z))
  g
  # ----------------------------------------------------
}</code></pre>
</div>
<div id="decision-boundary" class="section level3">
<h3>Decision boundary</h3>
<p>The prediction function returns probability value between 0
and 1. Map this to a discrete class (true/false) based on some threshold value.</p>
<p><span class="math display">\[p \geq 0.5, class=1 \\ p &lt; 0.5, class=0\]</span></p>
<p>For example, if our threshold is .5 and our prediction function
returned .7, we will classify this observation as positive. If our
prediction is .1 we would classify the observation as negative.</p>
<div class="figure">
<img src="/img/main/logistic_regression_sigmoid_w_threshold.png" alt="" />
<p class="caption">image</p>
</div>
</div>
<div id="making-predictions" class="section level3">
<h3>Making predictions</h3>
<p>To make predictions we need to find the probability of our observations.</p>
<p><strong>Math</strong></p>
<p><span class="math display">\[z = W_0 + W_1 Studied \]</span></p>
<p>We can transform the output using the sigmoid function to return a probability value between 0 and 1.</p>
<p><span class="math display">\[P(class=1) = \frac{1} {1 + e^{-z}}\]</span></p>
<p>If the model returns .3 it believes there is only a 30% chance of
passing and this would be classified as fail.</p>
<p><strong>Code</strong></p>
<pre class="r"><code>predict &lt;- function(theta, X) {
  
  m &lt;- dim(X)[1] # Number of training examples
  
  p &lt;- rep(0,m)
  
  p[sigmoid(X %*% theta) &gt;= 0.5] &lt;- 1
  
  p
  # ----------------------------------------------------
}</code></pre>
<p>A group of 20 students spend between 0 and 6 hours studying for an exam. How does the number of hours spent studying affect the probability that the student will pass the exam?</p>
<pre class="r"><code>Hours &lt;- c(0.50, 0.75, 1.00, 1.25, 1.50, 1.75, 1.75, 2.00, 2.25,
           2.50, 2.75, 3.00, 3.25, 3.50,    4.00,   4.25,   4.50,   4.75,
           5.00, 5.50)
Pass    &lt;- c(0, 0, 0, 0, 0, 0, 1,   0, 1, 0, 1, 0, 1, 0, 1, 1, 1,   1, 1, 1)

HrsStudying &lt;- data.frame(Hours, Pass)</code></pre>
<p>The table shows the number of hours each student spent studying, and whether they passed (1) or failed (0).</p>
<pre class="r"><code>HrsStudying_Table &lt;- t(HrsStudying); HrsStudying_Table</code></pre>
<pre><code>##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
## Hours  0.5 0.75    1 1.25  1.5 1.75 1.75    2 2.25   2.5  2.75     3  3.25
## Pass   0.0 0.00    0 0.00  0.0 0.00 1.00    0 1.00   0.0  1.00     0  1.00
##       [,14] [,15] [,16] [,17] [,18] [,19] [,20]
## Hours   3.5     4  4.25   4.5  4.75     5   5.5
## Pass    0.0     1  1.00   1.0  1.00     1   1.0</code></pre>
<p>The graph shows the probability of passing the exam versus the number of hours studying, with the logistic regression curve fitted to the data.</p>
<pre class="r"><code>library(ggplot2)
ggplot(HrsStudying, aes(Hours, Pass)) +
  geom_point(aes()) +
  geom_smooth(method=&#39;glm&#39;, family=&quot;binomial&quot;, se=FALSE) +
  labs (x=&quot;Hours Studying&quot;, y=&quot;Probability of Passing Exam&quot;,
        title=&quot;Probability of Passing Exam vs Hours Studying&quot;)</code></pre>
<pre><code>## Warning: Ignoring unknown parameters: family</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="/blog/2020-03-19-logistic-regression-28_files/figure-html/unnamed-chunk-5-1.png" width="672" />
The logistic regression analysis gives the following output.</p>
<pre class="r"><code>model &lt;- glm(Pass ~.,family=binomial(link=&#39;logit&#39;),data=HrsStudying)
model$coefficients</code></pre>
<pre><code>## (Intercept)       Hours 
##   -4.077713    1.504645</code></pre>
<p>Coefficient Std.Error z-value P-value (Wald)</p>
<p>Intercept -4.0777 1.7610 -2.316 0.0206</p>
<p>Hours 1.5046 0.6287 2.393 0.0167</p>
<p>The output indicates that hours studying is significantly associated with the probability of passing the exam (p=0.0167, Wald test). The output also provides the coefficients for Intercept = -4.0777 and Hours = 1.5046. These coefficients are entered in the logistic regression equation to estimate the probability of passing the exam:</p>
<p><span class="math display">\[P(class=1) = \frac{1} {1 + e^{-(-4.0777+1.5046* Hours)}}\]</span></p>
<p>For example, for a student who studies 3 hours, entering the value Hours = 3 in the equation gives the estimated probability of passing the exam of p = 0.60</p>
<pre class="r"><code>StudentHours &lt;- 3
ProbabilityOfPassingExam &lt;- 1/(1+exp(-(-4.0777+1.5046*StudentHours)))
ProbabilityOfPassingExam</code></pre>
<pre><code>## [1] 0.6073293</code></pre>
<p>This table shows the probability of passing the exam for several values of hours studying.</p>
<pre class="r"><code>ExamPassTable &lt;- data.frame(column1=c(1, 2, 3, 4, 5),
                            column2=c(1/(1+exp(-(-4.0777+1.5046*1))),
                                      1/(1+exp(-(-4.0777+1.5046*2))),
                                      1/(1+exp(-(-4.0777+1.5046*3))),
                                      1/(1+exp(-(-4.0777+1.5046*4))),
                                      1/(1+exp(-(-4.0777+1.5046*5)))))
names(ExamPassTable) &lt;- c(&quot;Hours of study&quot;, &quot;Probability of passing exam&quot;)
ExamPassTable</code></pre>
<pre><code>##   Hours of study Probability of passing exam
## 1              1                  0.07088985
## 2              2                  0.25568845
## 3              3                  0.60732935
## 4              4                  0.87442903
## 5              5                  0.96909067</code></pre>
</div>
<div id="cost-function" class="section level3">
<h3>Cost function</h3>
<p>Instead of Mean Squared Error, we use a cost function called Cross-entropy loss can be
divided into two separate cost functions: one for <span class="math inline">\(y=1\)</span> and one for
<span class="math inline">\(y=0\)</span>.</p>
<div class="figure">
<img src="/img/main/ng_cost_function_logistic.png" alt="" />
<p class="caption">image</p>
</div>
<p>The benefits of taking the logarithm reveal themselves when you look at
the cost function graphs for y=1 and y=0. These smooth monotonic
functions [^2] (always increasing or always decreasing) make it easy to
calculate the gradient and minimize cost. Image from Andrew Ng’s slides
on logistic regression [^3].</p>
<div class="figure">
<img src="/img/main/y1andy2_logistic_function.png" alt="" />
<p class="caption">image</p>
</div>
<p><strong>Above functions compressed into one</strong></p>
<div class="figure">
<img src="/img/main/logistic_cost_function_joined.png" alt="" />
<p class="caption">image</p>
</div>
<p>Multiplying by <span class="math inline">\(y\)</span> and <span class="math inline">\((1-y)\)</span> in the above equation is a sneaky trick
that let’s us use the same equation to solve for both y=1 and y=0 cases.
If y=0, the first side cancels out. If y=1, the second side cancels out.
In both cases we only perform the operation we need to perform.</p>
<p><strong>Vectorized cost function</strong></p>
<div class="figure">
<img src="/img/main/logistic_cost_function_vectorized.png" alt="" />
<p class="caption">image</p>
</div>
<p><strong>Code</strong></p>
<pre class="r"><code>costFunction  &lt;- function(X, y) {
  
  #COSTFUNCTION Compute cost for logistic regression
  #   J &lt;- COSTFUNCTION(theta, X, y) computes the cost of using theta as the
  #   parameter for logistic regression.
  
  function(theta) {
    # Initialize some useful values
    m &lt;- length(y) # number of training examples
    
    # You need to return the following variable correctly
    J &lt;- 0
    
  
    h &lt;- sigmoid(X %*% theta)
    J &lt;- (t(-y) %*% log(h) - t(1 - y) %*% log(1 - h)) / m
    J
    # ----------------------------------------------------
  }
}</code></pre>
</div>
<div id="gradient-descent" class="section level3">
<h3>Gradient descent</h3>
<p>Remember that the general form of gradient descent is:</p>
<p><span class="math display">\[\begin{align*}&amp; Repeat \; \lbrace \newline &amp; \; \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta) \newline &amp; \rbrace\end{align*}\]</span></p>
<p>We can do the derivative using calculus to get:</p>
<p><span class="math display">\[\begin{align*} &amp; Repeat \; \lbrace \newline &amp; \; \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \newline &amp; \rbrace \end{align*}\]</span></p>
<p>A vectorized implementation is:</p>
<p><span class="math display">\[\begin{align*} \newline &amp; \; \theta := \theta - \frac{\alpha}{m} {X^T (g(X\theta}) - y^ \rightarrow  )\end{align*}\]</span></p>
<pre class="r"><code>grad &lt;- function(X, y) {
  #COSTFUNCTION Compute gradient for logistic regression
    #   J &lt;- COSTFUNCTION(theta, X, y) computes the gradient of the cost
    #   w.r.t. to the parameters.
  function(theta) {

    # You need to return the following variable correctly
    grad &lt;- matrix(0,dim(as.matrix(theta)))
    m &lt;- length(y)

    h &lt;- sigmoid(X %*% theta)

    # calculate grads
    
    grad &lt;- (t(X) %*% (h - y)) / m
    
    grad
    # ----------------------------------------------------
    
  }
}</code></pre>
<p><strong>Pseudocode</strong></p>
<pre><code>Repeat {

  1. Calculate gradient average
  2. Multiply by learning rate
  3. Subtract from weights

}</code></pre>
<p><strong>Cost history</strong></p>
<div class="figure">
<img src="/img/main/logistic_regression_loss_history.png" alt="" />
<p class="caption">image</p>
</div>
<p><strong>Accuracy</strong></p>
<p>Accuracy measures how correct our predictions
were. In this case we simply compare predicted labels to true labels and
divide by the total.</p>
<p><strong>Decision boundary</strong></p>
<p>Another helpful technique is to plot the decision boundary on top of our
predictions to see how our labels compare to the actual labels. This
involves plotting our predicted probabilities and coloring them with
their true labels.</p>
<div class="figure">
<img src="/img/main/logistic_regression_final_decision_boundary.png" alt="" />
<p class="caption">image</p>
</div>
</div>
</div>
<div id="multiclass-logistic-regression" class="section level2">
<h2>Multiclass logistic regression</h2>
<p>Instead of <span class="math inline">\(y = {0,1}\)</span> we will expand our definition so that
<span class="math inline">\(y = {0,1...n}\)</span>. Basically we re-run binary classification multiple
times, once for each class.</p>
<div id="procedure" class="section level3">
<h3>Procedure</h3>
<blockquote>
<ol style="list-style-type: decimal">
<li>Divide the problem into n+1 binary classification problems (+1
because the index starts at 0?).</li>
<li>For each class…</li>
<li>Predict the probability the observations are in that single class.</li>
<li>prediction = &lt;math&gt;max(probability of the classes)</li>
</ol>
</blockquote>
<p>For each sub-problem, we select one class (YES) and lump all the others
into a second class (NO). Then we take the class with the highest
predicted value.</p>
<p>Since y = {0,1…n}, we divide our problem into n+1 (+1 because the index starts at 0) binary classification problems; in each one, we predict the probability that ‘y’ is a member of one of our classes.</p>
<p><span class="math display">\[\begin{align*}&amp; y \in \lbrace0, 1 ... n\rbrace \newline&amp; h_\theta^{(0)}(x) = P(y = 0 | x ; \theta) \newline&amp; h_\theta^{(1)}(x) = P(y = 1 | x ; \theta) \newline&amp; \cdots \newline&amp; h_\theta^{(n)}(x) = P(y = n | x ; \theta) \newline&amp; \mathrm{prediction} = \max_i( h_\theta ^{(i)}(x) )\newline\end{align*}\]</span></p>
<p><strong><em>Slide show</em></strong></p>
<pre class="r"><code>knitr::include_url(&#39;/slides/LogisticRegression.html&#39;)</code></pre>
<iframe src="/slides/LogisticRegression.html" width="672" height="400px">
</iframe>
</div>
</div>
</div>

      <div align = "center">
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        
        <ins class="adsbygoogle"
         style="display:block"
         data-ad-client="ca-pub-3804322353139756"
         data-ad-slot="3387213493"
         data-ad-format="auto"
         data-full-width-responsive="true"></ins>
        <script>
            $(document).ready(function(){(adsbygoogle = window.adsbygoogle || []).push({})})
        </script>
    </div>            
    </div>
    <footer>
      <div class="stats">
  <ul class="categories">
    
      
        
          <li><a class="article-terms-link" href="/categories/logistic-regression/">Logistic Regression</a></li>
        
      
    
  </ul>
  <ul class="tags">
    
      
        
          <li><a class="article-terms-link" href="/tags/logistic-regression/">Logistic Regression</a></li>
        
      
    
  </ul>
</div>

    </footer>
  </article>
  
    

  <article class="post">
    
    <div>
      <h2 id="say-something">Say Something</h2>
        <form id="comment-form" class="new-comment" method="POST">
          
          <h3 class='reply-notice hidden'>
            <span class='reply-name'></span>
          </h3>

          
          <input type="hidden" name="options[entryId]" value="f7e619559a1515af867b6cbf59d2db27">
          <input type='hidden' name='fields[replyThread]' value=''>
          <input type='hidden' name='fields[replyID]' value=''>
          <input type='hidden' name='fields[replyName]' value=''>

          
          <input required name='fields[name]' type='text' placeholder='Your Name'>
          <input name='fields[website]' type='text' placeholder='Your Website'>
          <input required name='fields[email]' type='email' placeholder='Your Email'>
          <textarea required name='fields[body]' placeholder='Your Message' rows='10'></textarea>

          
          

          
          <div class='submit-notice'>
            <strong class='submit-notice-text submit-success hidden'>Thanks for your comment! It will be shown on the site once it has been approved.</strong>
            <strong class='submit-notice-text submit-failed hidden'>Sorry, there was an error with your submission. Please make sure all required fields have been completed and try again.</strong>
          </div>

          
          <input type='submit' value='Submit' class='button'>
          <input type='submit' value='Submitted' class='hidden button' disabled>
          <input type='reset' value='Reset' class='button'>
        </form>
    </div>

    
    <div>
      <h2>Comments</h2><p>Nothing yet.</p>
      
    </div>
  </article>


  
  <div class="pagination">
    
      <a href="/blog/applying-logistic-regression/" class="button left"><span>Applying Logistic Regression</span></a>
    
    
      <a href="/blog/classification/" class="button right"><span>Classification</span></a>
    
  </div>
  

      </main>
      
<section id="site-sidebar">

  
    <section id="recent-posts">
      <header>
        <h1>Recent Posts</h1>
      </header>
      
      <article class="mini-post">
          <a href="/blog/convolutional-neural-networks/" class="image" style="--bg-image: url('https://laxmikants.github.io/img/main/cnn-neural-network.jpg');">
    <img src="https://laxmikants.github.io/img/main/cnn-neural-network.jpg" alt="">
  </a>
        <header>
          <h2><a href="/blog/convolutional-neural-networks/">Convolutional Neural Networks</a></h2>
          <time class="published" datetime="2021-01-29 00:00:00 &#43;0000 UTC">January 29, 2021</time>
        </header>
      </article>
      
      <article class="mini-post">
          <a href="/blog/neural-network-using-make-moons-dataset/" class="image" style="--bg-image: url('https://laxmikants.github.io/img/main/nn_makemoons_dataset.jpg');">
    <img src="https://laxmikants.github.io/img/main/nn_makemoons_dataset.jpg" alt="">
  </a>
        <header>
          <h2><a href="/blog/neural-network-using-make-moons-dataset/">Neural Network using Make Moons dataset</a></h2>
          <time class="published" datetime="2020-12-10 00:00:00 &#43;0000 UTC">December 10, 2020</time>
        </header>
      </article>
      
      <article class="mini-post">
          <a href="/blog/single-layer-perceptron/" class="image" style="--bg-image: url('https://laxmikants.github.io/img/main/Single_Layer_Perceptron.jpg');">
    <img src="https://laxmikants.github.io/img/main/Single_Layer_Perceptron.jpg" alt="">
  </a>
        <header>
          <h2><a href="/blog/single-layer-perceptron/">Single Layer Perceptron</a></h2>
          <time class="published" datetime="2020-12-10 00:00:00 &#43;0000 UTC">December 10, 2020</time>
        </header>
      </article>
      
      
        <footer>
          <a href="/blog/" class="button">See More</a>
        </footer>
      

      
    </section>
  

  <div align="center">
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
      
    <ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-3804322353139756"
     data-ad-slot="3115024406"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
    <script>
        $(document).ready(function(){(adsbygoogle = window.adsbygoogle || []).push({})})
    </script>
  </div>
  
  
    
      <section id="categories">
        
   
        <header>
          <h1><a href="/categories">Categories</a></h1>
        </header>
        <ul>
          
          
          <li>
              <a href="/categories/python/">python<span class="count">14</span></a>
          
          <li>
              <a href="/categories/linear-regression/">linear-regression<span class="count">2</span></a>
          
          <li>
              <a href="/categories/logistic-regression/">logistic-regression<span class="count">2</span></a>
          
          <li>
              <a href="/categories/android/">android<span class="count">1</span></a>
          
          <li>
              <a href="/categories/big-data/">big-data<span class="count">1</span></a>
          
          <li>
              <a href="/categories/bigdata/">bigdata<span class="count">1</span></a>
          
          <li>
              <a href="/categories/classification/">classification<span class="count">1</span></a>
          
          <li>
              <a href="/categories/clustering/">clustering<span class="count">1</span></a>
          
          <li>
              <a href="/categories/convolutional-neural-networks/">convolutional-neural-networks<span class="count">1</span></a>
          
          <li>
              <a href="/categories/data-science/">data-science<span class="count">1</span></a>
          
          <li>
              <a href="/categories/decision-trees/">decision-trees<span class="count">1</span></a>
          
          <li>
              <a href="/categories/eda/">eda<span class="count">1</span></a>
          
          <li>
              <a href="/categories/exploring-dataset/">exploring-dataset<span class="count">1</span></a>
          
          <li>
              <a href="/categories/frequency-tables/">frequency-tables<span class="count">1</span></a>
          
          <li>
              <a href="/categories/hadoop/">hadoop<span class="count">1</span></a>
          
          <li>
              <a href="/categories/k-nearest-neighbours/">k-nearest-neighbours<span class="count">1</span></a>
          
          <li>
              <a href="/categories/machine-learning/">machine-learning<span class="count">1</span></a>
          
          <li>
              <a href="/categories/natural-language-processing/">natural-language-processing<span class="count">1</span></a>
          
          <li>
              <a href="/categories/neural-network/">neural-network<span class="count">1</span></a>
          
          <li>
              <a href="/categories/neural-networks/">neural-networks<span class="count">1</span></a>
          
          <li>
              <a href="/categories/numeric-data/">numeric-data<span class="count">1</span></a>
          
          <li>
              <a href="/categories/pandas/">pandas<span class="count">1</span></a>
          
          <li>
              <a href="/categories/react-native/">react-native<span class="count">1</span></a>
          
          <li>
              <a href="/categories/reactjs/">reactjs<span class="count">1</span></a>
          
          <li>
              <a href="/categories/single-layer-perceptron/">single-layer-perceptron<span class="count">1</span></a>
          
          <li>
              <a href="/categories/stock-market/">stock-market<span class="count">1</span></a>
          
          <li>
              <a href="/categories/support-vector-machines/">support-vector-machines<span class="count">1</span></a>
          
          <li>
              <a href="/categories/text-analytics/">text-analytics<span class="count">1</span></a>
          
          <li>
              <a href="/categories/time-series/">time-series<span class="count">1</span></a>
          
          <li>
              <a href="/categories/web-scrapping/">web-scrapping<span class="count">1</span></a>
          
          </li>
        </ul>
        <div align="center">
          <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
          
            <ins class="adsbygoogle"
             style="display:block"
             data-ad-client="ca-pub-3804322353139756"
             data-ad-slot="5736252654"
             data-ad-format="auto"
             data-full-width-responsive="true"></ins>
          <script>
            $(document).ready(function(){(adsbygoogle = window.adsbygoogle || []).push({})})
          </script>             
        </div>             
      </section>
    
  

  
    <section id="mini-bio">
      <header>
        <h1>About</h1>
      </header>
      <p>about</p>
      <footer>
        <a href="/about" class="button">Learn More</a>
      </footer>
    </section>
  
</section>

      <footer id="site-footer">
  
  
      <ul class="socnet-icons">
        

        <li><a href="//github.com/laxmikants" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>











<li><a href="//linkedin.com/in/laxmikantsoni09" target="_blank" rel="noopener" title="LinkedIn" class="fab fa-linkedin"></a></li>




<li><a href="//facebook.com/laxmikantsoni09" target="_blank" rel="noopener" title="Facebook" class="fab fa-facebook"></a></li>










<li><a href="//twitter.com/laxmikantsoni09" target="_blank" rel="noopener" title="Twitter" class="fab fa-twitter"></a></li>













      </ul>
  
  <p class="copyright">
    © 2021 Data Science Posts and Resources
      <br>
  </p>
</footer>

      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/highlight.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/python.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script><script src="//code.jquery.com/jquery-3.5.1.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.js"></script>
    <script src="//unpkg.com/lunr/lunr.js"></script><script src="/js/bundlecdn.min.daca826145adfcf5004b4b16d74010eff217a0382fad56b874f5630927a20c4e.js" integrity="sha256-2sqCYUWt/PUAS0sW10AQ7/IXoDgvrVa4dPVjCSeiDE4="></script>
    <script src="/js/add-on.js"></script>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-155379268-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

    </div>
  </body>
  
</html>

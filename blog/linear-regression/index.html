<!doctype html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
<script data-ad-client="ca-pub-3804322353139756" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script async src = "https://www.googletagmanager.com/gtag/js?id=UA-155379268-1"></script>
<script>
 window.dataLayer = window.dataLayer || [];
 function gtag(){dataLayer.push(arguments);}
 gtag('js',new Date());
 
 gtag('config','UA-155379268-1');

</script>  
  <meta charset="utf-8">
<title>Decision Trees - Data Science Posts and Resources | Laxmikant Soni</title>
<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name= "keywords" content="Predictive, Business, Data, Analytics, Machine Learning, Mining, Python, Intelligence, Big, Modeling, Data Science, Integration, Visualization">

<meta name="robots" content="index">

<meta name="description" content="Articles and Posts on Data Science, Machine Learning and Analytics">



<meta name="generator" content="Hugo 0.74.3" /><meta itemprop="name" content="Decision Trees">
<meta itemprop="description" content="Decision trees area tree-like tool which can be used to represent a cause and its effect">
<meta itemprop="datePublished" content="2020-04-05T00:00:00+00:00" />
<meta itemprop="dateModified" content="2020-04-05T00:00:00+00:00" />
<meta itemprop="wordCount" content="1764">



<meta itemprop="keywords" content="Decision Trees," />
<meta property="og:title" content="Decision Trees" />
<meta property="og:description" content="Decision trees area tree-like tool which can be used to represent a cause and its effect" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://laxmikants.github.io/blog/linear-regression/" />
<meta property="article:published_time" content="2020-04-05T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-04-05T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Decision Trees"/>
<meta name="twitter:description" content="Decision trees area tree-like tool which can be used to represent a cause and its effect"/>
<meta name="twitter:site" content="@laxmikantsoni09"/>
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/monokai.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css">
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway:400,800,900|Source+Sans+Pro:400,700">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css">
        <script src="https://kit.fontawesome.com/be54eb011a.js" crossorigin="anonymous"></script>
      <script async   src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="/css/main.min.1f17d6560afe48677306aa7452f1c061799e35dbd1cc697cb7d6d22eed0b6b9b.css" integrity="sha256-HxfWVgr&#43;SGdzBqp0UvHAYXmeNdvRzGl8t9bSLu0La5s="><link rel="stylesheet" href="/css/add-on.css">

<title>Decision Trees : Data Science Posts and Resources</title>

<meta property="og:title" content="Decision Trees">
<meta property="og:site_name" content="Data Science Posts and Resources">
<meta property="og:url" content="https://laxmikants.github.io/blog/linear-regression/">
<meta property="og:type" content="article">
<meta property="og:locale" content="en">
<meta property="og:description" content="Decision trees area tree-like tool which can be used to represent a cause and its effect">
<meta name="description" content="Decision trees area tree-like tool which can be used to represent a cause and its effect">
<meta property="og:updated_time" content="2020-04-05T00:00:00Z">
<meta property="fb:app_id" content="428818034507005">
<meta name="author" content="Laxmikant Soni">
<meta property="article:author" content="https://laxmikants.github.io">
<meta property="article:published_time" content="2020-04-05T00:00:00Z">
<meta property="article:modified_time" content="2020-04-05T00:00:00Z">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "Decision Trees",
  "alternativeHeadline": "Decision Trees",
  "url": "https://laxmikants.github.io/blog/linear-regression/",
  "image": "https://laxmikants.github.io/",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://laxmikants.github.io/blog/linear-regression/"
  },
  "description": "Decision trees area tree-like tool which can be used to represent a cause and its effect",
  "author": {
    "@type": "Person",
    "name": "Laxmikant Soni"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data Science Posts and Resources",
    "logo": {
      "@type": "ImageObject",
      "url": "https://laxmikants.github.io/"
    }
  },
  "datePublished": "2020-04-05T00:00:00Z",
  "dateModified": "2020-04-05T00:00:00Z",
  "articleBody": "\r\n\u003cscript src=\"/rmarkdown-libs/header-attrs/header-attrs.js\"\u003e\u003c/script\u003e\r\n\r\n\r\n\u003cdiv id=\"decision-trees\" class=\"section level1\"\u003e\r\n\u003ch1\u003eDecision trees\u003c/h1\u003e\r\n\u003cp\u003eDecision trees area tree-like tool which can be used to represent a cause and its effect. In Machine Learning Decision trees are a type of Supervised machine learning where data is split according to given parameter while constructing a tree to solve a given problem. In decision tree there is a predictor variable and target variable or the desired output. The predictor variable could be anything such as technical indicators etc and the target variable could be desired output for example whether to invest in a given financial security or not.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"how-it-works\" class=\"section level1\"\u003e\r\n\u003ch1\u003eHow it works\u003c/h1\u003e\r\n\u003cp\u003eA decision basically gives flowchart of how to make some decision.You have some dependent variable, like whether to buy the stock depending on factors like RSI, MACD etc. When you have a decision like that that depends on multiple attributes or multiple variables, a decision tree could be a good choice.\u003c/p\u003e\r\n\u003cp\u003eThere are many different aspects of the weather that might influence my decision to buy a given stock. It might have to do with the stock closing prise today, the RSI,MACD, EMA etc. A decision tree can look at all these different features of the stock, and decide what are the thresholds. What are those factors which affects the stock movement.\u003c/p\u003e\r\n\u003cp\u003eFor example, Factors affecting the stock movement is shown by using decision tree\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"understanding-decision-tree\" class=\"section level1\"\u003e\r\n\u003ch1\u003eUnderstanding Decision Tree\u003c/h1\u003e\r\n\u003cp\u003eAt each iteration of the decision tree flowchart, we find the property that we can partition our data on which minimizes the entropy of the data at the next step. So we have a resulting set of classes in this case “BUY” or “SELL”, and we want to choose the attribute decision at that step that will minimize the entropy at the next step. So we just walk down the tree, minimize entropy at each step by choosing the right attribute to decide on, and we keep on going until we run out.\u003c/p\u003e\r\n\u003cp\u003e\u003cimg src=\"/blog/2020-04-05-DecisionTrees-29_files/figure-html/unnamed-chunk-2-1.png\" width=\"672\" /\u003e\u003c/p\u003e\r\n\u003cblockquote\u003e\r\n\u003cp\u003eThe decision to buy the stock in majority of the cases is dependent on the stock fundamentals. In 64% of the cases buying of stock is supported by decision tree if the fundamentals of the company are strong.\u003c/p\u003e\r\n\u003c/blockquote\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"decision-tree-example\" class=\"section level1\"\u003e\r\n\u003ch1\u003eDecision tree example\u003c/h1\u003e\r\n\u003cp\u003eLet’s say I want to build a system that will automatically predicts the stock movement at the end of the day based on the opening price of the stock. Given a stock the system should decide whether that stock is going to have upward movment or downward movement during intraday trading so that investor can make a decision of investment in that stock.\u003c/p\u003e\r\n\u003cp\u003eSo let’s make some totally fabricated stock data that we’re going to use in this example:\u003c/p\u003e\r\n\u003cp\u003eIn the preceding table, we have stock prices along with technical indicators. We are going to pick some attributes that we think might be interesting or helpful to predict whether or not they can predict movement of the stock (UP or DOWN). How much is William %R ? What is exponential moving average ? What is the value of stochastic momentun index ? Is the stock overbought/oversold (RSI) ? Depending on the factors which affect the stock price we can predict whether it will go up or down.\u003c/p\u003e\r\n\u003cp\u003eNow, obviously there’s a lot of information that isn’t in this model that might be very important, but the decision tree that we train from this data might actually be useful in doing an initial pass at weeding out some candidates. What we end up with might be a tree that looks like the following:\u003c/p\u003e\r\n\u003cp\u003e\u003cimg src=\"/blog/2020-04-05-DecisionTrees-29_files/figure-html/unnamed-chunk-3-1.png\" width=\"672\" /\u003e\r\nSo it just turns out that in totally fabricated data, if the william %R (WPR) is below 0.17 then the stock will go UP. So the first questioon decision tree related to WPR, i.e if WPR is above 0.17 then go to left. At node 2 check if the exponential moving average is above 9.7, if it is then we end up at the leaf node predicted value to stock going down. If at node 2 exponential moving average is below 9.7 then go to node 5 and check the value of stochastic mementun index, if it is above 42 then go to right at leaf node 11 having stock predicted value of UP and so on.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"walking-through-a-decision-tree\" class=\"section level1\"\u003e\r\n\u003ch1\u003eWalking through a decision tree\u003c/h1\u003e\r\n\u003cp\u003eSo that’s how we walk through the results of a decision tree. It’s just like going through a flowchart, and it’s kind of awesome that an algorithm can produce this for us. The algorithm itself is actually very simple. Let me explain how the algorithm works.\u003c/p\u003e\r\n\u003cp\u003eAt each step of the decision tree flowchart, we see the attribute that we can partition our data on that minimizes the entropy of the data at the next step. So we have a resulting set of classifications in this case UP or DOWN, and we want to choose the attribute decision at that step that will minimize the entropy at the next step.\u003c/p\u003e\r\n\u003cblockquote\u003e\r\n\u003cp\u003eEntropy meausures the level of impurity in a group. The group having minimum entropy is helps determinie attribute most useful for descriminating between classes to be learned.\u003c/p\u003e\r\n\u003c/blockquote\u003e\r\n\u003cp\u003eAt each step we want to make all of the remaining choices result in either as many downs or as many up decisions as possible. We want to make that data more and more uniform so as we work our way down the flowchart, and we ultimately end up with a set of candidates that are either all UPS or all DOWNS so we can classify into yes/no decisions on a decision tree. So we just walk down the tree, minimize entropy at each step by choosing the right attribute to decide on, and we keep on going until we run out.\u003c/p\u003e\r\n\u003cp\u003eThere’s a fancy name for this algorithm. It’s called ID3 ( Iterative Dichotomiser 3 ). It is what’s known as a greedy algorithm. So as it goes down the tree, it just picks the attribute that will minimize entropy at that point. Now that might not actually result in an optimal tree that minimizes the number of choices that you have to make, but it will result in a tree that works, given the data that you gave it.\u003c/p\u003e\r\n\u003cp\u003eThe tree starts at the top and finds the best data to split into nodes. It does this by recursive binary splitting using either the Gini index or cross-entropy measure. The Gini index is defined as:\u003c/p\u003e\r\n\u003cp\u003e\u003cspan class=\"math display\"\u003e\\[G = \\sum_{k=1}^K \\hat{p}_{mk}(1 - \\hat{p}_{mk})\\]\u003c/span\u003e\u003c/p\u003e\r\n\u003cp\u003eand is also referred as a measure of node purity, i.e., a smaller value indicates a node contains observations primarily from a single class.\u003c/p\u003e\r\n\u003cp\u003eCross-entropy is similar to the Gini index in that it will take a small value if the node is pure. It is defined as:\u003c/p\u003e\r\n\u003cp\u003e\u003cspan class=\"math display\"\u003e\\[D = -\\sum_{k=1}^K \\hat{p}_{mk}log\\hat{p}_{mk}\\]\u003c/span\u003e\u003c/p\u003e\r\n\u003cp\u003eThe Gini index and Cross-entropy measures dictate when a node split will occur in order to keep each node as pure as possible to reduce the total value of the Gini index or cross-entropy measures.\u003c/p\u003e\r\n\u003cp\u003eStart at the top, or root of the tree. 57% of the stock movements will have an DOWN movement with 43% going in UPWARD direction. If the William % R rating was equal to or above 0.17, we look left, otherwise you move right. To the right, we see only 17% of values having WPR below 0.17 will have UP movement, so the overall terminal node ends with the bucket having UP stock movement and so on.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"evaluating-the-decision-tree-model\" class=\"section level1\"\u003e\r\n\u003ch1\u003eEvaluating the Decision Tree Model\u003c/h1\u003e\r\n\u003cpre\u003e\u003ccode\u003e       DOWN  UP\r\n DOWN  231  78\r\n UP     87 220\r\n \u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eThe decision tree model have accuracy of 73%.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"benefit-of-decision-tree-over-neural-network\" class=\"section level1\"\u003e\r\n\u003ch1\u003eBenefit of Decision Tree over Neural network\u003c/h1\u003e\r\n\u003cp\u003eThe benefit of using Decision trees over Neural Network are:\u003c/p\u003e\r\n\u003col style=\"list-style-type: decimal\"\u003e\r\n\u003cli\u003e\u003cp\u003eThey are easy to program.\u003c/p\u003e\u003c/li\u003e\r\n\u003cli\u003e\u003cp\u003eThe top nodes in the tree will give the information about what data affects the prediction.\u003c/p\u003e\u003c/li\u003e\r\n\u003cli\u003e\u003cp\u003eTrees are interpretable and provide visual representation of data.\u003c/p\u003e\u003c/li\u003e\r\n\u003cli\u003e\u003cp\u003ePerforms faster than Neural Networks after training.\u003c/p\u003e\u003c/li\u003e\r\n\u003c/ol\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"issues-with-dt\" class=\"section level1\"\u003e\r\n\u003ch1\u003eIssues with DT\u003c/h1\u003e\r\n\u003cp\u003eNow one problem with decision trees is that they are very prone to overfitting, so you can end up with a decision tree that works well for the data that we trained it on, but it might not be that great for actually predicting the correct classification for values that it hasn’t seen before. Decision trees are all about arriving at the right decision for the training data that we gave it, but maybe we didn’t really take into account the right attributes, maybe we didn’t give it enough of a representative sample of values to learn from. This can result in real problems.\u003c/p\u003e\r\n\u003cp\u003eSo to combat this issue, we use a technique called random forests, where the idea is that we sample the data that we train on, in different ways, for multiple different decision trees. Each decision tree takes a different random sample from our set of training data and constructs a tree from it. Then each resulting tree can vote on the right result.\u003c/p\u003e\r\n\u003cp\u003eNow that technique of randomly resampling our data with the same model is a term called bootstrap aggregating, or bagging. This is a form of what we call ensemble learning. The basic idea is that we have multiple trees, a forest of trees, each uses a random subsample of the data that we have to train on. Then each of these trees can vote on the final result, and that will help us combat overfitting for a given set of training data.\u003c/p\u003e\r\n\u003cp\u003eThe other thing random forests can do is actually restrict the number of attributes that it can choose, between at each stage, while it is trying to minimize the entropy as it goes. And we can randomly pick which attributes it can choose from at each level. So that also gives us more variation from tree to tree, and therefore we get more of a variety of algorithms that can compete with each other. They can all vote on the final result using slightly different approaches to arriving at the same answer.\u003c/p\u003e\r\n\u003cp\u003eSo that’s how random forests work. Basically, it is a forest of decision trees where they are drawing from different samples and also different sets of attributes at each stage that it can choose between.\u003c/p\u003e\r\n\u003c/div\u003e\r\n\u003cdiv id=\"summary\" class=\"section level1\"\u003e\r\n\u003ch1\u003eSummary\u003c/h1\u003e\r\n\u003cp\u003eWe have a better understanding of decision trees now; why they are being used frequently in predictive modeling, how they are created, and how they can be optimized for best results. Decision trees are a powerful tool for data scientists, but they must be handled with care. All the repetitive tasks are achieved by use of computers (hence the term machine learning), all aspects of the process must be overseen by an experienced data scientist in order to create the most accurate model.\u003c/p\u003e\r\n\u003c/div\u003e"
}
</script>

  

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-155379268-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>  
</head>

  <body>
    
      
    

<header id="site-header">
  <nav id="site-nav">
    <h1 class="nav-title">
      <a href="/" class="nav">
        
          Blog
        
      </a>
    </h1>
    <menu id="site-nav-menu" class="flyout-menu menu">
      
        
          
          <a href="/" class="nav link"><i class='fa fa-home'></i> Home</a>
        
      
        
          
          <a href="/blog/" class="nav link"><i class='far fa-newspaper'></i> Blog</a>
        
      
        
          
          <a href="/categories/" class="nav link"><i class='fas fa-sitemap'></i> Categories</a>
        
      
      <a href="#share-menu" class="nav link share-toggle"><i class="fas fa-share-alt">&nbsp;</i>Share</a>
      <a href="#search-input" class="nav link search-toggle"><i class="fas fa-search">&nbsp;</i>Search</a>
    </menu>
    <a href="#search-input" class="nav search-toggle"><i class="fas fa-search fa-2x">&nbsp;</i></a>
    <a href="#share-menu" class="nav share-toggle"><i class="fas fa-share-alt fa-2x">&nbsp;</i></a>
    <a href="#lang-menu" class="nav lang-toggle" lang="en">en</a>
    <a href="#site-nav" class="nav nav-toggle"><i class="fas fa-bars fa-2x"></i></a>
  </nav>
  <menu id="search" class="menu"><input id="search-input" class="search-input menu"></input><div id="search-results" class="search-results menu"></div></menu>
  <menu id="lang-menu" class="flyout-menu menu">
  <a href="#" lang="en" class="nav link active">English (en)</a>
  
    
      
    
  
</menu>

  
    <menu id="share-menu" class="flyout-menu menu">
      <h1>Share Post</h1>
      




  
    
    <a href="//twitter.com/share?text=Decision%20Trees&amp;url=https%3a%2f%2flaxmikants.github.io%2fblog%2flinear-regression%2f" target="_blank" rel="noopener" class="nav share-btn twitter">
        <p>Twitter</p>
      </a>
  

  
      <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2flaxmikants.github.io%2fblog%2flinear-regression%2f" target="_blank" rel="noopener" class="nav share-btn facebook">
        <p>Facebook</p>
        </a>
  


    </menu>
  
</header>

    <div id="wrapper">
      <section id="site-intro" >
  
  <header>
    <h1>Data Science Posts and Resources</h1>
  </header>
  <main>
    
  </main>
  
    <footer>
      <ul class="socnet-icons">
        

        <li><a href="//github.com/laxmikants" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>











<li><a href="//linkedin.com/in/laxmikantsoni09" target="_blank" rel="noopener" title="LinkedIn" class="fab fa-linkedin"></a></li>




<li><a href="//facebook.com/laxmikantsoni09" target="_blank" rel="noopener" title="Facebook" class="fab fa-facebook"></a></li>










<li><a href="//twitter.com/laxmikantsoni09" target="_blank" rel="noopener" title="Twitter" class="fab fa-twitter"></a></li>













      </ul>
    </footer>
  
</section>

      <main id="site-main">
        
  <article class="post">
    <header>
  <div class="title">
    
      <h2><a href="/blog/linear-regression/">Decision Trees</a></h2>
    
    
      <p>Decision trees area tree-like tool which can be used to represent a cause and its effect</p>
    
  </div>
  <div class="meta">
    <time datetime="2020-04-05 00:00:00 &#43;0000 UTC">April 5, 2020</time>
    <p>Laxmi K Soni</p>
    <p>9-Minute Read</p>
  </div>
</header>

    <div id="socnet-share">
      




  
    
    <a href="//twitter.com/share?text=Decision%20Trees&amp;url=https%3a%2f%2flaxmikants.github.io%2fblog%2flinear-regression%2f" target="_blank" rel="noopener" class="nav share-btn twitter">
        <p>Twitter</p>
      </a>
  

  
      <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2flaxmikants.github.io%2fblog%2flinear-regression%2f" target="_blank" rel="noopener" class="nav share-btn facebook">
        <p>Facebook</p>
        </a>
  


    </div>
    <div class="content">
      <a href="/blog/linear-regression/" class="image" style="--bg-image: url('https://laxmikants.github.io/img/main/decision-trees-11.jpg');">
    <img src="https://laxmikants.github.io/img/main/decision-trees-11.jpg" alt="">
  </a>
      
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="decision-trees" class="section level1">
<h1>Decision trees</h1>
<p>Decision trees area tree-like tool which can be used to represent a cause and its effect. In Machine Learning Decision trees are a type of Supervised machine learning where data is split according to given parameter while constructing a tree to solve a given problem. In decision tree there is a predictor variable and target variable or the desired output. The predictor variable could be anything such as technical indicators etc and the target variable could be desired output for example whether to invest in a given financial security or not.</p>
</div>
<div id="how-it-works" class="section level1">
<h1>How it works</h1>
<p>A decision basically gives flowchart of how to make some decision.You have some dependent variable, like whether to buy the stock depending on factors like RSI, MACD etc. When you have a decision like that that depends on multiple attributes or multiple variables, a decision tree could be a good choice.</p>
<p>There are many different aspects of the weather that might influence my decision to buy a given stock. It might have to do with the stock closing prise today, the RSI,MACD, EMA etc. A decision tree can look at all these different features of the stock, and decide what are the thresholds. What are those factors which affects the stock movement.</p>
<p>For example, Factors affecting the stock movement is shown by using decision tree</p>
</div>
<div id="understanding-decision-tree" class="section level1">
<h1>Understanding Decision Tree</h1>
<p>At each iteration of the decision tree flowchart, we find the property that we can partition our data on which minimizes the entropy of the data at the next step. So we have a resulting set of classes in this case “BUY” or “SELL”, and we want to choose the attribute decision at that step that will minimize the entropy at the next step. So we just walk down the tree, minimize entropy at each step by choosing the right attribute to decide on, and we keep on going until we run out.</p>
<p><img src="/blog/2020-04-05-DecisionTrees-29_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<blockquote>
<p>The decision to buy the stock in majority of the cases is dependent on the stock fundamentals. In 64% of the cases buying of stock is supported by decision tree if the fundamentals of the company are strong.</p>
</blockquote>
</div>
<div id="decision-tree-example" class="section level1">
<h1>Decision tree example</h1>
<p>Let’s say I want to build a system that will automatically predicts the stock movement at the end of the day based on the opening price of the stock. Given a stock the system should decide whether that stock is going to have upward movment or downward movement during intraday trading so that investor can make a decision of investment in that stock.</p>
<p>So let’s make some totally fabricated stock data that we’re going to use in this example:</p>
<p>In the preceding table, we have stock prices along with technical indicators. We are going to pick some attributes that we think might be interesting or helpful to predict whether or not they can predict movement of the stock (UP or DOWN). How much is William %R ? What is exponential moving average ? What is the value of stochastic momentun index ? Is the stock overbought/oversold (RSI) ? Depending on the factors which affect the stock price we can predict whether it will go up or down.</p>
<p>Now, obviously there’s a lot of information that isn’t in this model that might be very important, but the decision tree that we train from this data might actually be useful in doing an initial pass at weeding out some candidates. What we end up with might be a tree that looks like the following:</p>
<p><img src="/blog/2020-04-05-DecisionTrees-29_files/figure-html/unnamed-chunk-3-1.png" width="672" />
So it just turns out that in totally fabricated data, if the william %R (WPR) is below 0.17 then the stock will go UP. So the first questioon decision tree related to WPR, i.e if WPR is above 0.17 then go to left. At node 2 check if the exponential moving average is above 9.7, if it is then we end up at the leaf node predicted value to stock going down. If at node 2 exponential moving average is below 9.7 then go to node 5 and check the value of stochastic mementun index, if it is above 42 then go to right at leaf node 11 having stock predicted value of UP and so on.</p>
</div>
<div id="walking-through-a-decision-tree" class="section level1">
<h1>Walking through a decision tree</h1>
<p>So that’s how we walk through the results of a decision tree. It’s just like going through a flowchart, and it’s kind of awesome that an algorithm can produce this for us. The algorithm itself is actually very simple. Let me explain how the algorithm works.</p>
<p>At each step of the decision tree flowchart, we see the attribute that we can partition our data on that minimizes the entropy of the data at the next step. So we have a resulting set of classifications in this case UP or DOWN, and we want to choose the attribute decision at that step that will minimize the entropy at the next step.</p>
<blockquote>
<p>Entropy meausures the level of impurity in a group. The group having minimum entropy is helps determinie attribute most useful for descriminating between classes to be learned.</p>
</blockquote>
<p>At each step we want to make all of the remaining choices result in either as many downs or as many up decisions as possible. We want to make that data more and more uniform so as we work our way down the flowchart, and we ultimately end up with a set of candidates that are either all UPS or all DOWNS so we can classify into yes/no decisions on a decision tree. So we just walk down the tree, minimize entropy at each step by choosing the right attribute to decide on, and we keep on going until we run out.</p>
<p>There’s a fancy name for this algorithm. It’s called ID3 ( Iterative Dichotomiser 3 ). It is what’s known as a greedy algorithm. So as it goes down the tree, it just picks the attribute that will minimize entropy at that point. Now that might not actually result in an optimal tree that minimizes the number of choices that you have to make, but it will result in a tree that works, given the data that you gave it.</p>
<p>The tree starts at the top and finds the best data to split into nodes. It does this by recursive binary splitting using either the Gini index or cross-entropy measure. The Gini index is defined as:</p>
<p><span class="math display">\[G = \sum_{k=1}^K \hat{p}_{mk}(1 - \hat{p}_{mk})\]</span></p>
<p>and is also referred as a measure of node purity, i.e., a smaller value indicates a node contains observations primarily from a single class.</p>
<p>Cross-entropy is similar to the Gini index in that it will take a small value if the node is pure. It is defined as:</p>
<p><span class="math display">\[D = -\sum_{k=1}^K \hat{p}_{mk}log\hat{p}_{mk}\]</span></p>
<p>The Gini index and Cross-entropy measures dictate when a node split will occur in order to keep each node as pure as possible to reduce the total value of the Gini index or cross-entropy measures.</p>
<p>Start at the top, or root of the tree. 57% of the stock movements will have an DOWN movement with 43% going in UPWARD direction. If the William % R rating was equal to or above 0.17, we look left, otherwise you move right. To the right, we see only 17% of values having WPR below 0.17 will have UP movement, so the overall terminal node ends with the bucket having UP stock movement and so on.</p>
</div>
<div id="evaluating-the-decision-tree-model" class="section level1">
<h1>Evaluating the Decision Tree Model</h1>
<pre><code>       DOWN  UP
 DOWN  231  78
 UP     87 220
 </code></pre>
<p>The decision tree model have accuracy of 73%.</p>
</div>
<div id="benefit-of-decision-tree-over-neural-network" class="section level1">
<h1>Benefit of Decision Tree over Neural network</h1>
<p>The benefit of using Decision trees over Neural Network are:</p>
<ol style="list-style-type: decimal">
<li><p>They are easy to program.</p></li>
<li><p>The top nodes in the tree will give the information about what data affects the prediction.</p></li>
<li><p>Trees are interpretable and provide visual representation of data.</p></li>
<li><p>Performs faster than Neural Networks after training.</p></li>
</ol>
</div>
<div id="issues-with-dt" class="section level1">
<h1>Issues with DT</h1>
<p>Now one problem with decision trees is that they are very prone to overfitting, so you can end up with a decision tree that works well for the data that we trained it on, but it might not be that great for actually predicting the correct classification for values that it hasn’t seen before. Decision trees are all about arriving at the right decision for the training data that we gave it, but maybe we didn’t really take into account the right attributes, maybe we didn’t give it enough of a representative sample of values to learn from. This can result in real problems.</p>
<p>So to combat this issue, we use a technique called random forests, where the idea is that we sample the data that we train on, in different ways, for multiple different decision trees. Each decision tree takes a different random sample from our set of training data and constructs a tree from it. Then each resulting tree can vote on the right result.</p>
<p>Now that technique of randomly resampling our data with the same model is a term called bootstrap aggregating, or bagging. This is a form of what we call ensemble learning. The basic idea is that we have multiple trees, a forest of trees, each uses a random subsample of the data that we have to train on. Then each of these trees can vote on the final result, and that will help us combat overfitting for a given set of training data.</p>
<p>The other thing random forests can do is actually restrict the number of attributes that it can choose, between at each stage, while it is trying to minimize the entropy as it goes. And we can randomly pick which attributes it can choose from at each level. So that also gives us more variation from tree to tree, and therefore we get more of a variety of algorithms that can compete with each other. They can all vote on the final result using slightly different approaches to arriving at the same answer.</p>
<p>So that’s how random forests work. Basically, it is a forest of decision trees where they are drawing from different samples and also different sets of attributes at each stage that it can choose between.</p>
</div>
<div id="summary" class="section level1">
<h1>Summary</h1>
<p>We have a better understanding of decision trees now; why they are being used frequently in predictive modeling, how they are created, and how they can be optimized for best results. Decision trees are a powerful tool for data scientists, but they must be handled with care. All the repetitive tasks are achieved by use of computers (hence the term machine learning), all aspects of the process must be overseen by an experienced data scientist in order to create the most accurate model.</p>
</div>

    </div>
    <footer>
      <div class="stats">
  <ul class="categories">
    
      
        
          <li><a class="article-terms-link" href="/categories/decision-trees/">Decision Trees</a></li>
        
      
    
  </ul>
  <ul class="tags">
    
      
        
          <li><a class="article-terms-link" href="/tags/decision-trees/">Decision Trees</a></li>
        
      
    
  </ul>
</div>

    </footer>
  </article>
  
    


  <article class="post">
    
    <div>
      <h2 id="say-something">Say Something</h2>
        <form id="comment-form" class="new-comment" method="POST">
          
          <h3 class='reply-notice hidden'>
            <span class='reply-name'></span>
          </h3>

          
          <input type="hidden" name="options[entryId]" value="38407da4bb9470a1a27d700a3e225850">
          <input type='hidden' name='fields[replyThread]' value=''>
          <input type='hidden' name='fields[replyID]' value=''>
          <input type='hidden' name='fields[replyName]' value=''>

          
          <input required name='fields[name]' type='text' placeholder='Your Name'>
          <input name='fields[website]' type='text' placeholder='Your Website'>
          <input required name='fields[email]' type='email' placeholder='Your Email'>
          <textarea required name='fields[body]' placeholder='Your Message' rows='10'></textarea>

          
          

          
          <div class='submit-notice'>
            <strong class='submit-notice-text submit-success hidden'>Thanks for your comment! It will be shown on the site once it has been approved.</strong>
            <strong class='submit-notice-text submit-failed hidden'>Sorry, there was an error with your submission. Please make sure all required fields have been completed and try again.</strong>
          </div>

          
          <input type='submit' value='Submit' class='button'>
          <input type='submit' value='Submitted' class='hidden button' disabled>
          <input type='reset' value='Reset' class='button'>
        </form>
    </div>

    
    <div>
      <h2>Comments</h2><p>Nothing yet.</p>
      
    </div>
  </article>


  
  <div class="pagination">
    
      <a href="/blog/support-vector-machines/" class="button left"><span>Support Vector Machines</span></a>
    
    
      <a href="/blog/applying-logistic-regression/" class="button right"><span>Applying Logistic Regression</span></a>
    
  </div>

      </main>
      <section id="site-sidebar">
  
    <section id="recent-posts">
      <header>
        <h1>Recent Posts</h1>
      </header>
      
      <article class="mini-post">
          <a href="/blog/react-js/" class="image" style="--bg-image: url('https://laxmikants.github.io/img/main/2020-06-28-React-JS-32.jpg');">
    <img src="https://laxmikants.github.io/img/main/2020-06-28-React-JS-32.jpg" alt="">
  </a>
        <header>
          <h2><a href="/blog/react-js/">React- JS</a></h2>
          <time class="published" datetime="2020-06-28 00:00:00 &#43;0000 UTC">June 28, 2020</time>
        </header>
      </article>
      
      <article class="mini-post">
          <a href="/blog/neural-networks/" class="image" style="--bg-image: url('https://laxmikants.github.io/img/main/Neural-Networks-32.jpg');">
    <img src="https://laxmikants.github.io/img/main/Neural-Networks-32.jpg" alt="">
  </a>
        <header>
          <h2><a href="/blog/neural-networks/">Neural Networks</a></h2>
          <time class="published" datetime="2020-05-15 00:00:00 &#43;0000 UTC">May 15, 2020</time>
        </header>
      </article>
      
      <article class="mini-post">
          <a href="/blog/clustering/" class="image" style="--bg-image: url('https://laxmikants.github.io/img/main/Clustering-31.jpg');">
    <img src="https://laxmikants.github.io/img/main/Clustering-31.jpg" alt="">
  </a>
        <header>
          <h2><a href="/blog/clustering/">Clustering</a></h2>
          <time class="published" datetime="2020-05-05 00:00:00 &#43;0000 UTC">May 5, 2020</time>
        </header>
      </article>
      
      
        <footer>
          <a href="/blog/" class="button">See More</a>
        </footer>
      
    </section>
  

  
    
      <section id="categories">
        <header>
          <h1><a href="/categories">categories</a></h1>
        </header>
        <ul>
          
          
          <li>
              <a href="/categories/python/">python<span class="count">13</span></a>
          
          <li>
              <a href="/categories/linear-regression/">linear-regression<span class="count">2</span></a>
          
          <li>
              <a href="/categories/logistic-regression/">logistic-regression<span class="count">2</span></a>
          
          <li>
              <a href="/categories/big-data/">big-data<span class="count">1</span></a>
          
          <li>
              <a href="/categories/bigdata/">bigdata<span class="count">1</span></a>
          
          <li>
              <a href="/categories/classification/">classification<span class="count">1</span></a>
          
          <li>
              <a href="/categories/clustering/">clustering<span class="count">1</span></a>
          
          <li>
              <a href="/categories/data-science/">data-science<span class="count">1</span></a>
          
          <li>
              <a href="/categories/decision-trees/">decision-trees<span class="count">1</span></a>
          
          <li>
              <a href="/categories/eda/">eda<span class="count">1</span></a>
          
          <li>
              <a href="/categories/exploring-dataset/">exploring-dataset<span class="count">1</span></a>
          
          <li>
              <a href="/categories/frequency-tables/">frequency-tables<span class="count">1</span></a>
          
          <li>
              <a href="/categories/hadoop/">hadoop<span class="count">1</span></a>
          
          <li>
              <a href="/categories/machine-learning/">machine-learning<span class="count">1</span></a>
          
          <li>
              <a href="/categories/natural-language-processing/">natural-language-processing<span class="count">1</span></a>
          
          <li>
              <a href="/categories/neural-networks/">neural-networks<span class="count">1</span></a>
          
          <li>
              <a href="/categories/numeric-data/">numeric-data<span class="count">1</span></a>
          
          <li>
              <a href="/categories/pandas/">pandas<span class="count">1</span></a>
          
          <li>
              <a href="/categories/slides/">slides<span class="count">1</span></a>
          
          <li>
              <a href="/categories/stock-market/">stock-market<span class="count">1</span></a>
          
          <li>
              <a href="/categories/support-vector-machines/">support-vector-machines<span class="count">1</span></a>
          
          <li>
              <a href="/categories/text-analytics/">text-analytics<span class="count">1</span></a>
          
          <li>
              <a href="/categories/time-series/">time-series<span class="count">1</span></a>
          
          </li>
        </ul>
      </section>
    
  

  
    <section id="mini-bio">
      <header>
        <h1>About</h1>
      </header>
      <p>about</p>
      <footer>
        <a href="/about" class="button">Learn More</a>
      </footer>
    </section>
  
</section>

      <footer id="site-footer">
  
      <ul class="socnet-icons">
        

        <li><a href="//github.com/laxmikants" target="_blank" rel="noopener" title="GitHub" class="fab fa-github"></a></li>











<li><a href="//linkedin.com/in/laxmikantsoni09" target="_blank" rel="noopener" title="LinkedIn" class="fab fa-linkedin"></a></li>




<li><a href="//facebook.com/laxmikantsoni09" target="_blank" rel="noopener" title="Facebook" class="fab fa-facebook"></a></li>










<li><a href="//twitter.com/laxmikantsoni09" target="_blank" rel="noopener" title="Twitter" class="fab fa-twitter"></a></li>













      </ul>
  
  <p class="copyright">
    © 2020 Data Science Posts and Resources
      <br>
  </p>
</footer>
<a id="back-to-top" href="#" class="fas fa-arrow-up fa-2x"></a>

      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/highlight.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/python.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script><script src="//code.jquery.com/jquery-3.5.1.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.js"></script>
    <script src="//unpkg.com/lunr/lunr.js"></script><script src="/js/bundlecdn.min.f2add8c14b1f0b4ba32b812ada31b9a7ae32730960754ccf519f6d49f5da77a1.js" integrity="sha256-8q3YwUsfC0ujK4Eq2jG5p64ycwlgdUzPUZ9tSfXad6E="></script>
    <script src="/js/add-on.js"></script>
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-155379268-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

    </div>
  </body>
</html>

---
title: "Linear Regression"
author: Laxmi K Soni 
description: "A linear equation that models a function such that if we give any `x` to it, it will predict a value `y` , where both `x and y` are input and output variables respectively"
slug: Linear Regression
date: 2020-03-12
lastmod: 2020-03-12
categories: ["Linear Regression"]
tags: ["Linear Regression"]
Summary: A linear equation that models a function such that if we give any `x` to it, it will predict a value `y` , where both `x and y` are input and output variables respectively
subtitle: Linear Regression
featured: "img/main/linear_regression-10.jpg"
output:
  html_document:
    highlight: tango
    theme: flatly
    toc: no
    toc_float: no

---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<div id="definition" class="section level2">
<h2>Definition</h2>
<p>A linear equation that models a function such that if we give any <code>x</code> to it, it will predict a value <code>y</code> , where both <code>x and y</code> are input and output variables respectively. These are numerical and continous values.
It is the most simple and well known algorithm used in machine learning.</p>
</div>
<div id="flowchart" class="section level2">
<h2>Flowchart</h2>
<p align="center">
<img src = '/img/main/Linear_Reg_Flowchart.png'>
</p>
<p><br></p>
<p>The above Flowchart represents that we choose our training set, feed it to an algorithm, it will learn the patterns and will output a function called <code>Hypothesis function 'H(x)'</code>. We then give any <code>x</code> value to that function and it will output an estimated <code>y</code> value for it.</p>
<p>For historical reasons, this function <code>H(x)</code> is called <code>hypothesis function.</code></p>
</div>
<div id="cost-function" class="section level2">
<h2>Cost Function</h2>
<p>The best fit line to our data will be where we have least distance between the <code>predicted 'y' value</code> and <code>trained 'y' value</code>.</p>
</div>
<div id="formula-for-cost-function" class="section level2">
<h2>Formula for Cost Function</h2>
<p align="center">
<img src = '/img/main/MSE.png'>
</p>
<blockquote>
<p>Where :
- h(x<sub>i</sub>) ðŸ‘‰ hypothesis function
- y<sub>i</sub> ðŸ‘‰ actual values of <code>y</code>
- 1/m ðŸ‘‰ gives Mean of Squared Errors
- 1/2 ðŸ‘‰ Mean is halved as a convenience for the computation of the <code>Gradient Descent</code>.</p>
</blockquote>
<pre class="r"><code>computeCost &lt;- function (X, y, theta){
        # number of training examples
        m &lt;- length(y);
        # need to return
        J &lt;- 0;
        
        predictions &lt;-  X %*% theta;
        sqerrors = (predictions - y)^2;
        J = 1/(2*m)* sum(sqerrors);
        
        J
    }</code></pre>
This formula inputs the sum of the distances between <i><code>predicted values</code> and <code>actual values</code> of training set, does sqaure it and take the average and multiply it by 0.5</i>
<br>
<br>
This cost function is also called as <code>Squared Error Function</code> or <code>Mean Squared Error</code>.
<br>
<br>
Why do we take squares of the errorâ€™s?<br>
The <code>MSE</code> function is commonly used and is a reasonable choice and works well for most Regression problems.
<br>
<br>
Letâ€™s subsititute <code>MSE</code> function to function <code>J</code> :
<p align="center">
<img src = '/img/main/MSE1.png'>
</p>
<p><br>
<br></p>
</div>
<div id="gradient-descent" class="section level2">
<h2>Gradient Descent</h2>
<p>So now we have our hypothesis function and we have a way of measuring how well it fits into the data. Now we need to estimate the parameters in the hypothesis function. Thatâ€™s where <code>Gradient Descent</code> comes in.<br>
<code>Gradient Descent</code> is used to minimize the cost function <code>J</code>, minimizing <code>J</code> is same as minimizing <code>MSE</code> to get best possible fit line to our data.</p>
<p><span class="math display">\[\displaystyle \min_{\theta_0,\theta_1}\frac{1}{2m}\sum_{i=1}^{m} \left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2\]</span></p>
</div>
<div id="formula-for-gradient-descent" class="section level2">
<h2>Formula for Gradient Descent</h2>
<p align="center">
<img src = '/img/main/Gradient_Descent.PNG'>
</p>
<blockquote>
<p>Where :
- <code>:=</code> Is the Assignment Operator
- <code>Î±</code> is <code>Alpha</code>, itâ€™s the number which is called learning rate. If its too high it may fail to converge and if too low then descending will be slow.
- â€˜Î¸<sub>j</sub>â€™ Taking Gradient Descent of a feature or a column of a dataset.
- âˆ‚/(âˆ‚Î¸<sub>j</sub>) J(Î¸<sub>0</sub>,Î¸<sub>1</sub>) Taking partial derivative of <code>MSE</code> cost function.</p>
</blockquote>
<p><br></p>
<pre class="r"><code>    gradientDescent &lt;- function(X, y, theta, alpha, num_iters){
        m &lt;- length(y);  
        J_history = rep(0, num_iters);
        for (iter in 1:num_iters){
            predictions &lt;-  X %*% theta;
            updates = t(X) %*% (predictions - y);
            theta = theta - alpha * (1/m) * updates;
            J_history[iter] &lt;- computeCost(X, y, theta);
        }
        list(&quot;theta&quot; = theta, &quot;J_history&quot; = J_history)  
    }</code></pre>
<p><strong>Now Letâ€™s apply Gradient Descend to minmize our <code>MSE</code> function.</strong>
<br>
In order to apply <code>Gradient Descent</code>, we need to figure out the partial derivative term.<br>
So lets solve partial derivative of cost function <code>J</code>.</p>
<p><br></p>
<p align="center">
<img src = '/img/main/Solving_Partial_Derivative.PNG'>
</p>
<p><br></p>
<p>Now letâ€™s plug these two values to our <code>Gradient Descent</code>:</p>
<p><br></p>
<p align="center">
<img src = '/img/main/Final_Gradient_Descent.PNG'>
</p>
<p><br></p>
<div id="applications" class="section level3">
<h3>Applications</h3>
<ul>
<li>Sales Forecasting</li>
<li>Demand Supply Forecasting</li>
<li>Operations cost optimization</li>
<li>Insurance industry - claim prediction</li>
<li>Banking</li>
<li>Healthcare industry - cost prediction</li>
<li>Ecommerce industry - Recommandation System</li>
</ul>
</div>
<div id="key-points" class="section level3">
<h3>Key Points</h3>
<ul>
<li><p>If sample is small ( &lt; 10000) then normal equation can be used to get the theta values</p></li>
<li><p>As the training set size increases it is better to use gradient descent algorithm instead of normal equation</p></li>
<li><p>If sample data contains large digits for x, y values then it is better to scale the values around mean before applying cost function and gradient descent</p></li>
<li><p>In R language <code>lm(x~y)</code> can be used directly for determining theta values which is more efficient than using gradient descent algorithm</p></li>
<li><p>Initially it is better to calculate correlation coeficient to ensure that variables are related in some way</p></li>
<li><p>Normalizing data is important to deal with when individual values are numerically large ( &gt; 4 digits)</p></li>
</ul>
</div>
<div id="slide-show" class="section level3">
<h3>Slide show</h3>
<pre class="r"><code>knitr::include_url(&#39;/slides/GradientDescentLR.html&#39;)</code></pre>
<iframe src="/slides/GradientDescentLR.html" width="672" height="400px">
</iframe>
</div>
<div id="slide-show-multiple-features" class="section level3">
<h3>Slide show (Multiple Features)</h3>
<pre class="r"><code>knitr::include_url(&#39;/slides/MultipleFeaturesLR.html&#39;)</code></pre>
<iframe src="/slides/MultipleFeaturesLR.html" width="672" height="400px">
</iframe>
</div>
</div>

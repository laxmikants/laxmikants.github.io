---
title: "Linear Regression"
author: Laxmi K Soni 
description: "A linear equation that models a function such that if we give any `x` to it, it will predict a value `y` , where both `x and y` are input and output variables respectively"
slug: Linear Regression
date: 2020-03-12
lastmod: 2020-03-12
categories: ["Linear Regression"]
tags: ["Linear Regression"]
Summary: A linear equation that models a function such that if we give any `x` to it, it will predict a value `y` , where both `x and y` are input and output variables respectively
subtitle: Linear Regression
featured: "img/main/linear_regression-10.jpg"
output:
  html_document:
    highlight: tango
    theme: flatly
    toc: no
    toc_float: no

---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```


## Definition

A linear equation that models a function such that if we give any `x` to it, it will predict a value `y` , where both `x and y` are input and output variables respectively. These are numerical and continous values.
It is the most simple and well known algorithm used in machine learning.

## Flowchart 

<p align = 'center'><img src = '/img/main/Linear_Reg_Flowchart.png'></p>

<br>

The above Flowchart represents that we choose our training set, feed it to an algorithm, it will learn the patterns and will output a function called `Hypothesis function 'H(x)'`. We then give any `x` value to that function and it will output an estimated `y` value for it.

For historical reasons, this function `H(x)` is called `hypothesis function.`



## Cost Function

The best fit line to our data will be where we have least distance between the `predicted 'y' value` and `trained 'y' value`.

## Formula for Cost Function

<p align = 'center'><img src = '/img/main/MSE.png'></p>

> Where :
>- h(x<sub>i</sub>) ðŸ‘‰ hypothesis function
>- y<sub>i</sub> ðŸ‘‰ actual values of `y`
>- 1/m ðŸ‘‰ gives Mean of Squared Errors
>- 1/2 ðŸ‘‰ Mean is halved as a convenience for the computation of the `Gradient Descent`.


```{r}
computeCost <- function (X, y, theta){
        # number of training examples
        m <- length(y);
        # need to return
        J <- 0;
        
        predictions <-  X %*% theta;
        sqerrors = (predictions - y)^2;
        J = 1/(2*m)* sum(sqerrors);
        
        J
    }
    
    

```
This formula inputs the sum of the distances between <i>`predicted values` and `actual values` of training set, does sqaure it and take the average and multiply it by 0.5</i>
<br>
<br>
This cost function is also called as `Squared Error Function` or `Mean Squared Error`.
<br>
<br>
Why do we take squares of the error's?<br>
The `MSE` function is commonly used and is a reasonable choice and works well for most Regression problems.
<br>
<br>
Let's subsititute `MSE` function to function `J` :
<p align = 'center'><img src = '/img/main/MSE1.png'></p>

<br>
<br>


## Gradient Descent 

So now we have our hypothesis function and we have a way of measuring how well it fits into the data. Now we need to estimate the parameters in the hypothesis function. That's where `Gradient Descent` comes in.<br>
`Gradient Descent` is used to minimize the cost function `J`, minimizing `J` is same as minimizing `MSE` to get best possible fit line to our data.

$$\displaystyle \min_{\theta_0,\theta_1}\frac{1}{2m}\sum_{i=1}^{m} \left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2$$


## Formula for Gradient Descent
<p align = 'center'><img src = '/img/main/Gradient_Descent.PNG'></p>

> Where :
>- `:=`  Is the Assignment Operator
>- `Î±`  is `Alpha`, it's the number which is called learning rate. If its too high it may fail to converge and if too low then descending will be slow.
>- 'Î¸<sub>j</sub>'  Taking Gradient Descent of a feature or a column of a dataset.
> - âˆ‚/(âˆ‚Î¸<sub>j</sub>) J(Î¸<sub>0</sub>,Î¸<sub>1</sub>)  Taking partial derivative of `MSE` cost function.

<br>

```{r}
    gradientDescent <- function(X, y, theta, alpha, num_iters){
        m <- length(y);  
        J_history = rep(0, num_iters);
        for (iter in 1:num_iters){
            predictions <-  X %*% theta;
            updates = t(X) %*% (predictions - y);
            theta = theta - alpha * (1/m) * updates;
            J_history[iter] <- computeCost(X, y, theta);
        }
        list("theta" = theta, "J_history" = J_history)  
    }
```



**Now Let's apply Gradient Descend to minmize our `MSE` function.**
<br>
In order to apply `Gradient Descent`, we need to figure out the partial derivative term.<br>
So lets solve partial derivative of cost function `J`.

<br>

<p align = 'center'><img src = '/img/main/Solving_Partial_Derivative.PNG'></p>

<br>

Now let's plug these two values to our `Gradient Descent`:

<br>

<p align = 'center'><img src = '/img/main/Final_Gradient_Descent.PNG'></p>

<br>


    
    
### Applications

- Sales Forecasting
- Demand Supply Forecasting
- Operations cost optimization
- Insurance industry - claim prediction
- Banking
- Healthcare industry - cost prediction
- Ecommerce industry - Recommandation System 

### Key Points

- If sample is small ( < 10000) then normal equation can be used to get the theta values

- As the training set size increases it is better to use gradient descent algorithm instead of normal equation 

- If sample data contains large digits for x, y values then it is better to scale the values around mean before applying cost function and gradient descent

- In R language `lm(x~y)` can be used directly for determining theta values which is more efficient than using gradient descent algorithm

- Initially it is better to calculate correlation coeficient to ensure that variables are related in some way

- Normalizing data is important to deal with when individual values are numerically large ( > 4 digits)



### Slide show

```{r}
knitr::include_url('/slides/GradientDescentLR.html')
```


### Slide show (Multiple Features)

```{r}
knitr::include_url('/slides/MultipleFeaturesLR.html')
```

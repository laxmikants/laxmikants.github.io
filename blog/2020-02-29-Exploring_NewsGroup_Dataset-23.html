---
title: "Text analytics"
author: Laxmi K Soni 
description: "The analysis of text data gives useful insigths. This post uses news group data set to investigate text data"
slug: Text analytics
date: 2020-02-29
lastmod: 2020-02-29
categories: ["Text analytics", "Natural language processing"]
tags: ["Text analytics","nltk"]
Summary: The analysis of text data gives useful insigths. This post uses news group data set to explor text data
subtitle: Text analytics
featured: "img/main/text-analytics06.jpg"
output:
  html_document:
    highlight: tango
    theme: flatly
    toc: no
    toc_float: no
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<p>Processing large amounts text data is an important area in natural language processing. The analysis of text data with machine learning tools can give us important insights. Given a text data such as a book, posts or tweets, one may ask questions such as list of common words.</p>
<p>In this post we are going to analyse 20 news groups dataset. The <code>Newsgroups</code> dataset comprises around 18000 newsgroups posts on 20 topics. The dataset can by obtained by using <code>fetch_20newsgroups</code> in <code>sklearn.datasets</code> as <code>fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)</code></p>
<p><strong><em>1: First step is to get the dataset and look into it to get understanding about how it is organized…</em></strong></p>
<pre class="python"><code>from sklearn.datasets import fetch_20newsgroups
newsgroups_full = fetch_20newsgroups(subset=&#39;all&#39;, remove=(&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;), shuffle=True, random_state=42)
print(newsgroups_full.keys())</code></pre>
<pre><code>## dict_keys([&#39;data&#39;, &#39;filenames&#39;, &#39;target_names&#39;, &#39;target&#39;, &#39;DESCR&#39;])</code></pre>
<p>The <code>newsgroups_full</code> dataset has properties and function such as <code>keys()</code> which important keys for fetching the details of different types.
For example <code>target_names</code> specifies various names of the newsgroups, <code>target</code> is 20 different unique index corresponding to target_names
the key <code>data</code> is used to get actual data stored in different files having some <code>filenames</code>. Lets see how go use different <code>keys</code></p>
<pre class="python"><code># The target names are the names of the news groups
print(newsgroups_full.target_names)</code></pre>
<pre><code>## [&#39;alt.atheism&#39;, &#39;comp.graphics&#39;, &#39;comp.os.ms-windows.misc&#39;, &#39;comp.sys.ibm.pc.hardware&#39;, &#39;comp.sys.mac.hardware&#39;, &#39;comp.windows.x&#39;, &#39;misc.forsale&#39;, &#39;rec.autos&#39;, &#39;rec.motorcycles&#39;, &#39;rec.sport.baseball&#39;, &#39;rec.sport.hockey&#39;, &#39;sci.crypt&#39;, &#39;sci.electronics&#39;, &#39;sci.med&#39;, &#39;sci.space&#39;, &#39;soc.religion.christian&#39;, &#39;talk.politics.guns&#39;, &#39;talk.politics.mideast&#39;, &#39;talk.politics.misc&#39;, &#39;talk.religion.misc&#39;]</code></pre>
<pre class="python"><code># The data is actual data stred as list
print(newsgroups_full.target_names[newsgroups_full.target[1]])</code></pre>
<pre><code>## comp.sys.ibm.pc.hardware</code></pre>
<pre class="python"><code>print(newsgroups_full.data[1])</code></pre>
<pre><code>## My brother is in the market for a high-performance video card that supports
## VESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:
## 
##   - Diamond Stealth Pro Local Bus
## 
##   - Orchid Farenheit 1280
## 
##   - ATI Graphics Ultra Pro
## 
##   - Any other high-performance VLB card
## 
## 
## Please post or email.  Thank you!
## 
##   - Matt</code></pre>
<p>As we can se the above two statements give us the data about <code>post</code> belonging to <code>comp.sys.ibm.pc.hardware</code> which contains:</p>
<pre class="python"><code>
# Putting the words in the dictionary

newsgroups_full_dnry = dict()
for ind in range(len(newsgroups_full.data)):
    grp_name = newsgroups_full.target_names[newsgroups_full.target[ind]]
    if grp_name in newsgroups_full_dnry:
        newsgroups_full_dnry[grp_name] += 1
    else:
        newsgroups_full_dnry[grp_name] = 1
print(&quot;Total number of articles in dataset &quot; + str(len(newsgroups_full.data)))        </code></pre>
<pre><code>## Total number of articles in dataset 18846</code></pre>
<pre class="python"><code>print(&quot;Number of articles category wise: &quot;)</code></pre>
<pre><code>## Number of articles category wise:</code></pre>
<pre class="python"><code>print(newsgroups_full_dnry)</code></pre>
<pre><code>## {&#39;rec.sport.hockey&#39;: 999, &#39;comp.sys.ibm.pc.hardware&#39;: 982, &#39;talk.politics.mideast&#39;: 940, &#39;comp.sys.mac.hardware&#39;: 963, &#39;sci.electronics&#39;: 984, &#39;talk.religion.misc&#39;: 628, &#39;sci.crypt&#39;: 991, &#39;sci.med&#39;: 990, &#39;alt.atheism&#39;: 799, &#39;rec.motorcycles&#39;: 996, &#39;rec.autos&#39;: 990, &#39;comp.windows.x&#39;: 988, &#39;comp.graphics&#39;: 973, &#39;sci.space&#39;: 987, &#39;talk.politics.guns&#39;: 910, &#39;misc.forsale&#39;: 975, &#39;rec.sport.baseball&#39;: 994, &#39;talk.politics.misc&#39;: 775, &#39;comp.os.ms-windows.misc&#39;: 985, &#39;soc.religion.christian&#39;: 997}</code></pre>
<p>Pie chart of distribution of the articles</p>
<pre class="python"><code>import matplotlib.pyplot as plt


labels = newsgroups_full.target_names

slices = []

for key in newsgroups_full_dnry:
    slices.append(newsgroups_full_dnry[key])
    
fig , ax = plt.subplots()

ax.pie(slices, labels = labels , autopct = &#39;%1.1f%%&#39;, shadow = True, startangle = 90)

ax.axis(&quot;equal&quot;)
ax.set_title(&quot;News groups messages distribution&quot;)</code></pre>
<p><img src="/img/main/ngpie.png" /></p>
<p>The distribution of messages posted in different newsgroups is almost similar. The sports groups have most number of messages</p>
<p>Viewing the data as tabular form. We can put the data in the dataframe and see the top ten records</p>
<pre class="python"><code>import pandas as pd
data_labels_map = dict(enumerate(newsgroups_full.target_names))
message, target_labels, target_names = (newsgroups_full.data, newsgroups_full.target, [data_labels_map[label] for label in newsgroups_full.target])
newsgroups_full_df = pd.DataFrame({&#39;text&#39;: message, &#39;source&#39;: target_labels, &#39;source_name&#39;: target_names})
print(newsgroups_full_df.shape)</code></pre>
<pre><code>## (18846, 3)</code></pre>
<pre class="python"><code>newsgroups_full_df.head(10)</code></pre>
<pre><code>##                                                 text  ...               source_name
## 0  \n\nI am sure some bashers of Pens fans are pr...  ...          rec.sport.hockey
## 1  My brother is in the market for a high-perform...  ...  comp.sys.ibm.pc.hardware
## 2  \n\n\n\n\tFinally you said what you dream abou...  ...     talk.politics.mideast
## 3  \nThink!\n\nIt&#39;s the SCSI card doing the DMA t...  ...  comp.sys.ibm.pc.hardware
## 4  1)    I have an old Jasmine drive which I cann...  ...     comp.sys.mac.hardware
## 5  \n\nBack in high school I worked as a lab assi...  ...           sci.electronics
## 6  \n\nAE is in Dallas...try 214/241-6060 or 214/...  ...     comp.sys.mac.hardware
## 7  \n[stuff deleted]\n\nOk, here&#39;s the solution t...  ...          rec.sport.hockey
## 8  \n\n\nYeah, it&#39;s the second one.  And I believ...  ...          rec.sport.hockey
## 9  \nIf a Christian means someone who believes in...  ...        talk.religion.misc
## 
## [10 rows x 3 columns]</code></pre>
<p><strong><em>2: Next step is cleaning the text…</em></strong></p>
<p>To clean the large amounts of text we use <code>nltk</code> tools such as <code>WordNetLemmatizer</code>, <code>PorterStemmer</code>, <code>stopwords</code>, <code>names</code>.
Lets import them first</p>
<pre class="python"><code>import nltk
from nltk.corpus import names
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re

stopWords = set(stopwords.words(&#39;english&#39;))
validwords = set(nltk.corpus.words.words())</code></pre>
<p><code>re</code> is regular expression library in python. We need to first define few functions such as <code>text_tokenizer</code>. The main aim is to clean the posts first by removing the alpha-numeric, numeric and non-alphabatic characters then by applying <code>stemming</code> and <code>lemmmatizing</code> techiniques so that we are left with only the words which are meaningful for the analysis. Lets write the functions for the same</p>
<pre class="python"><code>porter_stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()
def text_tokenizer(str_input):
    words = re.sub(r&quot;[^A-Za-z\-]&quot;, &quot; &quot;, str_input).lower().split()
    words = [porter_stemmer.stem(word) for word in words if len(word) &gt; 2 ]
    words = [lemmatizer.lemmatize(word) for word in words if len(word) &gt; 2 and word in validwords and word not in stopWords]
    return &#39; &#39;.join(words)</code></pre>
<p><strong><em>2.1: Next is to apply <code>text_tokenizer</code> function to get a new column having clean text…</em></strong></p>
<pre class="python"><code>newsgroups_full_df[&#39;clean_text&#39;] = newsgroups_full_df.text.apply(lambda x: text_tokenizer(x))
newsgroups_full_df.sort_values(by=[&#39;source&#39;],inplace=True)
newsgroups_full_df.head(5)</code></pre>
<pre><code>##                                                     text  ...                                         clean_text
## 8501   \nI could give much the same testimonial about...  ...  could give much scout back gay thank well put ...
## 14285  \nFine... THE ILLIAD IS THE WORD OF GOD(tm)  (...  ...              fine word god matter prove wrong west
## 17533  Hello Gang,\n\nThere have been some notes rece...  ...  hello gang note recent ask obtain fish questio...
## 1527   \n  Sorry, gotta disagree with you on this one...  ...  one bill prefer half bake bob vice said queen ...
## 14271  The latest news seems to be that Koresh will g...  ...          latest news seem give finish write sequel
## 
## [5 rows x 4 columns]</code></pre>
<p><strong><em>2.3:Creating a dictionary of newsgroup cleaned text</em></strong></p>
<pre class="python"><code>wordlst = list()
newsgroup_dic = dict()
label = &#39;&#39;</code></pre>
<pre class="python"><code>for i in range(0,20):
    newsgroups_full_df_1 = newsgroups_full_df.loc[newsgroups_full_df[&#39;source&#39;] == i]
    for row in newsgroups_full_df_1[[&#39;source_name&#39;, &#39;clean_text&#39;]].iterrows():
        r = row[1]
        label = r.source_name
        wordlst.append(&#39;&#39;.join(map(str,r.clean_text)))
        wordstr = &#39; &#39;.join(map(str, wordlst))
    newsgroup_dic[label] = wordstr
    label = &#39;&#39;
    wordstr = &#39;&#39;
    wordlst.clear() </code></pre>
<p>Next steps will create the features out of the dictionary of the newsgroups words just created in the previous steps. In natural language processing feature extraction is an important step. In this case the words themselves becomes the features. To extract the features python provides an important library called <code>CountVectorizer</code>. We need to transform our <code>cleaned_text</code> using <code>sklearn.feature_extraction.text</code> and <code>CountVectorizer</code> library. Lets apply it to our newsgroup data.</p>
<p><strong><em>3: Feature extraction…</em></strong></p>
<p>The feature vector can be created with <code>sklearn</code> <code>CountVectorizer</code>. When creating the feature vectors we can decide the number of features, as well as set limits for the minimum and maximum number of documents a word can appear.</p>
<p>Note that the transformed data is stored in a <code>sparse matrix</code> (which is much more efficient for large data sets).</p>
<pre class="python"><code># First lets import it
from  sklearn.feature_extraction.text import CountVectorizer
count_vectorizer = CountVectorizer(stop_words = &#39;english&#39;)</code></pre>
<p>The function <code>get_word_freq_dict_sorted</code> returns a sorted dictionary of words counts. It taks a dataframe as its argument.</p>
<pre class="python"><code>def get_word_freq_dict_sorted(ng_X_df):
    wordfreq = ng_X_df.sum(axis=0)
    features = ng_X_df.columns.tolist()
    counts = wordfreq.tolist()
    wordfreq_df = pd.DataFrame()
    wordfreq_df[&#39;word&#39;] = features
    wordfreq_df[&#39;count&#39;] = counts
    wordfreq_dict = dict(wordfreq_df.values.tolist())
    wordfreqdict_sorted = dict(sorted(wordfreq_dict.items(), key=lambda x: x[1],reverse=True))
    return wordfreqdict_sorted</code></pre>
<p>Now iterate over the newsgroup dictionary obtained from the newsgroups dataframe and create another dictionary where keys are the newsgroups and values are another dictionary of word counts in that newsgroup.</p>
<pre class="python"><code>ng_dict_of_words = dict()

for key in newsgroup_dic:
    ng_X = count_vectorizer.fit_transform([newsgroup_dic[key]])
    ng_X_df = pd.DataFrame(ng_X.toarray(), columns=count_vectorizer.get_feature_names())
    ng_dict_of_words[key] = get_word_freq_dict_sorted(ng_X_df)
    </code></pre>
<p><strong><em>4: Exploring words in the news groups..</em></strong></p>
<p>QUESTION: What are the top words in newsgroup <code>comp.sys.ibm.pc.hardware</code> by their count ?</p>
<p>ANSWER: Iterating over the dictionary corresponding to <code>comp.sys.ibm.pc.hardware</code> we get the top ten words as {space orbit launch use like time mission year earth moon}. Like wise we get the most common words in each newsgroup by their count.</p>
<pre class="python"><code>word_dic = ng_dict_of_words[&#39;comp.sys.ibm.pc.hardware&#39;] 
word_df = pd.DataFrame.from_dict(word_dic, orient=&#39;index&#39;)
print(word_df.T.iloc[0:1,0:10])</code></pre>
<pre><code>##    drive  use  card  ani  control  disk  work  problem  know  ide
## 0    990  792   537  476      441   384   369      356   333  309</code></pre>
<p>Various other approaches to explore words in news groups include graphical methods, which help us visualize the distribution of words across news groups. We can use <code>matplotlib.pyplot</code> to draw differnt graphs.</p>
<p>Next we will explore various algorithms for text classification.</p>
<p><strong><em>5 Text Classification…</em></strong></p>
<p>Text classification is done using various machine learning algorithms. The most popular ones are</p>
<ul>
<li>MultinomialNB</li>
<li>LogisticRegression</li>
<li>SVC</li>
</ul>
<p>The goal of the <code>text classification</code> is to predict which newsgroup a post belongs to based on the post text.</p>
<p><code>BOW</code> and <code>TF-IDF</code> are two different techniques for text classification</p>
<p>Bag of Words (BoW) is an algorithm that counts frequency of a word in newsgroups. Those word counts allow us to compare different newsgroups and gauge their similarities for applications like search, topic modeling etc.</p>
<p>In <code>TF-IDF</code>, words are given weight. TF-IDF measures relevance, not frequency. That is, wordcounts are replaced with TF-IDF scores across the whole dataset.</p>
<p>To use text classification algorithm we need to randomly separates data into training and testing dataset and <code>fit</code> the classifier with selected training data. A <code>classifer</code> defines model for text classification. The <code>score</code> gives us the accuracy for testing data.</p>
<p>Different classifiers can give us different results for accuracy. Accuracy depends on the specific problem, number of categories and differences between them, etc.</p>
<p><strong><em>6 Evaluation…</em></strong></p>
<p>Evaluation of the model can be done using the confusion matrix which can be ploted using the heatmap plot. A basic heatmap is shown below</p>
<div class="figure">
<img src="/img/main/newgroupsheatmap.png" alt="" />
<p class="caption">newgroupsheatmap.png</p>
</div>
<p>The confusion matrix depicts the wrongly classified records. For example 4 articles from comp.graphics are wrongly classified as comp.windows.x.</p>
<p>***7 Slide show</p>
<pre class="r"><code>knitr::include_url(&#39;/slides/NewsGroupsAnalysis.html&#39;)</code></pre>
<iframe src="/slides/NewsGroupsAnalysis.html" width="672" height="400px">
</iframe>
<p><code>Summary:</code> Text classifcation has usefull applications in detection of spam pages, personal email sorting, tagging products or document filtering, automatic classification of the text based on its contents, sentiment analysis etc. There are different methods and models availble in <code>sklearn</code> and <code>nltp</code> libraries in python which can be utilized for text classification and natural language processing applications.</p>

---
title: "Classification"
author: Laxmi K Soni 
description: "Classifcation models a function to predict a specific outcome out of mulitple possible outcomes."
slug: Classification
date: 2020-03-17
lastmod: 2020-03-17
categories: ["Classification"]
tags: ["Classification"]
Summary: Classifcation models a function to predict a specific outcome out of mulitple possible outcomes.
subtitle: Classification
featured: "img/main/classification-17.jpg"
output:
  html_document:
    highlight: tango
    theme: flatly
    toc: no
    toc_float: no

---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<div id="classification" class="section level2">
<h2>Classification</h2>
<p>With regression we now predicted specific output-values for certain given input-values. Sometimes, however, we are not trying to predict outputs but to categorize or classify our elements. For this, we use classification algorithms</p>
<p><img src="/img/main/KNN.png" /></p>
<p>In the figure above, we see one specific kind of classification algorithm, namely the K-Nearest-Neighbor classifier. Here we already have a decent amount of classified elements. We then add a new one (represented by the stars) and try to predict its class by looking at its nearest neighbors.</p>
</div>
<div id="classification-algorithms" class="section level2">
<h2>CLASSIFICATION ALGORITHMS</h2>
<p>There are various different classification algorithms and they are often used for predicting medical data or other real life use-cases. For example, by providing a large amount of tumor samples, we can classify if a tumor is benign or malignant with a pretty high certainty.</p>
</div>
<div id="k-nearest-neighbors" class="section level2">
<h2>K-NEAREST-NEIGHBORS</h2>
<p>As already mentioned, by using the K-Nearest-Neighbors classifier, we assign the class of the new object, based on its nearest neighbors. The K specifies the amount of neighbors to look at. For example, we could say that we only want to look at the one neighbor who is nearest but we could also say that we want to factor in 100 neighbors.
Notice that K shouldn’t be a multiple of the number of classes since it might cause conflicts when we have an equal amount of elements from one class as from the other.</p>
</div>
<div id="naive-bayes" class="section level2">
<h2>NAIVE-BAYES</h2>
<p>The Naive Bayes algorithm might be a bit confusing when you encounter it the first time. However, we are only going to discuss the basics and focus more on the implementation in Python later on.</p>
<table>
<thead>
<tr class="header">
<th>Outlook</th>
<th>Temperture</th>
<th>Humidity</th>
<th>Windy</th>
<th>Play</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sunny</td>
<td>Hot</td>
<td>High</td>
<td>FALSE</td>
<td>No</td>
</tr>
<tr class="even">
<td>Sunny</td>
<td>Hot</td>
<td>High</td>
<td>TRUE</td>
<td>No</td>
</tr>
<tr class="odd">
<td>Rainy</td>
<td>Mild</td>
<td>High</td>
<td>FALSE</td>
<td>No</td>
</tr>
<tr class="even">
<td>Rainy</td>
<td>Hot</td>
<td>High</td>
<td>TRUE</td>
<td>No</td>
</tr>
<tr class="odd">
<td>Overcast</td>
<td>Hot</td>
<td>Normal</td>
<td>TRUE</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>Sunny</td>
<td>Hot</td>
<td>Normal</td>
<td>TRUE</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td>Sunny</td>
<td>Mild</td>
<td>High</td>
<td>TRUE</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>Overcast</td>
<td>Cold</td>
<td>Normal</td>
<td>TRUE</td>
<td>No</td>
</tr>
</tbody>
</table>
<p>Imagine that we have a table like the one above. We have four input values (which we would have to make numerical of course) and one label or output. The two classes are Yes and No and they indicate if we are going to play outside or not.</p>
<p>What Naive Bayes now does is to write down all the probabilities for the individual scenarios. So we would start by writing the general probability of playing and not playing. In this case, we only play three out of eight times and thus our probability of playing will be 3/8 and the probability of not playing will be 5/8.</p>
<p>Also, out of the five times we had a high humidity we only played once, whereas out of the three times it was normal, we played twice. So our probability for playing when we have a high humidity is 1/5 and for playing when we have a medium humidity is 2/3. We go on like that and note all the probabilities we have in our table. To then get the classification for a new entry, we multiply the probabilities together and end up with a prediction.</p>
</div>
<div id="logistic-regression" class="section level2">
<h2>LOGISTIC REGRESSION</h2>
<p>Another popular classification algorithm is called logistic regression . Even though the name says regression , this is actually a classification algorithm. It looks at probabilities and determines how likely it is that a certain event happens (or a certain class is the right one), given the input data. This is done by plotting something similar to a logistic growth curve and splitting the data into two.</p>
<p><img src="/img/main/LogisticRegg.png" /></p>
</div>
<div id="decision-trees" class="section level2">
<h2>DECISION TREES</h2>
<p>With decision tree classifiers, we construct a decision tree out of our training data and use it to predict the classes of new elements.</p>
<p><img src="/img/main/dtreeweather.png" /></p>
<p>Since we are not using a line (and thus our model is not linear), we are also preventing mistakes caused by outliers.</p>
<p>This classification algorithm requires very little data preparation and it is also very easy to understand and visualize. On the other hand, it is very easy to be overfitting the model. Here, the model is very closely matched to the training data and thus has worse chances to make a correct prediction on new data.</p>
</div>
<div id="random-forest" class="section level2">
<h2>RANDOM FOREST</h2>
<p>Rndom forest classifier is based on decision trees. What it does is creating a forest of multiple decision trees. To classify a new object, all the various trees determine a class and the most frequent result gets chosen. This makes the result more accurate and it also prevents overfitting. It is also more suited to handle data sets with higher dimensions. On the other hand, since the generation of the forest is random , you have very little control over your model.</p>
</div>
<div id="loading-data" class="section level2">
<h2>LOADING DATA</h2>
<p>Now let us get into the code. In this example, we will get our data directly from the sklearn module. For the program we need the following imports:</p>
<pre class="python"><code>import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_breast_cancer </code></pre>
<p>At the last import, we import a dataset containing data on breast cancer. Also notice that we are only importing the KNeighborsClassifier for now.</p>
<pre class="python"><code>data = load_breast_cancer()
print (data.feature_names)</code></pre>
<pre><code>## [&#39;mean radius&#39; &#39;mean texture&#39; &#39;mean perimeter&#39; &#39;mean area&#39;
##  &#39;mean smoothness&#39; &#39;mean compactness&#39; &#39;mean concavity&#39;
##  &#39;mean concave points&#39; &#39;mean symmetry&#39; &#39;mean fractal dimension&#39;
##  &#39;radius error&#39; &#39;texture error&#39; &#39;perimeter error&#39; &#39;area error&#39;
##  &#39;smoothness error&#39; &#39;compactness error&#39; &#39;concavity error&#39;
##  &#39;concave points error&#39; &#39;symmetry error&#39; &#39;fractal dimension error&#39;
##  &#39;worst radius&#39; &#39;worst texture&#39; &#39;worst perimeter&#39; &#39;worst area&#39;
##  &#39;worst smoothness&#39; &#39;worst compactness&#39; &#39;worst concavity&#39;
##  &#39;worst concave points&#39; &#39;worst symmetry&#39; &#39;worst fractal dimension&#39;]</code></pre>
<pre class="python"><code>print (data.target_names)</code></pre>
<pre><code>## [&#39;malignant&#39; &#39;benign&#39;]</code></pre>
<p>targets, we have two options in this dataset: malignant and benign .</p>
</div>
<div id="preparing-data" class="section level2">
<h2>PREPARING DATA</h2>
<p>Again, we convert our data back into NumPy arrays and split them into training and test data.</p>
<pre class="python"><code>X = np.array(data.data)
Y = np.array(data.target)
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.1 )</code></pre>
<p>The data attribute refers to our features and the target attribute points to the classes or labels. We again choose a test size of ten percent.</p>
</div>
<div id="training-and-testing" class="section level2">
<h2>TRAINING AND TESTING</h2>
<p>We start by first defining our K-Nearest-Neighbors classifier and then training it.</p>
<pre class="python"><code>knn = KNeighborsClassifier( n_neighbors = 5 )
knn.fit(X_train, Y_train)</code></pre>
<pre><code>## KNeighborsClassifier()</code></pre>
<p>The n_neighbors parameter specifies how many neighbor points we want to consider. In this case, we take five. Then we test our model again for its accuracy.</p>
<pre class="python"><code>accuracy = knn.score(X_test, Y_test)
print (accuracy)</code></pre>
<pre><code>## 0.9122807017543859</code></pre>
<p>We get a pretty decent accuracy for such a complex task.</p>
<p>0.9649122807017544</p>
<div id="the-best-algorithm" class="section level3">
<h3>THE BEST ALGORITHM</h3>
<p>Now let’s put all the classification algorithms that we’ve discussed up until now to use and see which one performs best.</p>
<pre class="python"><code>from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier </code></pre>
<pre class="python"><code>clf1 = KNeighborsClassifier( n_neighbors = 5 )
clf2 = GaussianNB()
clf3 = LogisticRegression()
clf4 = DecisionTreeClassifier()
clf5 = RandomForestClassifier()

clf1.fit(X_train, Y_train)</code></pre>
<pre><code>## KNeighborsClassifier()</code></pre>
<pre class="python"><code>clf2.fit(X_train, Y_train)</code></pre>
<pre><code>## GaussianNB()</code></pre>
<pre class="python"><code>clf3.fit(X_train, Y_train)</code></pre>
<pre><code>## LogisticRegression()
## 
## C:\Users\slaxm\AppData\Local\R-MINI~1\envs\R-RETI~1\lib\site-packages\sklearn\linear_model\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):
## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
## 
## Increase the number of iterations (max_iter) or scale the data as shown in:
##     https://scikit-learn.org/stable/modules/preprocessing.html
## Please also refer to the documentation for alternative solver options:
##     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
##   extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)</code></pre>
<pre class="python"><code>clf4.fit(X_train, Y_train)</code></pre>
<pre><code>## DecisionTreeClassifier()</code></pre>
<pre class="python"><code>clf5.fit(X_train, Y_train)</code></pre>
<pre><code>## RandomForestClassifier()</code></pre>
<pre class="python"><code>print (clf1.score(X_test, Y_test))</code></pre>
<pre><code>## 0.9122807017543859</code></pre>
<pre class="python"><code>print (clf2.score(X_test, Y_test))</code></pre>
<pre><code>## 0.9298245614035088</code></pre>
<pre class="python"><code>print (clf3.score(X_test, Y_test))</code></pre>
<pre><code>## 0.9649122807017544</code></pre>
<pre class="python"><code>print (clf4.score(X_test, Y_test))</code></pre>
<pre><code>## 0.8421052631578947</code></pre>
<pre class="python"><code>print (clf5.score(X_test, Y_test))</code></pre>
<pre><code>## 0.9649122807017544</code></pre>
<p>When you run this program a couple of times, you will notice that we can’t really say which algorithm is the best. Every time we run this script, we will see different results, at least for this specific data set.</p>
</div>
<div id="predicting-labels" class="section level3">
<h3>PREDICTING LABELS</h3>
<p>Again, we can again make predictions for new, unknown data. The chance of success in the classification is even very high. We just need to pass an array of input values and use the predict function .</p>
<pre class="python"><code>X_new = np.array([[...]])
Y_new = clf.predict(X_new)</code></pre>
</div>
</div>

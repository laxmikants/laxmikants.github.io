---
title: "Exploring and preparing data"
author: Laxmi K Soni 
description: "Exploring and preparing data with titanic dataset"
slug: Exploring and preparing data
date: 2020-02-08
lastmod: 2020-02-08
categories: ["Exploring Dataset"]
tags: ["Exploring Dataset"]
Summary: In data analysis and predictive modeling preparation of data paves the way for further insights.
subtitle: Exploring and preparing data 
featured: "img/main/eda_iris05.jpg"
output:
  html_document:
    highlight: tango
    theme: flatly
    toc: no
    toc_float: no
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<pre class="css"><code>.badCode {
background-color: black;
}</code></pre>
<style type="text/css">
.badCode {
background-color: black;
}
</style>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>The first a part of any data analysis or predictive modeling task is an initial exploration of the datasets. Albeit we collected the datasets ourself and we have already got an inventory of questions in mind that we simply want to answer, it’s important to explore the datasets before doing any serious analysis, since oddities within the datasets can cause bugs and muddle your results. Before exploring deeper questions, we got to answer many simpler ones about the shape and quality of datasets . That said, it’s important to travel into initial data exploration with an enormous picture question in mind.</p>
<p>This post aims to boost a number of the questions we ought to consider once we check out a replacement data set for the primary time and show the way to perform various Python operations associated with those questions.</p>
<p>In this post, we’ll explore the Titanic disaster training set available from Kaggle.co. The dataset consists of 889 passengers who rode aboard the Titanic.</p>
</div>
<div id="getting-the-dataset" class="section level2">
<h2>Getting the dataset</h2>
<p>To get the dataset into <code>pandas dataframe</code> simply call the function <code>read_csv</code>.</p>
<pre class="python bg-success"><code>import pandas as pd
import numpy as np

tit_train = pd.read_csv(&quot;../data/titanic/train.csv&quot;)      # Read the data
</code></pre>
<p>Checking the dimensions of the dataset with <code>df.shape</code> and the variable data types of <code>df.dtypes</code>.</p>
<pre class="python"><code>tit_train.shape</code></pre>
<pre><code>## (891, 12)</code></pre>
<pre class="python"><code>tit_train.dtypes</code></pre>
<pre><code>## PassengerId      int64
## Survived         int64
## Pclass           int64
## Name            object
## Sex             object
## Age            float64
## SibSp            int64
## Parch            int64
## Ticket          object
## Fare           float64
## Cabin           object
## Embarked        object
## dtype: object</code></pre>
<p>The output displays that we re working with a set of 891 records and 12 columns. Most of the column variables are encoded as numeric data types (ints and floats) but a some of them are encoded as “object”.</p>
<p>Check the head of the data to get a better sense of what the variables look like:</p>
<pre class="python"><code>tit_train.head(5)</code></pre>
<pre><code>##    PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked
## 0            1         0       3  ...   7.2500   NaN         S
## 1            2         1       1  ...  71.2833   C85         C
## 2            3         1       3  ...   7.9250   NaN         S
## 3            4         1       1  ...  53.1000  C123         S
## 4            5         0       3  ...   8.0500   NaN         S
## 
## [5 rows x 12 columns]</code></pre>
<p>We have a combination of numeric columns and columns with text data.</p>
<p>In dataset analysis, variables or features that split records into a fixed number of unique categories, such as Sex, are called as categorical variables.</p>
<p>Pandas can be used to interpret categorical variables as such when we load dataset, but we can convert a variable to categorical if necessary</p>
<p>After getting a sense of the datasets structure, it is a good practice to look at a statistical summary of the features with df.describe():</p>
<pre class="python"><code>tit_train.describe().transpose()</code></pre>
<pre><code>##              count        mean         std  ...       50%    75%       max
## PassengerId  891.0  446.000000  257.353842  ...  446.0000  668.5  891.0000
## Survived     891.0    0.383838    0.486592  ...    0.0000    1.0    1.0000
## Pclass       891.0    2.308642    0.836071  ...    3.0000    3.0    3.0000
## Age          714.0   29.699118   14.526497  ...   28.0000   38.0   80.0000
## SibSp        891.0    0.523008    1.102743  ...    0.0000    1.0    8.0000
## Parch        891.0    0.381594    0.806057  ...    0.0000    0.0    6.0000
## Fare         891.0   32.204208   49.693429  ...   14.4542   31.0  512.3292
## 
## [7 rows x 8 columns]</code></pre>
<p>we notice that the non-numeric columns are omitted from the statistical summary provided by <code>df.describe()</code>.</p>
<p>We can find the summary of the categorical variables by passing only those columns to describe():</p>
<pre class="python"><code>
cat_vars = tit_train.dtypes[tit_train.dtypes == &quot;object&quot;].index

print(cat_vars)
</code></pre>
<pre><code>## Index([&#39;Name&#39;, &#39;Sex&#39;, &#39;Ticket&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;], dtype=&#39;object&#39;)</code></pre>
<pre class="python"><code>tit_train[cat_vars].describe().transpose()</code></pre>
<pre><code>##          count unique                 top freq
## Name       891    891  Wright, Mr. George    1
## Sex        891      2                male  577
## Ticket     891    681            CA. 2343    7
## Cabin      204    147         C23 C25 C27    4
## Embarked   889      3                   S  644</code></pre>
<p>The summary of the categorical features shows the count of non-NaN records, the number of unique categories, the most frequent occurring value and the number of occurrences of the most frequent value.</p>
<p>Although describe() gives a concise overview of each variable, it does not necessarily give us enough information to determine what each variable means.</p>
<p>Certain features like “Age” and “Fare” are easy to understand, while others like “SibSp” and “Parch” are not. The details of these are provided by kaggle on the data download page.</p>
<pre class="python"><code># VARIABLE DESCRIPTIONS:
# survival        Survival
#                 (0 = No; 1 = Yes)
# pclass          Passenger Class
#                 (1 = 1st; 2 = 2nd; 3 = 3rd)
# name            Name
# sex             Sex
# age             Age
# sibsp           Number of Siblings/Spouses Aboard
# parch           Number of Parents/Children Aboard
# ticket          Ticket Number
# fare            Passenger Fare
# cabin           Cabin
# embarked        Port of Embarkation
#                 (C = Cherbourg; Q = Queenstown; S = Southampton)</code></pre>
<p>After looking at the data we ask yourself a few questions:</p>
<table>
<thead>
<tr class="header">
<th>QUESTIONS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Do we require all of the variables ?</td>
</tr>
<tr class="even">
<td>Should we transform any variables ?</td>
</tr>
<tr class="odd">
<td>Check if there are any NA values, outliers etc ?</td>
</tr>
<tr class="even">
<td>Should we create new variables?</td>
</tr>
</tbody>
</table>
<div id="do-we-require-all-of-the-variables" class="section level4">
<h4>Do we require all of the Variables?</h4>
<p>Removal of unnecessary variables is a first step when dealing with any data set, since removing variables reduces complexity and can make computation on the data faster.</p>
<p>Whether we should get rid of a variable or not will depend on size of the data set and the goal of the analysis. With a dataset like the titanic data, there’s no requirement to remove variables from a computing perspective.
But it can be helpful to drop variables that will only distract from your goal.</p>
<p>Let’s go through each variable and consider whether we should keep it or not in the context of survival prediction.</p>
<p>“PassengerId” is just a number assigned to each passenger. We can remove it</p>
<pre class="python"><code>del tit_train[&#39;PassengerId&#39;]</code></pre>
<p>Variable “Survived” shows whether each passenger lived or died. Since survival prediction is our goal, we definitely need to keep it.</p>
<p>Features describing passengers numerically or grouping them into a few broad categories could be useful for survival prediction. Therefore variables Pclass, Sex, Age, SibSp, Parch, Fare and Embarked can be kept.</p>
<p>further, “Name” appears to be a character string of the name of each passenger and it will also help in identifying passenger so we can keep it</p>
<p>Next, let’s see at “Ticket”</p>
<pre class="python"><code>tit_train[&#39;Ticket&#39;][0:10]</code></pre>
<pre><code>## 0           A/5 21171
## 1            PC 17599
## 2    STON/O2. 3101282
## 3              113803
## 4              373450
## 5              330877
## 6               17463
## 7              349909
## 8              347742
## 9              237736
## Name: Ticket, dtype: object</code></pre>
<pre class="python"><code>tit_train[&#39;Ticket&#39;].describe()</code></pre>
<pre><code>## count          891
## unique         681
## top       CA. 2343
## freq             7
## Name: Ticket, dtype: object</code></pre>
<p>Ticket has 681 unique values: almost as many as there are passengers. Categorical variables with this many levels are generally not very useful for prediction. Let’s remove it</p>
<pre class="python"><code>del tit_train[&#39;Ticket&#39;]</code></pre>
<p>Lastly let’s see the “Cabin” variable</p>
<pre class="python"><code>tit_train[&#39;Cabin&#39;][0:10]</code></pre>
<pre><code>## 0     NaN
## 1     C85
## 2     NaN
## 3    C123
## 4     NaN
## 5     NaN
## 6     E46
## 7     NaN
## 8     NaN
## 9     NaN
## Name: Cabin, dtype: object</code></pre>
<pre class="python"><code>tit_train[&#39;Cabin&#39;].describe()</code></pre>
<pre><code>## count             204
## unique            147
## top       C23 C25 C27
## freq                4
## Name: Cabin, dtype: object</code></pre>
<p>Cabin also has 147 unique values, which shows it may not be useful for prediction. On the other hand, the names of the different levels for the cabin variable seem to have a some structure, each starts with a capital letter followed by a number. We can use that structure to reduce the number of levels to make categories large enough that they might be useful for prediction later on. So Lets Keep Cabin for now.</p>
</div>
<div id="should-we-transform-any-variables" class="section level4">
<h4>Should we transform Any Variables?</h4>
<p><code>Pclass</code> is an integer variable that indicates a passenger’s class, with 1 being first class, 2 as second class and 3 as third class. We can transform this by transforming Pclass into an ordered categorical variable</p>
<pre class="python"><code>
pclass_new = pd.Categorical(tit_train[&#39;Pclass&#39;], ordered=True)

pclass_new = pclass_new.rename_categories([&quot;class1&quot;,&quot;class2&quot;,&quot;class3&quot;])

pclass_new.describe()</code></pre>
<pre><code>##             counts     freqs
## categories                  
## class1         216  0.242424
## class2         184  0.206510
## class3         491  0.551066</code></pre>
<pre class="python"><code>tit_train[&#39;Pclass&#39;] = pclass_new</code></pre>
<p>Now see the Cabin variable. It appears that each Cabin is in a general section of the ship indicated by the capital letter at the start of each factor level</p>
<pre class="python"><code>tit_train[&#39;Cabin&#39;].unique()</code></pre>
<pre><code>## array([nan, &#39;C85&#39;, &#39;C123&#39;, &#39;E46&#39;, &#39;G6&#39;, &#39;C103&#39;, &#39;D56&#39;, &#39;A6&#39;,
##        &#39;C23 C25 C27&#39;, &#39;B78&#39;, &#39;D33&#39;, &#39;B30&#39;, &#39;C52&#39;, &#39;B28&#39;, &#39;C83&#39;, &#39;F33&#39;,
##        &#39;F G73&#39;, &#39;E31&#39;, &#39;A5&#39;, &#39;D10 D12&#39;, &#39;D26&#39;, &#39;C110&#39;, &#39;B58 B60&#39;, &#39;E101&#39;,
##        &#39;F E69&#39;, &#39;D47&#39;, &#39;B86&#39;, &#39;F2&#39;, &#39;C2&#39;, &#39;E33&#39;, &#39;B19&#39;, &#39;A7&#39;, &#39;C49&#39;, &#39;F4&#39;,
##        &#39;A32&#39;, &#39;B4&#39;, &#39;B80&#39;, &#39;A31&#39;, &#39;D36&#39;, &#39;D15&#39;, &#39;C93&#39;, &#39;C78&#39;, &#39;D35&#39;,
##        &#39;C87&#39;, &#39;B77&#39;, &#39;E67&#39;, &#39;B94&#39;, &#39;C125&#39;, &#39;C99&#39;, &#39;C118&#39;, &#39;D7&#39;, &#39;A19&#39;,
##        &#39;B49&#39;, &#39;D&#39;, &#39;C22 C26&#39;, &#39;C106&#39;, &#39;C65&#39;, &#39;E36&#39;, &#39;C54&#39;,
##        &#39;B57 B59 B63 B66&#39;, &#39;C7&#39;, &#39;E34&#39;, &#39;C32&#39;, &#39;B18&#39;, &#39;C124&#39;, &#39;C91&#39;, &#39;E40&#39;,
##        &#39;T&#39;, &#39;C128&#39;, &#39;D37&#39;, &#39;B35&#39;, &#39;E50&#39;, &#39;C82&#39;, &#39;B96 B98&#39;, &#39;E10&#39;, &#39;E44&#39;,
##        &#39;A34&#39;, &#39;C104&#39;, &#39;C111&#39;, &#39;C92&#39;, &#39;E38&#39;, &#39;D21&#39;, &#39;E12&#39;, &#39;E63&#39;, &#39;A14&#39;,
##        &#39;B37&#39;, &#39;C30&#39;, &#39;D20&#39;, &#39;B79&#39;, &#39;E25&#39;, &#39;D46&#39;, &#39;B73&#39;, &#39;C95&#39;, &#39;B38&#39;,
##        &#39;B39&#39;, &#39;B22&#39;, &#39;C86&#39;, &#39;C70&#39;, &#39;A16&#39;, &#39;C101&#39;, &#39;C68&#39;, &#39;A10&#39;, &#39;E68&#39;,
##        &#39;B41&#39;, &#39;A20&#39;, &#39;D19&#39;, &#39;D50&#39;, &#39;D9&#39;, &#39;A23&#39;, &#39;B50&#39;, &#39;A26&#39;, &#39;D48&#39;,
##        &#39;E58&#39;, &#39;C126&#39;, &#39;B71&#39;, &#39;B51 B53 B55&#39;, &#39;D49&#39;, &#39;B5&#39;, &#39;B20&#39;, &#39;F G63&#39;,
##        &#39;C62 C64&#39;, &#39;E24&#39;, &#39;C90&#39;, &#39;C45&#39;, &#39;E8&#39;, &#39;B101&#39;, &#39;D45&#39;, &#39;C46&#39;, &#39;D30&#39;,
##        &#39;E121&#39;, &#39;D11&#39;, &#39;E77&#39;, &#39;F38&#39;, &#39;B3&#39;, &#39;D6&#39;, &#39;B82 B84&#39;, &#39;D17&#39;, &#39;A36&#39;,
##        &#39;B102&#39;, &#39;B69&#39;, &#39;E49&#39;, &#39;C47&#39;, &#39;D28&#39;, &#39;E17&#39;, &#39;A24&#39;, &#39;C50&#39;, &#39;B42&#39;,
##        &#39;C148&#39;], dtype=object)</code></pre>
<p>If we grouped the cabin just by this letter, we could lesser the number of levels while getting some useful information.</p>
<pre class="python"><code>
chr_cabin = tit_train[&quot;Cabin&quot;].astype(str)

n_Cabin = np.array([cabin[0] for cabin in chr_cabin]) 

n_Cabin = pd.Categorical(n_Cabin)

n_Cabin.describe()</code></pre>
<pre><code>##             counts     freqs
## categories                  
## A               15  0.016835
## B               47  0.052750
## C               59  0.066218
## D               33  0.037037
## E               32  0.035915
## F               13  0.014590
## G                4  0.004489
## T                1  0.001122
## n              687  0.771044</code></pre>
<p>The output of describe() shows we can group Cabin into broader categories, but we also discovered something interesting: 688 of the records have Cabin are “n” which is shortened from “nan”. In other words, more than 2/3 of the passengers do not have a cabin.</p>
<p>A missing cabin variable could be an indication that a passenger died.</p>
<p>We can keep the new variable cabin</p>
<pre class="python"><code>tit_train[&quot;Cabin&quot;] = n_Cabin</code></pre>
</div>
<div id="checking-to-if-there-are-null-values-outliers-or-other-garbage-values" class="section level3">
<h3>Checking to if there are null Values, Outliers or Other garbage Values?</h3>
<p>To check the missing values we us <code>pd.isnull()</code> function for example</p>
<pre class="python"><code>
mock_vector = pd.Series([1,None,3,None,7,8])

mock_vector.isnull()</code></pre>
<pre><code>## 0    False
## 1     True
## 2    False
## 3     True
## 4    False
## 5    False
## dtype: bool</code></pre>
<p>If the missing values are numeric then they can be simple deleted. If missing values are categorical then they can be treated as additional category with value as NA.</p>
<p>To check if there is missing age values in titanic dataset</p>
<pre class="python"><code>tit_train[&quot;Age&quot;].describe()</code></pre>
<pre><code>## count    714.000000
## mean      29.699118
## std       14.526497
## min        0.420000
## 25%       20.125000
## 50%       28.000000
## 75%       38.000000
## max       80.000000
## Name: Age, dtype: float64</code></pre>
<p>We see that the count of age (712) is less than the total row count of dataset(889).
To check indexes of the missing ages we use <code>np.where()</code></p>
<pre class="python"><code>missvalues = np.where(tit_train[&quot;Age&quot;].isnull() == True)
missvalues</code></pre>
<pre><code>## (array([  5,  17,  19,  26,  28,  29,  31,  32,  36,  42,  45,  46,  47,
##         48,  55,  64,  65,  76,  77,  82,  87,  95, 101, 107, 109, 121,
##        126, 128, 140, 154, 158, 159, 166, 168, 176, 180, 181, 185, 186,
##        196, 198, 201, 214, 223, 229, 235, 240, 241, 250, 256, 260, 264,
##        270, 274, 277, 284, 295, 298, 300, 301, 303, 304, 306, 324, 330,
##        334, 335, 347, 351, 354, 358, 359, 364, 367, 368, 375, 384, 388,
##        409, 410, 411, 413, 415, 420, 425, 428, 431, 444, 451, 454, 457,
##        459, 464, 466, 468, 470, 475, 481, 485, 490, 495, 497, 502, 507,
##        511, 517, 522, 524, 527, 531, 533, 538, 547, 552, 557, 560, 563,
##        564, 568, 573, 578, 584, 589, 593, 596, 598, 601, 602, 611, 612,
##        613, 629, 633, 639, 643, 648, 650, 653, 656, 667, 669, 674, 680,
##        692, 697, 709, 711, 718, 727, 732, 738, 739, 740, 760, 766, 768,
##        773, 776, 778, 783, 790, 792, 793, 815, 825, 826, 828, 832, 837,
##        839, 846, 849, 859, 863, 868, 878, 888], dtype=int64),)</code></pre>
<pre class="python"><code>len(missvalues)</code></pre>
<pre><code>## 1</code></pre>
<p>Before we do anything with missing values its good to check the distribution of the missing values to know the central tendency of age.</p>
<pre class="python"><code>tit_train.hist(column=&#39;Age&#39;,    
                   figsize=(9,6),   
                   bins=20)         </code></pre>
<p><img src="/img/main/titdsage.png" /></p>
<p>The histogram shows that couple of passengers are near age 80.</p>
<p>To check the <code>fare</code> variable we create the box plot</p>
<pre class="python"><code>tit_train[&quot;Fare&quot;].plot(kind=&quot;box&quot;,
                           figsize=(9,9))</code></pre>
<p><img src="/img/main/titdsfare.png" /></p>
<p>50% of the data in the box plot represents the median. There are outliers in the data. There are passengers who paid double the amount than any other passenger. We can check this using <code>np.where()</code> function</p>
<pre class="python"><code>ind = np.where(tit_train[&quot;Fare&quot;] == max(tit_train[&quot;Fare&quot;]) )

tit_train.loc[ind]</code></pre>
<pre><code>##      Survived  Pclass  ... Cabin Embarked
## 258         1  class1  ...     n        C
## 679         1  class1  ...     B        C
## 737         1  class1  ...     B        C
## 
## [3 rows x 10 columns]</code></pre>
<p>Before modeling datasets using ML models it is better to address missing values, outliers, mislabeled data, bad data because they can corrupt the analysis and lead to wrong results.</p>
</div>
<div id="should-we-create-new-variables" class="section level3">
<h3>Should we Create New Variables?</h3>
<p>The decision to create new variables should be taken while preparing the data. The new variable could represent aggregate of existing variables, for example in titanic dataset we can create a new variable called <code>family</code> which stores the number of members in that family.</p>
<pre class="python"><code>tit_train[&quot;Family&quot;] = tit_train[&quot;SibSp&quot;] + tit_train[&quot;Parch&quot;]</code></pre>
<p>we can check who has most family members on the board</p>
<pre class="python"><code>mostfamily = np.where(tit_train[&quot;Family&quot;] == max(tit_train[&quot;Family&quot;]))

tit_train.loc[mostfamily]</code></pre>
<pre><code>##      Survived  Pclass                               Name  ... Cabin  Embarked  Family
## 159         0  class3         Sage, Master. Thomas Henry  ...     n         S      10
## 180         0  class3       Sage, Miss. Constance Gladys  ...     n         S      10
## 201         0  class3                Sage, Mr. Frederick  ...     n         S      10
## 324         0  class3           Sage, Mr. George John Jr  ...     n         S      10
## 792         0  class3            Sage, Miss. Stella Anna  ...     n         S      10
## 846         0  class3           Sage, Mr. Douglas Bullen  ...     n         S      10
## 863         0  class3  Sage, Miss. Dorothy Edith &quot;Dolly&quot;  ...     n         S      10
## 
## [7 rows x 11 columns]</code></pre>
</div>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>There are question that should be answered while investing any dataset. Once the basic questions are answered one can move further to find relationship between variables/features and build the machine learning models.</p>
</div>

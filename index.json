[{
    "title": "React Native",
    "date": "",
    "description": "React Native",
    "body": "\r\rReact Native\rReact Native is framework to building mobile apps using JavaScript and React.\rReact Native converts React code to Java for Android or Objective-C for iOS apps.\rReact Native uses most of the React concepts, like components, props, state and lifecycle methods.\n\rAdvantages of React Native:\r\r\rYou code one time, and you get two native apps (Android and iOS)\n\r\rYou need not to have experience with Java, Objective-C, or Swift\n\r\rFaster development\n\r\rMIT license (open source)\n\r\rRequirements for Windows:\r\r\rAndroid Studio\n\r\rAndroid SDK (\u0026gt;= 7.0 Nougat)\n\r\rAndroid AVD\n\rCreating first React Native Application\rIn order to create our new React Native application, we need to install the react-native-cli package:\nnpm install -g react-native-cli\rnpm install -g react-native-cli\r\rNow, to create our first app:\nLet’s do it with this command:\r\r react-native init MyFirstReactNativeApp\rreact-native init MyFirstReactNativeApp\r\rOnce we built our React Native app, we need install Watchman, which is a file-watching service required by React Native. To install it, go to https://facebook.github.io/watchman/docs/install.html and download the latest version for your OS (Windows, Mac, or Linux).\nIn this case, we are going to use Homebrew to install it for Mac. If you don’t have Homebrew, you can install it with this command:\r\r /usr/bin/ruby -e \u0026#39;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\u0026#39;\r/usr/bin/ruby -e '$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)'\r\rTo install Watchman, you need to run:\r\r brew update brew install watchman\rbrew update brew install watchman\r\rTo start the React Native project, we need to use:\r\r react-native start\rreact-native start\r\rIf everything works fine, you should see this:\nOpen a new terminal (Cmd + T) and run this command (depending on the device you want to use):\r\r react-native run-ios or\rreact-native run-android\rreact-native run-ios or\rreact-native run-android\r\rIf there are no errors, you should see the simulator running the default application:\nNow that we have our application running, let’s open our code and modify it a bit:\nChange the App.js file:\r\r ...\rexport default class App extends Component\u0026lt;Props\u0026gt; {\rrender() {\rreturn (\r\u0026lt;View style={styles.container}\u0026gt;\r\u0026lt;Text style={styles.welcome}\u0026gt;\rThis is my first React Native App!\r\u0026lt;/Text\u0026gt;\r\u0026lt;Text style={styles.instructions}\u0026gt;\rTo get started, edit App.js\r\u0026lt;/Text\u0026gt;\r\u0026lt;Text style={styles.instructions}\u0026gt;{instructions}\u0026lt;/Text\u0026gt;\r\u0026lt;/View\u0026gt;\r);\r}\r}\r...\rFile: App.js\r...\rexport default class App extends Component {\rrender() {\rreturn (\rThis is my first React Native App!\r\rTo get started, edit App.js\r\r{instructions}\r\r);\r}\r}\r...\rFile: App.js\r\rIf you go to the simulator again, you will need to press Cmd + R to reload the app to see the new changes reflected:\r\r\rSummary:\rReact Native is an framework which helps web developers in creating the robust\rmobile applications using their existing JavaScript knowledge. It offers more faster mobile\rdevelopment and more efficient code sharing between iOS, Android, and the web\rwithout sacrificing the end user’s experience or application quality.\n\r\r",
    "ref": "/blog/react-native/"
  },{
    "title": "About",
    "date": "",
    "description": "Hi",
    "body": "My name is Laxmikant and I have been into software industries for over 18 years. I have been developing front-end and back-end web platforms services since the early days of the internet.\nA specialist in JEE, cloud technologies, Decision Sciences development for many years, I also regularly add to my skill set. While always looking for ways to improve my craft in a rapidly advancing industry.I m also very aware that the bottom line is team productivity.\nHave worked with companies, stakeholders across industries, I understand how to handle complex requirements and guide development teams, solve technical problems and collaborate with the management in selecting the right development environment and making sure projects are completed in time. Working at all levels of development has taught me how to understand and respond to every situation uniquely and apply innovative solutions where necessary.\n",
    "ref": "/about/"
  },{
    "title": "React- JS",
    "date": "",
    "description": "React JS Programming",
    "body": "\r\r*** React-JS\nSetup\rfirst, we need to create our React application using create-react-app. Once that is done, you can proceed to create your first React component.\nInstall create-react-app globally by typing this command in your Terminal:\n npm install -g create-react-app\rnpm install -g create-react-app\r\rOr you can use a shortcut:\n npm i -g create-react-app\rnpm i -g create-react-app\r\r\rBuild the App\rLet’s build our first React application by following these steps:\nCreate our React application with the following command:\ncreate-react-app my-first-react-app\rcreate-react-app my-first-react-app\r\rGo to the new application with cd my-first-react-app and start it with npm start.\rThe application should now be running at http://localhost:3000.\nCreate a new file called Home.js inside your src folder:\nimport React, { Component } from \u0026#39;react\u0026#39;;\rclass Home extends Component {\rrender() {\rreturn \u0026lt;h1\u0026gt;I\u0026#39;m Home Component\u0026lt;/h1\u0026gt;;\r}\r}\rexport default Home;\rimport React, { Component } from 'react';\rclass Home extends Component {\rrender() {\rreturn I'm Home Component;\r}\r}\rexport default Home;\r\rYou may have noticed that we are exporting our class component at the end of the file, but it’s fine to export it directly on the class declaration, like this:\n import React, { Component } from \u0026#39;react\u0026#39;;\rexport default class Home extends Component {\rrender() {\rreturn \u0026lt;h1\u0026gt;I\u0026#39;m Home Component\u0026lt;/h1\u0026gt;;\r}\r}\rimport React, { Component } from 'react';\rexport default class Home extends Component {\rrender() {\rreturn I'm Home Component;\r}\r}\r\rNow that we have created the first component, we need to render it. So we need to open the App.js file, import the Home component, and then add it to the render method of the App component. If we are opening this file for the first time, we will probably see a code like this in File: src/App.js\n import React, { Component } from \u0026#39;react\u0026#39;;\rimport logo from \u0026#39;./logo.svg\u0026#39;;\rimport \u0026#39;./App.css\u0026#39;;\rclass App extends Component {\rrender() {\rreturn (\r\u0026lt;div className=\u0026#39;App\u0026#39;\u0026gt;\r\u0026lt;header className=\u0026#39;App-header\u0026#39;\u0026gt;\r\u0026lt;img src={logo} className=\u0026#39;App-logo\u0026#39; alt=\u0026#39;logo\u0026#39; /\u0026gt;\r\u0026lt;h1 className=\u0026#39;App-title\u0026#39;\u0026gt;Welcome to React\u0026lt;/h1\u0026gt;\r\u0026lt;/header\u0026gt;\r\u0026lt;p className=\u0026#39;App-intro\u0026#39;\u0026gt;\rTo get started, edit \u0026lt;code\u0026gt;src/App.js\u0026lt;/code\u0026gt; and save to reload.\r\u0026lt;/p\u0026gt;\r\u0026lt;/div\u0026gt;\r);\r}\r}\rexport default App;\rimport React, { Component } from 'react';\rimport logo from './logo.svg';\rimport './App.css';\rclass App extends Component {\rrender() {\rreturn (\rWelcome to React\r\rTo get started, edit src/App.js and save to reload.\r\r);\r}\r}\rexport default App;\r\rLet’s change this code a little bit. As I said before, we need to import our Home component and then add it to the JSX. We also need to replace the\r\relement with our component, like this in File: src/App.js\n import React, { Component } from \u0026#39;react\u0026#39;;\rimport logo from \u0026#39;./logo.svg\u0026#39;;\r// We import our Home component here...\rimport Home from \u0026#39;./Home\u0026#39;;\rimport \u0026#39;./App.css\u0026#39;;\rclass App extends Component {\rrender() {\rreturn (\r\u0026lt;div className=\u0026#39;App\u0026#39;\u0026gt;\r\u0026lt;header className=\u0026#39;App-header\u0026#39;\u0026gt;\r\u0026lt;img src={logo} className=\u0026#39;App-logo\u0026#39; alt=\u0026#39;logo\u0026#39; /\u0026gt;\r\u0026lt;h1 className=\u0026#39;App-title\u0026#39;\u0026gt;Welcome to React\u0026lt;/h1\u0026gt;\r\u0026lt;/header\u0026gt;\r{/* Here we add our Home component to be render it */}\r\u0026lt;Home /\u0026gt;\r\u0026lt;/div\u0026gt;\r);\r}\r}\rexport default App;\rimport React, { Component } from 'react';\rimport logo from './logo.svg';\r// We import our Home component here...\rimport Home from './Home';\rimport './App.css';\rclass App extends Component {\rrender() {\rreturn (\rWelcome to React\r\r{/* Here we add our Home component to be render it */}\r\r);\r}\r}\rexport default App;\r\rAs you can see, we imported React and Component from the React library. You probably noticed that we are not using the React object directly. To write code in JSX, you need to import React. JSX is similar to HTML, but with a few differences. In the following recipes, you will learn more about JSX.\nThis component is called a class component (React.Component), and there are different types: pure components (React.PureComponent) and functional components, also known as stateless components, which we will cover in the following recipes.\nIf you run the application, you should see something like this:\n\r\r",
    "ref": "/blog/react-js/"
  },{
    "title": "Web Scrapping Basics",
    "date": "",
    "description": "A beginer level introduction to Web Scrapping using Python",
    "body": "\r\rWhat is web scrapping\rWebscrapping also known as the the automated gathering of data from the Internet.\ritself. This is usually accomplished by writing an automated program\rthat queries a web server, requests data , and then parses that data to extract needed informa‐\rtion. BeautifulSoup, Python requests, LXML, Mechanical Soup and Scrappy are most common libraries used for web scraping.\nFeatures:\n\rLarge amounts of data can be extracted from websites\n\rConvert unstructred data to structured data\n\rThere is a Crawler in webscrapping to index and search content\n\rThere is a Scrapper in webscrapping which extracts the data from the web page\n\rWebsites, News resources, RSS feeds, Pricing Websites, Social Media, Company information, Schemas/charts/tables/graphs are the main sources of unstructured data which a webscrapper uses to get data.\n\r\rWhy Web Scraping?\rBrowsers displays the contents in the human readable format but it can not answers to specific queries that can be used to integrate with other systems.\rFor example if a program requires information such as cheapest flights to newyork then the browser will provide lots of information containing images,\radvertisements etc. but a web scrapping program will get the specif answer to our query. Practically web scrapping involves a wide variety of programming techniques\rand technologies, such as data analysis and information security.\n\r\rHow it works\r\rGet Request is sent using http protocol to the site the scrapper is targetting\n\rWeb server processes the request and then allowed to read and extract the html of the web page\n\rThe data is retrieved in html format after which it is carefully parsed to extricate the raw data we want from th noise surrounding it.\n\rFinally the data is stored in the format to exact the specifications of the project.\n\r\r\rFlow\r\rRequest\n\rCheck Response\n\rParse\n\rFilter\n\rDownload\n\r\r\rCommon python libraries for web-scrapping\rBeautifulSoup\rThe most common library is BeautifulSoup.\n\rIt parses html document\n\rIt extracts text from it\n\rIt searches tags by their attributes\n\rIt has findAll and find functions are commonly used to find all attributes.\n\r\rExample#1: Getting covid-19 data from the web\nimport requests\rfrom bs4 import BeautifulSoup\rurl = \u0026quot;https://www.worldometers.info/coronavirus/\u0026quot;\rpage = requests.get(url)\rsoup = BeautifulSoup(page.text,\u0026#39;html.parser\u0026#39;)\rtotal = soup.find(\u0026quot;div\u0026quot;,class_ = \u0026quot;maincounter-number\u0026quot;).text\rtotal = total[1:len(total)-1]\rother = soup.find_all(\u0026quot;span\u0026quot;,class_=\u0026quot;number-table\u0026quot;)\rrecovered = other[2].text\rdeaths = other[3].text\rdeaths = deaths[1:]\rans = {\u0026quot;total cases\u0026quot;: total, \u0026quot;recovered\u0026quot;: recovered, \u0026quot;deaths\u0026quot;: deaths}\rprint(ans)\r## {\u0026#39;total cases\u0026#39;: \u0026#39;40,824,733 \u0026#39;, \u0026#39;recovered\u0026#39;: \u0026#39;30,456,293\u0026#39;, \u0026#39;deaths\u0026#39;: \u0026#39;1,125,409\u0026#39;}\rExample#2: Scrapping yourdictionary.com\nimport requests\rfrom bs4 import BeautifulSoup\rurl = \u0026quot;https://examples.yourdictionary.com/20-words-to-avoid-on-your-resume.html\u0026quot;\rpage = requests.get(url)\rsoup = BeautifulSoup(page.text,\u0026#39;html.parser\u0026#39;)\rparas = soup.findAll(\u0026#39;p\u0026#39;)\rfor p in paras:\rprint(p.text + \u0026#39;\\n\u0026#39;)\rExample#2: Getting top mathematicians from web\nfrom urllib.request import urlopen as uReq\rfrom bs4 import BeautifulSoup as BeautifulSoup, Tag\rimport pandas as pd\rhtml = uReq(\u0026quot;http://www.fabpedigree.com/james/gmat200.htm\u0026quot;).read()\rsoup = BeautifulSoup(html,\u0026#39;html5lib\u0026#39;)\rnames = []\rfor item in soup.find_all(\u0026#39;li\u0026#39;):\rif isinstance(item, Tag):\rnames.append(item.text.rstrip())\rnames_df = pd.DataFrame(names) print(names_df.head()) \r## 0\r## 0 Isaac Newton\r## 1 Archimedes\r## 2 Carl F. Gauss\r## 3 Leonhard Euler\r## 4 Bernhard Riemann\r\rLXML\rPython provides lxml library which is easier to use and has lots of features. lxml and Beautiful soup have similarity. It allows to parse XML and HTML documents easily. Ease of use and performance are the key features of lxml library.\nExample:\nfrom lxml import html\rimport requests\rpage = requests.get(\u0026#39;https://projecteuler.net/problem=1\u0026#39;)\rtree = html.fromstring(page.content)\rtext=tree.xpath(\u0026#39;//div[@role=\u0026quot;problem\u0026quot;]/p/text()\u0026#39;)\rprint (text)\r## [\u0026#39;If we list all the natural numbers below 10 that are multiples of 3 or 5, we get 3, 5, 6 and 9. The sum of these multiples is 23.\u0026#39;, \u0026#39;Find the sum of all the multiples of 3 or 5 below 1000.\u0026#39;]\r\rMachenical Soup\rIt is a library for automating interaction with the websites. User can login and logout of the website, submit forms etc.\n\rPython Requests\rSubmitting a form with the Requests library can be done in four lines, including the\rimport and the instruction to print the content (yes, it’s that easy):\nExample#3 Getting exchange rates\n\rimport requests\rimport pandas as pd\r# base_url variable store base url base_url = \u0026quot;https://www.alphavantage.co/query?function=CURRENCY_EXCHANGE_RATE\u0026quot;\rfrom_currency = \u0026quot;USD\u0026quot;\rto_currency = \u0026quot;INR\u0026quot;\rmain_url = base_url + \u0026quot;\u0026amp;from_currency=\u0026quot; + from_currency + \u0026quot;\u0026amp;to_currency=\u0026quot; + to_currency + \u0026quot;\u0026amp;apikey=4RSKNH0KOBS5TMP1\u0026quot;\rreq_ob = requests.get(main_url) result = req_ob.json() oneusdequals = result[\u0026quot;Realtime Currency Exchange Rate\u0026quot;][\u0026#39;5. Exchange Rate\u0026#39;]\rprint(float(oneusdequals))\r\r## 73.47\r\rSummary\rThere are different ways to scrape data from the internet.\rRegular expressions can be useful for a one-off scrape or to avoid the overhead of parsing the entire web page.\rBeautifulSoup provides a high-level interface while avoiding any difficult dependencies.\rWeb scraping services provide an essential service at a low cost.\rIt is used to scrape Price and Products for Comparison Sites and many such use cases to provide useful data for further processing.\n\r\r",
    "ref": "/blog/web-scrapping/"
  },{
    "title": "Neural Networks",
    "date": "",
    "description": "The Neural Network is one of the most powerful learning algorithms.",
    "body": "\r\rNeural Networks\rThe Neural Network is one of the most powerful learning algorithms when a linear classifier doesn’t work, this is what we usually turn to.\nSTRUCTURE OF A NEURAL NETWORK\rWith neural networks we are trying to build our models based on the structure of the human brain, namely with neurons. The human brain is composed of multiple billions of neurons which are interconnected. Neural networks are structures which try to use a similar principle.\nIn the figure above, we can see three layers. First the input layer , at the end the output layer and in between the hidden layer .\nObviously the input layer is where our inputs go. There we put all the things which are being entered or sensed by the script or the machine. Basically these are our features.\nWe can use neural networks to classify data or to act on inputs and the output layer is where we get our results. These results might be a class or action steps. Maybe when we input a high temperature into our model, the output will be the action of cooling down the machine.\nAll the layers between input and output are called hidden layers . They make the model more abstract and more complex. They extend the internal logic.\nThe more hidden layers and neurons you add, the more sophisticated the model gets.\nHere for example we have two hidden layers, one with four neurons and one with three neurons. Notice that every neuron of a layer is connected to every neuron of the next layer.\n\rSTRUCTURE OF A NEURON\rIn order to understand how a neural network works in general, we need to understand how the individual neurons work.\nAs you can see, every neuron has a certain input, which is either the output of another neuron or the input of the first layer.\nThis number (which is the input) now gets multiplied by each of the weights (w1, w2, w3…) . After that, we subtract the bias b . The results of these calculations are the outputs of the neuron.\rWhat I have just explained and what you can see on the picture is an outdated version of a neuron called a perceptron . Nowadays, we are using more complex neurons like the sigmoid neurons which use more sophisticated activation functions to calculate the outputs.\n\rHOW NEURAL NETWORKS WORK\rBut what has all this to do with artificial intelligence or machine learning? Since neural networks are structures with a huge amount of parameters that can be changed, we can use certain algorithms so that the model can adjust itself. We input our data and the desired outcome. Then the model tries to adjust its weights and biases so that we can get from our inputs to the respective outputs.\nSince we are dealing with multiple thousands of neurons, we can’t do all this manually.\rWe use different algorithms like backpropagation and gradient descent , to optimize and train our model. We are not going to deep into the mathematics here. Our focus will be on the coding and the implementation.\n\r\r",
    "ref": "/blog/neural-networks/"
  },{
    "title": "Clustering",
    "date": "",
    "description": "Clustering is Machine Learning method that helps in finding the pattern of similarity and relationships among data samples in dataset and then cluster these samples into various groups.",
    "body": "\r\rClustering\rClustering is Machine Learning method that helps in finding the pattern of similarity and relationships among data samples in dataset and then cluster these samples into various groups.\nThe clustering algorithm gets raw data and tries to divide it up into clusters . K-Means-Clustering is the method that we are going to use here. Similar to K-Nearest-Neighbors, the K states the amount of clusters we want.\nHOW CLUSTERING WORKS\rThe clustering itself works with so-called centroids . These are the points, which lie in the center of the respective clusters.\nThe figure above illustrates quite well how clustering works. First, we randomly place the centroids somewhere in our data. This is the initialization . Here, we have defined three clusters, which is why we also have three centroids.\nThen, we look at each individual data point and assign the cluster of the nearest centroid to it. When we have done this, we continue by realigning our centroids. We place them in the middle of all points of their cluster.\nAfter that, we again reassign the points to the new centroids. We continue doing this over and over again until almost nothing changes anymore. Then we will hopefully end up with the optimal clusters. The result then looks like this:\nTypes of Clustering methods\n• Centroid based methods such as K-means and K-medoids\n• Hierarchical clustering methods such as agglomerative and divisive (Ward’s, affinity\rpropagation)\n• Distribution based clustering methods such as Gaussian mixture models\n• Density based methods such as dbscan and optics.\n\rLOADING DATA\rFor the clustering algorithm, we will use a dataset of handwritten digits. Since we are using unsupervised learning, we are not going to classify the digits. We are just going to put them into clusters. The following imports are necessary:\nfrom sklearn.cluster import KMeans\rfrom sklearn.preprocessing import scale\rfrom sklearn.datasets import load_digits\rBesides the KMeans module and the load_digits dataset, we are also importing the function scale from the preprocessing library. We will use this function for preparing our data.\ndigits = load_digits()\rdata = scale(digits.data)\rAfter loading our dataset we use the scale function, to standardize our data. We are dealing with quite large values here and by scaling them down to smaller values we save computation time.\n\rTRAINING AND PREDICTING\rWe can now train our model in the same way we trained the supervised learning models up until now.\nclf = KMeans( n_clusters = 10 , init = \u0026#39;random\u0026#39; , n_init = 10 )\rclf.fit(data)\r## KMeans(algorithm=\u0026#39;auto\u0026#39;, copy_x=True, init=\u0026#39;random\u0026#39;, max_iter=300,\r## n_clusters=10, n_init=10, n_jobs=None, precompute_distances=\u0026#39;auto\u0026#39;,\r## random_state=None, tol=0.0001, verbose=0)\rcentroids somewhere. Alternatively, we could use k-means++ for intelligent placing.\rThe last parameter (n_init ) states how many times the algorithm will be run with different centroid seeds to find the best clusters.\nSince we are dealing with unsupervised learning here, scoring the model is not really possible. You won’t be able to really score if the model is clustering right or not. We could only benchmark certain statistics like completeness or homogeneity .\nWhat we can do however is to predict which cluster a new input belongs to.\nclf.predict([…])\nIn this case, inputting data might be quite hard, since we would need to manually put in all the pixels. You could either try to write a script what converts images into NumPy arrays or you could work with a much simpler data set.\nAlso, since we are working with huge dimensions here, visualization is quite hard. When you work with two- or three-dimensional data, you can use the Matplotlib knowledge from volume three, in order to visualize your model.\n\r\r",
    "ref": "/blog/clustering/"
  },{
    "title": "Support Vector Machines",
    "date": "",
    "description": "In machine learning, support-vector machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.",
    "body": "\r\rSupport Vector Machines\rSupport Vector Machines are very powerful, very efficient machine learning algorithms and they even achieve much better results than neural networks in some areas. We are again dealing with classification here but the methodology is quite different.\nWhat we are looking for is a hyperplane that distinctly classifies our data points and has the maximum margin to all of our points. We want our model to be as generalized as possible.\nIn the graph above the model is very general and the line is the optimal function to separate our data. We can use an endless amount of lines to separate the two classes but we don’t want to overfit our model so that it only works for the data we already have. We also want it to work for unknown data.\nHere our model also separates the data we already have perfectly. But we’ve got a new red data point here. When we just look at this with our intuition it is obvious that this point belongs to the orange triangles. However, our model classifies it as a blue circle because it is overfitting our current data.\rTo find our perfect line we are using so-called support vectors , which are parallel lines.\nWe are looking for the two points that are the farthest away from the other class. In between of those, we draw our hyperplane so that the distance to both points is the same and as large as possible. The two parallel lines are the support vectors. In between the orange and the blue line there are no data points. This is our margin. We want this margin to be as big as possible because it makes our predictions more reliable.\n\rKERNELS\rThe data we have looked at so far is relatively easy to classify because it is clearly separated. Such data can almost never be found in the real world. Also, we are oftentimes working in higher dimensions with many features. This makes things more complicated.\nData taken from the real world often looks like this in figure. Here it is impossible to draw a straight line, and even a quadratic or cubic function does not help us here. In such cases we can use so-called kernels . These add a new dimension to our data. By doing that, we hope to increase the complexity of the data and possibly use a hyperplane as a separator.\nNotice that the kernel (a.k.a. the additional dimension) should be derived from the data that we already have. We are just making it more abstract. A kernel is not some random feature but a combination of the features we already have. But that wouldn’t be reasonable or helpful. Therefore, there are pre-defined and effective kernels that we can choose from.\n\rSOFT MARGIN\rSometimes, we will encounter statistical outliers in our data. It would be very easy to draw a hyperplane that separates the data into the classes, if it wasn’t for these outliers.\nn the figure above, you can see such a data set. We can see that almost all of the orange triangles are in the top first third, whereas almost all the blue dots are in the bottom two thirds. The problem here is with the outliers.\rNow instead of using a kernel or a polynomial function to solve this problem, we can define a so-called soft margin. With this, we allow for conscious misclassification of outliers in order to create a more accurate model. Caring too much about these outliers would again mean overfitting the model.\nAs you can see, even though we are misclassifying two data points our model is very accurate.\n\rLOADING DATA\rNow that we understand how SVMs work, let’s get into the coding. For this machine learning algorithm, we are going to once again use the breast cancer data set. We will need the following imports:\nfrom sklearn.svm import SVC\rfrom sklearn.datasets import load_breast_cancer\rfrom sklearn.neighbors import KNeighborsClassifier\rfrom sklearn.model_selection import train_test_split \rBesides the libraries we already know, we are importing the SVC module. This is the support vector classifier that we are going to use as our model. Notice that we are also importing the KNeighborsClassifier again, since we are going to compare the accuracies at the end.\ndata = load_breast_cancer()\rX = data.data\rY = data.target\rX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1 , random_state = 30 )\rThis time we use a new parameter named random_state . It is a seed that always produces the exact same split of our data. Usually, the data gets split randomly every time we run the script. You can use whatever number you want here. Each number creates a certain split which doesn’t change no matter how many times we run the script. We do this in order to be able to objectively compare the different classifiers.\n\rTRAINING AND TESTING\rSo first we define our support vector classifier and start training it.\nmodel = SVC( kernel = \u0026#39;linear\u0026#39; , C = 3 )\rmodel.fit(X_train, Y_train)\r## SVC(C=3, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\r## decision_function_shape=\u0026#39;ovr\u0026#39;, degree=3, gamma=\u0026#39;scale\u0026#39;, kernel=\u0026#39;linear\u0026#39;,\r## max_iter=-1, probability=False, random_state=None, shrinking=True,\r## tol=0.001, verbose=False)\rWe are using two parameters when creating an instance of the SVC class. The first one is our kernel and the second one is C which is our soft margin. Here we choose a linear kernel and allow for three misclassifications. Alternatively we could choose poly, rbf, sigmoid, precomputed or a self-defined kernel. Some are more effective in certain situations but also a lot more time-intensive than linear kernels.\naccuracy = model.score(X_test, Y_test)\rprint (accuracy)\r## 0.9649122807017544\rWhen we now score our model, we will see a very good result.\r0.9649122807017544\n\r",
    "ref": "/blog/support-vector-machines/"
  },{
    "title": "Decision Trees",
    "date": "",
    "description": "Decision trees area tree-like tool which can be used to represent a cause and its effect",
    "body": "\r\rDecision trees\rDecision trees area tree-like tool which can be used to represent a cause and its effect. In Machine Learning Decision trees are a type of Supervised machine learning where data is split according to given parameter while constructing a tree to solve a given problem. In decision tree there is a predictor variable and target variable or the desired output. The predictor variable could be anything such as technical indicators etc and the target variable could be desired output for example whether to invest in a given financial security or not.\n\rHow it works\rA decision basically gives flowchart of how to make some decision.You have some dependent variable, like whether to buy the stock depending on factors like RSI, MACD etc. When you have a decision like that that depends on multiple attributes or multiple variables, a decision tree could be a good choice.\nThere are many different aspects of the weather that might influence my decision to buy a given stock. It might have to do with the stock closing prise today, the RSI,MACD, EMA etc. A decision tree can look at all these different features of the stock, and decide what are the thresholds. What are those factors which affects the stock movement.\nFor example, Factors affecting the stock movement is shown by using decision tree\n\rUnderstanding Decision Tree\rAt each iteration of the decision tree flowchart, we find the property that we can partition our data on which minimizes the entropy of the data at the next step. So we have a resulting set of classes in this case “BUY” or “SELL”, and we want to choose the attribute decision at that step that will minimize the entropy at the next step. So we just walk down the tree, minimize entropy at each step by choosing the right attribute to decide on, and we keep on going until we run out.\n\rThe decision to buy the stock in majority of the cases is dependent on the stock fundamentals. In 64% of the cases buying of stock is supported by decision tree if the fundamentals of the company are strong.\n\r\rDecision tree example\rLet’s say I want to build a system that will automatically predicts the stock movement at the end of the day based on the opening price of the stock. Given a stock the system should decide whether that stock is going to have upward movment or downward movement during intraday trading so that investor can make a decision of investment in that stock.\nSo let’s make some totally fabricated stock data that we’re going to use in this example:\nIn the preceding table, we have stock prices along with technical indicators. We are going to pick some attributes that we think might be interesting or helpful to predict whether or not they can predict movement of the stock (UP or DOWN). How much is William %R ? What is exponential moving average ? What is the value of stochastic momentun index ? Is the stock overbought/oversold (RSI) ? Depending on the factors which affect the stock price we can predict whether it will go up or down.\nNow, obviously there’s a lot of information that isn’t in this model that might be very important, but the decision tree that we train from this data might actually be useful in doing an initial pass at weeding out some candidates. What we end up with might be a tree that looks like the following:\nSo it just turns out that in totally fabricated data, if the william %R (WPR) is below 0.17 then the stock will go UP. So the first questioon decision tree related to WPR, i.e if WPR is above 0.17 then go to left. At node 2 check if the exponential moving average is above 9.7, if it is then we end up at the leaf node predicted value to stock going down. If at node 2 exponential moving average is below 9.7 then go to node 5 and check the value of stochastic mementun index, if it is above 42 then go to right at leaf node 11 having stock predicted value of UP and so on.\n\rWalking through a decision tree\rSo that’s how we walk through the results of a decision tree. It’s just like going through a flowchart, and it’s kind of awesome that an algorithm can produce this for us. The algorithm itself is actually very simple. Let me explain how the algorithm works.\nAt each step of the decision tree flowchart, we see the attribute that we can partition our data on that minimizes the entropy of the data at the next step. So we have a resulting set of classifications in this case UP or DOWN, and we want to choose the attribute decision at that step that will minimize the entropy at the next step.\n\rEntropy meausures the level of impurity in a group. The group having minimum entropy is helps determinie attribute most useful for descriminating between classes to be learned.\n\rAt each step we want to make all of the remaining choices result in either as many downs or as many up decisions as possible. We want to make that data more and more uniform so as we work our way down the flowchart, and we ultimately end up with a set of candidates that are either all UPS or all DOWNS so we can classify into yes/no decisions on a decision tree. So we just walk down the tree, minimize entropy at each step by choosing the right attribute to decide on, and we keep on going until we run out.\nThere’s a fancy name for this algorithm. It’s called ID3 ( Iterative Dichotomiser 3 ). It is what’s known as a greedy algorithm. So as it goes down the tree, it just picks the attribute that will minimize entropy at that point. Now that might not actually result in an optimal tree that minimizes the number of choices that you have to make, but it will result in a tree that works, given the data that you gave it.\nThe tree starts at the top and finds the best data to split into nodes. It does this by recursive binary splitting using either the Gini index or cross-entropy measure. The Gini index is defined as:\n\\[G = \\sum_{k=1}^K \\hat{p}_{mk}(1 - \\hat{p}_{mk})\\]\nand is also referred as a measure of node purity, i.e., a smaller value indicates a node contains observations primarily from a single class.\nCross-entropy is similar to the Gini index in that it will take a small value if the node is pure. It is defined as:\n\\[D = -\\sum_{k=1}^K \\hat{p}_{mk}log\\hat{p}_{mk}\\]\nThe Gini index and Cross-entropy measures dictate when a node split will occur in order to keep each node as pure as possible to reduce the total value of the Gini index or cross-entropy measures.\nStart at the top, or root of the tree. 57% of the stock movements will have an DOWN movement with 43% going in UPWARD direction. If the William % R rating was equal to or above 0.17, we look left, otherwise you move right. To the right, we see only 17% of values having WPR below 0.17 will have UP movement, so the overall terminal node ends with the bucket having UP stock movement and so on.\n\rEvaluating the Decision Tree Model\r DOWN UP\rDOWN 231 78\rUP 87 220\r\rThe decision tree model have accuracy of 73%.\n\rBenefit of Decision Tree over Neural network\rThe benefit of using Decision trees over Neural Network are:\nThey are easy to program.\n\rThe top nodes in the tree will give the information about what data affects the prediction.\n\rTrees are interpretable and provide visual representation of data.\n\rPerforms faster than Neural Networks after training.\n\r\r\rIssues with DT\rNow one problem with decision trees is that they are very prone to overfitting, so you can end up with a decision tree that works well for the data that we trained it on, but it might not be that great for actually predicting the correct classification for values that it hasn’t seen before. Decision trees are all about arriving at the right decision for the training data that we gave it, but maybe we didn’t really take into account the right attributes, maybe we didn’t give it enough of a representative sample of values to learn from. This can result in real problems.\nSo to combat this issue, we use a technique called random forests, where the idea is that we sample the data that we train on, in different ways, for multiple different decision trees. Each decision tree takes a different random sample from our set of training data and constructs a tree from it. Then each resulting tree can vote on the right result.\nNow that technique of randomly resampling our data with the same model is a term called bootstrap aggregating, or bagging. This is a form of what we call ensemble learning. The basic idea is that we have multiple trees, a forest of trees, each uses a random subsample of the data that we have to train on. Then each of these trees can vote on the final result, and that will help us combat overfitting for a given set of training data.\nThe other thing random forests can do is actually restrict the number of attributes that it can choose, between at each stage, while it is trying to minimize the entropy as it goes. And we can randomly pick which attributes it can choose from at each level. So that also gives us more variation from tree to tree, and therefore we get more of a variety of algorithms that can compete with each other. They can all vote on the final result using slightly different approaches to arriving at the same answer.\nSo that’s how random forests work. Basically, it is a forest of decision trees where they are drawing from different samples and also different sets of attributes at each stage that it can choose between.\n\rSummary\rWe have a better understanding of decision trees now; why they are being used frequently in predictive modeling, how they are created, and how they can be optimized for best results. Decision trees are a powerful tool for data scientists, but they must be handled with care. All the repetitive tasks are achieved by use of computers (hence the term machine learning), all aspects of the process must be overseen by an experienced data scientist in order to create the most accurate model.\n\r",
    "ref": "/blog/linear-regression/"
  },{
    "title": "Applying Logistic Regression",
    "date": "",
    "description": "A case study on applying logistic regression to stock market",
    "body": "\r\r\r0.1 z-scores\r\r\r1:Loading Stock data\n1.1:Importing libraries and data\n\rimport investpy\rfrom datetime import datetime\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.metrics import confusion_matrix\rfrom sklearn import metrics\rfrom scipy import stats import numpy as np\rimport pandas as pd\rimport seaborn as sn\rimport matplotlib.pyplot as plt\rimport talib\rimport quandl\r\r1.2:Fetching the data\nTo fetch the stock data we use investpy library. This library fetchs data from investing for example:\ndf = investpy.commodities.get_commodity_historical_data(stock=\u0026#39;ABC\u0026#39;,from_date=\u0026#39;01/01/2015\u0026#39;,to_date=\u0026#39;14/08/2020\u0026#39;, country = \u0026quot;India\u0026quot;)\r## Open High Low Close Volume\r## Date ## 2019-12-02 44.290 44.416 44.077 44.344 354\r## 2019-12-03 44.250 44.985 44.150 44.620 138\r## 2019-12-04 44.949 45.100 44.100 44.392 34\r## 2019-12-05 44.200 45.287 43.850 44.064 44\r## 2019-12-06 44.599 44.714 43.470 43.545 24469\r## ... ... ... ... ... ...\r## 2020-10-13 62.126 62.965 60.213 60.530 31791\r## 2020-10-14 60.722 62.240 60.619 61.552 25329\r## 2020-10-15 61.002 61.639 59.840 61.420 28875\r## 2020-10-16 61.554 62.168 61.355 61.355 336\r## 2020-10-19 61.355 63.235 61.300 62.049 23793\r## ## [226 rows x 5 columns]\r0.1 z-scores\r\rdf[\u0026#39;zscore\u0026#39;] = stats.zscore(df[\u0026#39;Close\u0026#39;])\rprint(df[\u0026#39;zscore\u0026#39;].tail(30))\r## Date\r## 2020-09-07 1.786201\r## 2020-09-08 1.809582\r## 2020-09-09 1.804235\r## 2020-09-10 1.861693\r## 2020-09-11 1.750237\r## 2020-09-14 1.858967\r## 2020-09-15 1.859176\r## 2020-09-16 1.839674\r## 2020-09-17 1.772675\r## 2020-09-18 1.744890\r## 2020-09-21 1.056971\r## 2020-09-22 1.046171\r## 2020-09-23 0.760455\r## 2020-09-24 0.880089\r## 2020-09-25 0.816970\r## 2020-09-28 0.960509\r## 2020-09-29 1.177548\r## 2020-09-30 0.910496\r## 2020-10-01 1.039041\r## 2020-10-05 1.122502\r## 2020-10-06 0.978858\r## 2020-10-07 0.962920\r## 2020-10-08 0.973405\r## 2020-10-09 1.221375\r## 2020-10-12 1.243813\r## 2020-10-13 0.974559\r## 2020-10-14 1.081715\r## 2020-10-15 1.067875\r## 2020-10-16 1.061060\r## 2020-10-19 1.133826\r## Name: zscore, dtype: float64\rpd.set_option(\u0026#39;display.max_columns\u0026#39;, 10)\rPP = pd.Series((df[\u0026#39;High\u0026#39;] + df[\u0026#39;Low\u0026#39;] + df[\u0026#39;Close\u0026#39;]) / 3) R1 = pd.Series(2 * PP - df[\u0026#39;Low\u0026#39;]) S1 = pd.Series(2 * PP - df[\u0026#39;High\u0026#39;]) R2 = pd.Series(PP + df[\u0026#39;High\u0026#39;] - df[\u0026#39;Low\u0026#39;]) S2 = pd.Series(PP - df[\u0026#39;High\u0026#39;] + df[\u0026#39;Low\u0026#39;]) R3 = pd.Series(df[\u0026#39;High\u0026#39;] + 2 * (PP - df[\u0026#39;Low\u0026#39;])) S3 = pd.Series(df[\u0026#39;Low\u0026#39;] - 2 * (df[\u0026#39;High\u0026#39;] - PP)) psr = {\u0026#39;PP\u0026#39;:PP, \u0026#39;R1\u0026#39;:R1, \u0026#39;S1\u0026#39;:S1, \u0026#39;R2\u0026#39;:R2, \u0026#39;S2\u0026#39;:S2, \u0026#39;R3\u0026#39;:R3, \u0026#39;S3\u0026#39;:S3} PSR = pd.DataFrame(psr) print(PSR.tail())\r## PP R1 S1 R2 S2 R3 \\\r## Date ## 2020-10-13 61.236000 62.259000 59.507000 63.988000 58.484000 65.011000 ## 2020-10-14 61.470333 62.321667 60.700667 63.091333 59.849333 63.942667 ## 2020-10-15 60.966333 62.092667 60.293667 62.765333 59.167333 63.891667 ## 2020-10-16 61.626000 61.897000 61.084000 62.439000 60.813000 62.710000 ## 2020-10-19 62.194667 63.089333 61.154333 64.129667 60.259667 65.024333 ## ## S3 ## Date ## 2020-10-13 56.755000 ## 2020-10-14 59.079667 ## 2020-10-15 58.494667 ## 2020-10-16 60.271000 ## 2020-10-19 59.219333\rAs we can see, we now have a data frame with all the entries from start date to end date. We have multiple columns here and not only the closing stock price of the respective day. Let’s take a quick look at the individual columns and their meaning.\nOpen: That’s the share price the stock had when the markets opened that day.\nClose: That’s the share price the stock had when the markets closed that day.\nHigh: That’s the highest share price that the stock had that day.\nLow: That’s the lowest share price that the stock had that day.\nVolume: Amount of shares that changed hands that day.\n1.3:Reading individual values\nSince our data is stored in a Pandas data frame, we can use the indexing we already know, to get individual values. For example, we could only print the closing values using print (df[ 'Close' ])\nAlso, we can go ahead and print the closing value of a specific date that we are interested in. This is possible because the date is our index column.\nprint (df[ \u0026#39;Close\u0026#39; ][ \u0026#39;2020-07-14\u0026#39; ])\r## 52.649\rBut we could also use simple indexing to access certain positions.\nprint (df[ \u0026#39;Close\u0026#39; ][ 5 ])\r## 43.502\rHere we printed the closing price of the fifth entry.\n2:Graphical Visualization\nEven though tables are nice and useful, we want to visualize our financial data, in order to get a better overview. We want to look at the development of the share price.\nActually plotting our share price curve with Pandas and Matplotlib is very simple. Since Pandas builds on top of Matplotlib, we can just select the column we are interested in and apply the plot method. The results are amazing. Since the date is the index of our data frame, Matplotlib uses it for the x-axis. The y-values are then our adjusted close values.\nimport matplotlib.pyplot as plt\rfrom matplotlib import style\rstyle.use( \u0026#39;ggplot\u0026#39; )\rplt.ylabel( \u0026#39;Close\u0026#39; )\rplt.title( \u0026#39;Share Price\u0026#39; )\rdf[\u0026#39;Close\u0026#39;].plot()\rplt.show()\r2.1:CandleStick Charts\nThe best way to visualize stock data is to use so-called candlestick charts . This type of chart gives us information about four different values at the same time, namely the high, the low, the open and the close value. In order to plot candlestick charts, we will need to import a function of the MPL-Finance library.\nimport mplfinance as fplt\rWe are importing the candlestick_ohlc function. Notice that there also exists a candlestick_ochl function that takes in the data in a different order.\rAlso, for our candlestick chart, we will need a different date format provided by Matplotlib. Therefore, we need to import the respective module as well. We give it the alias mdates .\nimport matplotlib.dates as mdates\r2.2: Preparing the data for CandleStick charts\nNow in order to plot our stock data, we need to select the four columns in the right order.\ndf1 = df[[ \u0026#39;Open\u0026#39; , \u0026#39;High\u0026#39; , \u0026#39;Low\u0026#39; , \u0026#39;Close\u0026#39; ]]\rNow, we have our columns in the right order but there is still a problem. Our date doesn’t have the right format and since it is the index, we cannot manipulate it.\rTherefore, we need to reset the index and then convert our datetime to a number.\ndf1.reset_index( inplace = True )\rdf1[ \u0026#39;Date\u0026#39; ] = df1[ \u0026#39;Date\u0026#39; ].map(mdates.date2num)\r## C:/ProgramData/Anaconda3/python.exe:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame.\r## Try using .loc[row_indexer,col_indexer] = value instead\r## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\rFor this, we use the reset_index function so that we can manipulate our Date column. Notice that we are using the inplace parameter to replace the data frame by the new one. After that, we map the date2num function of the matplotlib.dates module on all of our values. That converts our dates into numbers that we can work with.\n2.3:Plotting the data\nNow we can start plotting our graph. For this, we just define a subplot (because we need to pass one to our function) and call our candlestick_ohlc function.\nax = plt.subplot()\rfplt(ax, df1.values,\rwidth = 5 ,\rcolordown = \u0026#39;r\u0026#39; , colorup = \u0026#39;g\u0026#39; )\rax.grid()\rax.xaxis_date()\rplt.show()\rOne candlestick gives us the information about all four values of one specific day. The highest point of the stick is the high and the lowest point is the low of that day. The colored area is the difference between the open and the close price.\rIf the stick is green, the close value is at the top and the open value at the bottom, since the close must be higher than the open. If it is red, it is the other way around.\n2.4:Analysis and Statistics \nNow let’s get a little bit deeper into the numbers here and away from the visual.\rFrom our data we can derive some statistical values that will help us to analyze it.\nPERCENTAGE CHANGE\nOne value that we can calculate is the percentage change of that day. This means by how many percent the share price increased or decreased that day.\ndf[ \u0026#39;PCT_Change\u0026#39; ] = (df[ \u0026#39;Close\u0026#39; ] - df[ \u0026#39;Open\u0026#39; ]) / df[ \u0026#39;Open\u0026#39; ]\rThe calculation is quite simple. We create a new column with the name PCT_Change and the values are just the difference of the closing and opening values divided by the opening values. Since the open value is the beginning value of that day, we take it as a basis. We could also multiply the result by 100 to get the actual percentage.\n*** HIGH LOW PERCENTAGE ***\nAnother interesting statistic is the high low percentage. Here we just calculate the difference between the highest and the lowest value and divide it by the closing value.\nBy doing that we can get a feeling of how volatile the stock is.\ndf[ \u0026#39;HL_PCT\u0026#39; ] = (df[ \u0026#39;High\u0026#39; ] - df[ \u0026#39;Low\u0026#39; ]) / df[ \u0026#39;Close\u0026#39; ]\rprint(df.tail())\r## Open High Low Close Volume ... usdinr usdsilver \\\r## Date ... ## 2020-10-13 62.126 62.965 60.213 60.530 31791 ... 73.400 24.129 ## 2020-10-14 60.722 62.240 60.619 61.552 25329 ... 73.240 24.395 ## 2020-10-15 61.002 61.639 59.840 61.420 28875 ... 73.420 24.224 ## 2020-10-16 61.554 62.168 61.355 61.355 336 ... 73.442 24.405 ## 2020-10-19 61.355 63.235 61.300 62.049 23793 ... 73.350 24.698 ## ## zscore PCT_Change HL_PCT ## Date ## 2020-10-13 0.974559 -0.025690 0.045465 ## 2020-10-14 1.081715 0.013669 0.026335 ## 2020-10-15 1.067875 0.006852 0.029290 ## 2020-10-16 1.061060 -0.003233 0.013251 ## 2020-10-19 1.133826 0.011311 0.031185 ## ## [5 rows x 11 columns]\rThese statistical values can be used with many others to get a lot of valuable information about specific stocks. This improves the decision making\n*** MOVING AVERAGE ***\nwe are going to derive the different moving averages . It is the arithmetic mean of all the values of the past n days. Of course this is not the only key statistic that we can derive, but it is the one we are going to use now. We can play around with other functions as well.\nWhat we are going to do with this value is to include it into our data frame and to compare it with the share price of that day.\nFor this, we will first need to create a new column. Pandas does this automatically when we assign values to a column name. This means that we don’t have to explicitly define that we are creating a new column.\n\rdf[ \u0026#39;5d_ma\u0026#39; ] = round(df[ \u0026#39;Close\u0026#39; ].rolling( window = 5 , min_periods = 0 ).mean(),2)\rdf[ \u0026#39;20d_ma\u0026#39; ] = round(df[ \u0026#39;Close\u0026#39; ].rolling( window = 20 , min_periods = 0 ).mean(),2)\rdf[\u0026#39;50d_ma\u0026#39;] = round(df[ \u0026#39;Close\u0026#39; ].rolling(window = 50, min_periods = 0).mean(),2)\rdf[\u0026#39;100d_ma\u0026#39;] = round(df[ \u0026#39;Close\u0026#39; ].rolling(window = 100, min_periods = 0).mean(),2)\rdf[\u0026#39;200d_ma\u0026#39;] = round(df[ \u0026#39;Close\u0026#39; ].rolling(window = 200, min_periods = 0).mean(),2)\r#df[ \u0026#39;20d_ma\u0026#39; ] = round(df.Close.ewm(span=21, adjust=False).mean(),2)\r#df[\u0026#39;50d_ma\u0026#39;] = round(df.Close.ewm(span=49, adjust=False).mean(),2)\r#df[\u0026#39;100d_ma\u0026#39;] = round(df.Close.ewm(span=98, adjust=False).mean(),2)\r#df[\u0026#39;200d_ma\u0026#39;] = round(df.Close.ewm(span=196, adjust=False).mean(),2)\rprint(df[[\u0026#39;Close\u0026#39;,\u0026#39;5d_ma\u0026#39;,\u0026#39;20d_ma\u0026#39;,\u0026#39;50d_ma\u0026#39;,\u0026#39;100d_ma\u0026#39;,\u0026#39;200d_ma\u0026#39;]].tail(7))\r## Close 5d_ma 20d_ma 50d_ma 100d_ma 200d_ma\r## Date ## 2020-10-09 62.884 61.27 63.03 66.10 58.78 51.57\r## 2020-10-12 63.098 61.50 62.79 66.07 58.93 51.65\r## 2020-10-13 60.530 61.49 62.37 65.96 59.05 51.72\r## 2020-10-14 61.552 61.72 62.00 65.80 59.19 51.79\r## 2020-10-15 61.420 61.90 61.63 65.59 59.32 51.86\r## 2020-10-16 61.355 61.59 61.29 65.29 59.45 51.93\r## 2020-10-19 62.049 61.38 61.00 65.05 59.57 52.00\rHere we define a three new columns with the name 20d_ma, 50d_ma, 100d_ma,200d_ma . We now fill this column with the mean values of every n entries. The rolling function stacks a specific amount of entries, in order to make a statistical calculation possible. The window parameter is the one which defines how many entries we are going to stack. But there is also the min_periods parameter. This one defines how many entries we need to have as a minimum in order to perform the calculation. This is relevant because the first entries of our data frame won’t have a n entries previous to them. By setting this value to zero we start the calculations already with the first number, even if there is not a single previous value. This has the effect that the first value will be just the first number, the second one will be the mean of the first two numbers and so on, until we get to a b values.\nBy using the mean function, we are obviously calculating the arithmetic mean. However, we can use a bunch of other functions like max, min or median if we like to.\n*** Standard Deviation ***\nThe variability of the closing stock prices determinies how vo widely prices are dispersed from the average price. If the prices are trading in narrow trading range the standard deviation will return a low value that indicates low volatility. If the prices are trading in wide trading range the standard deviation will return high value that indicates high volatility.\ndf[\u0026#39;Std_dev\u0026#39;]= df[\u0026#39;Close\u0026#39;].rolling(7).std() print(df[\u0026#39;Std_dev\u0026#39;].tail(7))\r## Date\r## 2020-10-09 1.029147\r## 2020-10-12 1.138909\r## 2020-10-13 1.194226\r## 2020-10-14 1.175008\r## 2020-10-15 1.121689\r## 2020-10-16 1.024474\r## 2020-10-19 0.906168\r## Name: Std_dev, dtype: float64\r*** Relative Strength Index ***\nThe relative strength index is a indicator of mementum used in technical analysis that measures the magnitude of current price changes to know overbought or oversold conditions in the price of a stock or other asset. If RSI is above 70 then it is overbought. If RSI is below 30 then it is oversold condition.\ndf[\u0026#39;RSI\u0026#39;] = talib.RSI(df[\u0026#39;Close\u0026#39;].values, timeperiod = 9) print(df[[\u0026#39;RSI\u0026#39;]].tail())\r## RSI\r## Date ## 2020-10-13 42.427100\r## 2020-10-14 47.448622\r## 2020-10-15 46.854813\r## 2020-10-16 46.532210\r## 2020-10-19 50.616315\r*** Wiliams %R ***\nWilliams %R, or just %R, is a technical analysis oscillator showing the current closing price in relation to the high and low of the past N days.The oscillator is on a negative scale, from −100 (lowest) up to 0 (highest). A value of −100 means the close today was the lowest low of the past N days, and 0 means today’s close was the highest high of the past N days.\ndf[\u0026#39;Williams %R\u0026#39;] = talib.WILLR(df[\u0026#39;High\u0026#39;].values,df[\u0026#39;Low\u0026#39;].values, df[\u0026#39;Close\u0026#39;].values, 7)\rprint(df[[\u0026#39;Williams %R\u0026#39;]].tail(7))\r## Williams %R\r## Date ## 2020-10-09 -9.170082\r## 2020-10-12 -21.404682\r## 2020-10-13 -75.083612\r## 2020-10-14 -53.720736\r## 2020-10-15 -56.479933\r## 2020-10-16 -64.619337\r## 2020-10-19 -48.411957\rReadings below -80 represent oversold territory and readings above -20 represent overbought.\n*** ADX ***\nADX is used to quantify trend strength. ADX calculations are based on a moving average of price range expansion over a given period of time. The average directional index (ADX) is used to determine when the price is trending strongly.\n\r0-25: Absent or Weak Trend\n\r\r25-50: Strong Trend\n\r\r50-75: Very Strong Trend\n\r\r75-100: Extremely Strong Trend\n\r\rdf[\u0026#39;ADX\u0026#39;] = talib.ADX(df[\u0026#39;High\u0026#39;].values,df[\u0026#39;Low\u0026#39;].values, df[\u0026#39;Close\u0026#39;].values, 7)\rprint(df[[\u0026#39;ADX\u0026#39;]].tail(7))\r## ADX\r## Date ## 2020-10-09 28.249595\r## 2020-10-12 27.494609\r## 2020-10-13 25.049426\r## 2020-10-14 22.953556\r## 2020-10-15 22.627446\r## 2020-10-16 20.910263\r## 2020-10-19 18.999989\r*** MACD ***\nMoving Average Convergence Divergence (MACD) is a trend-following momentum indicator that shows the relationship between two moving averages of a security’s price. The MACD is calculated by subtracting the 26-period Exponential Moving Average (EMA) from the 12-period EMA.\nShortEMA = df.Close.ewm(span=12, adjust=False).mean() #AKA Fast moving average\r#Calculate the Long Term Exponential Moving Average\rLongEMA = df.Close.ewm(span=26, adjust=False).mean() #AKA Slow moving average\r#Calculate the Moving Average Convergence/Divergence (MACD)\rMACD = ShortEMA - LongEMA\r#Calcualte the signal line\rsignal = MACD.ewm(span=9, adjust=False).mean()\rdf[\u0026#39;MACD\u0026#39;] = MACD\rdf[\u0026#39;Signal Line\u0026#39;] = signal\rdf[\u0026#39;MACD_IND\u0026#39;] = df[\u0026#39;MACD\u0026#39;] - df[\u0026#39;Signal Line\u0026#39;]\rprint(df[[\u0026#39;MACD_IND\u0026#39;]].tail())\r## MACD_IND\r## Date ## 2020-10-13 0.128242\r## 2020-10-14 0.158771\r## 2020-10-15 0.172156\r## 2020-10-16 0.177979\r## 2020-10-19 0.226788\r*** Bollinger Bands ***\nBollinger Bands are a type of statistical chart characterizing the prices and volatility over time of a financial instrument or commodity.\n\rfrom talib import MA_Type\rupper, middle, lower = talib.BBANDS(df[\u0026#39;Close\u0026#39;].values, matype=MA_Type.T3)\rprint(round(upper[-1],2),round(middle[-1],2),round(lower[-1],2))\r## 62.62 61.63 60.65\rIn case we choose another value than zero for our min_periods parameter, we will end up with a couple of NaN-Values . These are not a number values and they are useless. Therefore, we would want to delete the entries that have such values.\ndf.dropna( inplace = True )\rWe do this by using the dropna function. If we would have had any entries with NaN values in any column, they would now have been deleted\n3: Predicting the movement of stock\nTo predict the movement of the stock we use 5 lag returns as the dependent variables. The first leg is return yesterday, leg2 is return day before yesterday and so on. The dependent variable is whether the prices went up or down on that day. Other variables include the technical indicators which along with 5 lag returns are used to predict the movement of stock using logistic regression.\n3.1: Creating lag returns\n\rtslagret = pd.DataFrame(index=df.index)\rtslagret[\u0026quot;Today\u0026quot;] = df[\u0026quot;Close\u0026quot;]\rno_lags = 6\rfor i in range(0, no_lags):\rtslagret[\u0026quot;Lag%s\u0026quot; % str(i + 1)] = df[\u0026quot;Close\u0026quot;].shift(i + 1)\r3.2: Creating returns dataframe\n\rdf_ret = pd.DataFrame(index=tslagret.index)\rdf_ret[\u0026quot;Today\u0026quot;] = tslagret[\u0026quot;Today\u0026quot;].pct_change()*100.0\r3.2: create the lagged percentage returns columns\nfor i in range(0, no_lags):\rdf_ret[\u0026quot;Lag%s\u0026quot; % str(i + 1)] = tslagret[\u0026quot;Lag%s\u0026quot; % str(i + 1)].pct_change() * 100.0\rdf_ret.drop(df_ret.index[:7], inplace=True)\r3.3: “Direction” column (+1 or -1) indicating an up/down day\ndf_ret[\u0026quot;Direction\u0026quot;] = np.sign(df_ret[\u0026quot;Today\u0026quot;])\rdf_ret[\u0026quot;Direction\u0026quot;] = np.where(df_ret[\u0026quot;Direction\u0026quot;] \u0026lt;= 0, -1 , 1)\r#df_ret[\u0026quot;Direction\u0026quot;] = np.where(df[\u0026#39;Close\u0026#39;].shift(-1) \u0026gt; df[\u0026#39;Close\u0026#39;], 1,-1)\r3.4: Create the dependent and independent variables \n\rdata = df_ret.copy()\rX = data[[\u0026quot;Lag1\u0026quot;, \u0026quot;Lag2\u0026quot;, \u0026quot;Lag3\u0026quot;, \u0026quot;Lag4\u0026quot;,\u0026quot;Lag5\u0026quot;,\u0026quot;Lag6\u0026quot;]]\rX[\u0026#39;usdinr\u0026#39;] = df[\u0026#39;usdinr\u0026#39;] - 0.5\r\r## C:/ProgramData/Anaconda3/python.exe:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame.\r## Try using .loc[row_indexer,col_indexer] = value instead\r## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\rX[\u0026#39;usdsilver\u0026#39;] = df[\u0026#39;usdsilver\u0026#39;]\rX[\u0026quot;20d_ma\u0026quot;] = df[ \u0026#39;20d_ma\u0026#39; ]\rX[\u0026quot;50d_ma\u0026quot;] = df[\u0026#39;50d_ma\u0026#39;]\rX[\u0026quot;100d_ma\u0026quot;] = df[\u0026#39;100d_ma\u0026#39;]\rX[\u0026quot;200d_ma\u0026quot;] = df[\u0026#39;200d_ma\u0026#39;]\rX[\u0026#39;Std_dev\u0026#39;] = df[\u0026#39;Std_dev\u0026#39;]\rX[\u0026#39;RSI\u0026#39;] = df[\u0026#39;RSI\u0026#39;]\rX[\u0026#39;Williams %R\u0026#39;] = df[\u0026#39;Williams %R\u0026#39;]\rX[\u0026#39;MACD_IND\u0026#39;] = df[\u0026#39;MACD_IND\u0026#39;] y = data[\u0026quot;Direction\u0026quot;]\rprint(X.tail())\r## Lag1 Lag2 Lag3 Lag4 Lag5 ... 200d_ma \\\r## Date ... ## 2020-10-13 0.340309 3.907864 0.165511 -0.250945 -2.211782 ... 51.72 ## 2020-10-14 -4.069860 0.340309 3.907864 0.165511 -0.250945 ... 51.79 ## 2020-10-15 1.688419 -4.069860 0.340309 3.907864 0.165511 ... 51.86 ## 2020-10-16 -0.214453 1.688419 -4.069860 0.340309 3.907864 ... 51.93 ## 2020-10-19 -0.105829 -0.214453 1.688419 -4.069860 0.340309 ... 52.00 ## ## Std_dev RSI Williams %R MACD_IND ## Date ## 2020-10-13 1.194226 42.427100 -75.083612 0.128242 ## 2020-10-14 1.175008 47.448622 -53.720736 0.158771 ## 2020-10-15 1.121689 46.854813 -56.479933 0.172156 ## 2020-10-16 1.024474 46.532210 -64.619337 0.177979 ## 2020-10-19 0.906168 50.616315 -48.411957 0.226788 ## ## [5 rows x 16 columns]\rdata.describe()\r## Today Lag1 Lag2 Lag3 Lag4 Lag5 \\\r## count 205.000000 205.000000 205.000000 205.000000 205.000000 205.000000 ## mean 0.173542 0.166871 0.168877 0.173248 0.174040 0.202116 ## std 2.690434 2.689743 2.689692 2.689794 2.690266 2.675753 ## min -11.221052 -11.221052 -11.221052 -11.221052 -11.221052 -11.221052 ## 25% -0.850223 -0.850223 -0.850223 -0.850223 -0.850223 -0.844359 ## 50% 0.169328 0.165511 0.169328 0.195046 0.195046 0.212620 ## 75% 1.301824 1.301824 1.301824 1.301824 1.301824 1.323359 ## max 7.031671 7.031671 7.031671 7.031671 7.031671 7.031671 ## ## Lag6 Direction ## count 205.000000 205.000000 ## mean 0.201632 0.092683 ## std 2.675737 0.998133 ## min -11.221052 -1.000000 ## 25% -0.844359 -1.000000 ## 50% 0.212620 1.000000 ## 75% 1.323359 1.000000 ## max 7.031671 1.000000\r3.5: Create training and test sets\nstart_test = datetime(2020, 8, 1)\rX_train = X[X.index \u0026lt; start_test]\rX_test = X[X.index \u0026gt;= start_test]\ry_train = y[y.index \u0026lt; start_test]\ry_test = y[y.index \u0026gt;= start_test]\r3.6: Create model \n\rmodel = LogisticRegression()\r3.7: train the model on the training set\nmodel.fit(X_train, y_train)\r## LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\r## intercept_scaling=1, l1_ratio=None, max_iter=100,\r## multi_class=\u0026#39;auto\u0026#39;, n_jobs=None, penalty=\u0026#39;l2\u0026#39;,\r## random_state=None, solver=\u0026#39;lbfgs\u0026#39;, tol=0.0001, verbose=0,\r## warm_start=False)\r## ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\r## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\r## ## Increase the number of iterations (max_iter) or scale the data as shown in:\r## https://scikit-learn.org/stable/modules/preprocessing.html\r## Please also refer to the documentation for alternative solver options:\r## https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\r## extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\r3.8: make an array of predictions on the test set\ny_pred = model.predict(X_test)\r3.9: output the hit-rate and the confusion matrix for the model\nprint(\u0026#39;Accuracy: \u0026#39;,metrics.accuracy_score(y_test, y_pred))\r## Accuracy: 0.8181818181818182\rprint(confusion_matrix(y_pred,y_test))\r## [[19 1]\r## [ 9 26]]\r3.10: Predict movement of stock for tomorrow. \ncomp = pd.DataFrame()\rcomp[\u0026#39;y_test\u0026#39;] = y_test\rcomp[\u0026#39;y_pred\u0026#39;] = y_pred\rprint(comp)\r## y_test y_pred\r## Date ## 2020-08-03 1 1\r## 2020-08-04 1 1\r## 2020-08-05 1 1\r## 2020-08-06 1 1\r## 2020-08-07 -1 -1\r## 2020-08-10 1 1\r## 2020-08-11 -1 -1\r## 2020-08-12 -1 1\r## 2020-08-13 1 1\r## 2020-08-14 -1 -1\r## 2020-08-17 1 1\r## 2020-08-18 1 1\r## 2020-08-19 -1 1\r## 2020-08-20 -1 1\r## 2020-08-21 -1 -1\r## 2020-08-24 -1 -1\r## 2020-08-25 -1 -1\r## 2020-08-26 1 1\r## 2020-08-27 -1 -1\r## 2020-08-28 1 1\r## 2020-08-31 1 1\r## 2020-09-01 1 1\r## 2020-09-02 -1 -1\r## 2020-09-03 -1 -1\r## 2020-09-04 -1 -1\r## 2020-09-07 1 1\r## 2020-09-08 1 1\r## 2020-09-09 -1 1\r## 2020-09-10 1 1\r## 2020-09-11 -1 -1\r## 2020-09-14 1 1\r## 2020-09-15 1 1\r## 2020-09-16 -1 1\r## 2020-09-17 -1 1\r## 2020-09-18 -1 1\r## 2020-09-21 -1 -1\r## 2020-09-22 -1 1\r## 2020-09-23 -1 -1\r## 2020-09-24 1 1\r## 2020-09-25 -1 -1\r## 2020-09-28 1 1\r## 2020-09-29 1 1\r## 2020-09-30 -1 -1\r## 2020-10-01 1 1\r## 2020-10-05 1 1\r## 2020-10-06 -1 -1\r## 2020-10-07 -1 -1\r## 2020-10-08 1 -1\r## 2020-10-09 1 1\r## 2020-10-12 1 1\r## 2020-10-13 -1 -1\r## 2020-10-14 1 1\r## 2020-10-15 -1 -1\r## 2020-10-16 -1 1\r## 2020-10-19 1 1\rpredict = model.predict(X_test.tail(1))\rprint(int(predict)) # 1: UP, -1: DOWN\r## 1\r\r",
    "ref": "/blog/applying-logistic-regression/"
  },{
    "title": "Logistic Regression in ML",
    "date": "",
    "description": "Applying Logistic Regression to datasets",
    "body": "\r\rLogistic Regression\rIntroduction\rLogistic regression is a machine learning classification algorithm. The algorithm assigns observations to a set of classes. The logistic regration is used to predict a categorical variabble.\nComparison to linear regression\rIf we are given dataset which contains study time and exam scrores. Then\n\r\rLinear Regression helps to predic the exam scroes which is a continuous\rvariable.\rLogistic Regression can predict if the student\rpassed or failed which is a discrete categorical variable\r\r\r\rTypes of logistic regression\r\r\rBinary (Pass/Fail)\rMulti (Cats, Dogs, Sheep)\rOrdinal (Low, Medium, High)\r\r\r\r\rBinary logistic regression\rIf we are having dataset of student exam results and the objective is to predict student will pass or faile based on hours slept and hours spent studying.\n\r\rStudied\rSlept\rPassed\r\r4.85\r9.63\r1\r\r8.62\r3.23\r0\r\r5.43\r8.23\r1\r\r9.21\r6.34\r0\r\r\r\rGraph of the data\nimage\n\rSigmoid activation\rGenerally probability is assigned to predicted values. For this we use sigmoid function which maps predictions to probabilities.\nMath\n\\[S(z) = \\frac{1} {1 + e^{-z}}\\]\n\rnote\n\r\\(s(z)\\) = output between 0 and 1 (probability estimate)\r\\(z\\) = input to the function (your algorithm’s prediction e.g. mx +\r\r\r\\(e\\) = base of natural log\r\r\rGraph\nimage\n\rCode\nsigmoid \u0026lt;- function(z) {\r#SIGMOID Compute sigmoid functoon\r# J \u0026lt;- SIGMOID(z) computes the sigmoid of z.\r# You need to return the following variables correctly\rz \u0026lt;- as.matrix(z)\rg \u0026lt;- matrix(0,dim(z)[1],dim(z)[2])\r# ----------------------- YOUR CODE HERE -----------------------\r# Instructions: Compute the sigmoid of each value of z (z can be a matrix,\r# vector or scalar).\rg \u0026lt;- 1 / (1 + exp(-1 * z))\rg\r# ----------------------------------------------------\r}\r\rDecision boundary\rThe prediction function returns probability value between 0\rand 1. Map this to a discrete class (true/false) based on some threshold value.\n\\[p \\geq 0.5, class=1 \\\\ p \u0026lt; 0.5, class=0\\]\nFor example, if our threshold is .5 and our prediction function\rreturned .7, we will classify this observation as positive. If our\rprediction is .1 we would classify the observation as negative.\nimage\n\r\rMaking predictions\rTo make predictions we need to find the probability of our observations.\nMath\n\\[z = W_0 + W_1 Studied \\]\nWe can transform the output using the sigmoid function to return a probability value between 0 and 1.\n\\[P(class=1) = \\frac{1} {1 + e^{-z}}\\]\nIf the model returns .3 it believes there is only a 30% chance of\rpassing and this would be classified as fail.\nCode\npredict \u0026lt;- function(theta, X) {\rm \u0026lt;- dim(X)[1] # Number of training examples\rp \u0026lt;- rep(0,m)\rp[sigmoid(X %*% theta) \u0026gt;= 0.5] \u0026lt;- 1\rp\r# ----------------------------------------------------\r}\rA group of 20 students spend between 0 and 6 hours studying for an exam. How does the number of hours spent studying affect the probability that the student will pass the exam?\nHours \u0026lt;- c(0.50, 0.75, 1.00, 1.25, 1.50, 1.75, 1.75, 2.00, 2.25,\r2.50, 2.75, 3.00, 3.25, 3.50, 4.00, 4.25, 4.50, 4.75,\r5.00, 5.50)\rPass \u0026lt;- c(0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1)\rHrsStudying \u0026lt;- data.frame(Hours, Pass)\rThe table shows the number of hours each student spent studying, and whether they passed (1) or failed (0).\nHrsStudying_Table \u0026lt;- t(HrsStudying); HrsStudying_Table\r## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\r## Hours 0.5 0.75 1 1.25 1.5 1.75 1.75 2 2.25 2.5 2.75 3 3.25\r## Pass 0.0 0.00 0 0.00 0.0 0.00 1.00 0 1.00 0.0 1.00 0 1.00\r## [,14] [,15] [,16] [,17] [,18] [,19] [,20]\r## Hours 3.5 4 4.25 4.5 4.75 5 5.5\r## Pass 0.0 1 1.00 1.0 1.00 1 1.0\rThe graph shows the probability of passing the exam versus the number of hours studying, with the logistic regression curve fitted to the data.\nlibrary(ggplot2)\rggplot(HrsStudying, aes(Hours, Pass)) +\rgeom_point(aes()) +\rgeom_smooth(method=\u0026#39;glm\u0026#39;, family=\u0026quot;binomial\u0026quot;, se=FALSE) +\rlabs (x=\u0026quot;Hours Studying\u0026quot;, y=\u0026quot;Probability of Passing Exam\u0026quot;,\rtitle=\u0026quot;Probability of Passing Exam vs Hours Studying\u0026quot;)\r## Warning: Ignoring unknown parameters: family\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rThe logistic regression analysis gives the following output.\nmodel \u0026lt;- glm(Pass ~.,family=binomial(link=\u0026#39;logit\u0026#39;),data=HrsStudying)\rmodel$coefficients\r## (Intercept) Hours ## -4.077713 1.504645\rCoefficient Std.Error z-value P-value (Wald)\nIntercept -4.0777 1.7610 -2.316 0.0206\nHours 1.5046 0.6287 2.393 0.0167\nThe output indicates that hours studying is significantly associated with the probability of passing the exam (p=0.0167, Wald test). The output also provides the coefficients for Intercept = -4.0777 and Hours = 1.5046. These coefficients are entered in the logistic regression equation to estimate the probability of passing the exam:\n\\[P(class=1) = \\frac{1} {1 + e^{-(-4.0777+1.5046* Hours)}}\\]\nFor example, for a student who studies 3 hours, entering the value Hours = 3 in the equation gives the estimated probability of passing the exam of p = 0.60\nStudentHours \u0026lt;- 3\rProbabilityOfPassingExam \u0026lt;- 1/(1+exp(-(-4.0777+1.5046*StudentHours)))\rProbabilityOfPassingExam\r## [1] 0.6073293\rThis table shows the probability of passing the exam for several values of hours studying.\nExamPassTable \u0026lt;- data.frame(column1=c(1, 2, 3, 4, 5),\rcolumn2=c(1/(1+exp(-(-4.0777+1.5046*1))),\r1/(1+exp(-(-4.0777+1.5046*2))),\r1/(1+exp(-(-4.0777+1.5046*3))),\r1/(1+exp(-(-4.0777+1.5046*4))),\r1/(1+exp(-(-4.0777+1.5046*5)))))\rnames(ExamPassTable) \u0026lt;- c(\u0026quot;Hours of study\u0026quot;, \u0026quot;Probability of passing exam\u0026quot;)\rExamPassTable\r## Hours of study Probability of passing exam\r## 1 1 0.07088985\r## 2 2 0.25568845\r## 3 3 0.60732935\r## 4 4 0.87442903\r## 5 5 0.96909067\r\rCost function\rInstead of Mean Squared Error, we use a cost function called Cross-entropy loss can be\rdivided into two separate cost functions: one for \\(y=1\\) and one for\r\\(y=0\\).\nimage\n\rThe benefits of taking the logarithm reveal themselves when you look at\rthe cost function graphs for y=1 and y=0. These smooth monotonic\rfunctions [^2] (always increasing or always decreasing) make it easy to\rcalculate the gradient and minimize cost. Image from Andrew Ng’s slides\ron logistic regression [^3].\nimage\n\rAbove functions compressed into one\nimage\n\rMultiplying by \\(y\\) and \\((1-y)\\) in the above equation is a sneaky trick\rthat let’s us use the same equation to solve for both y=1 and y=0 cases.\rIf y=0, the first side cancels out. If y=1, the second side cancels out.\rIn both cases we only perform the operation we need to perform.\nVectorized cost function\nimage\n\rCode\ncostFunction \u0026lt;- function(X, y) {\r#COSTFUNCTION Compute cost for logistic regression\r# J \u0026lt;- COSTFUNCTION(theta, X, y) computes the cost of using theta as the\r# parameter for logistic regression.\rfunction(theta) {\r# Initialize some useful values\rm \u0026lt;- length(y) # number of training examples\r# You need to return the following variable correctly\rJ \u0026lt;- 0\rh \u0026lt;- sigmoid(X %*% theta)\rJ \u0026lt;- (t(-y) %*% log(h) - t(1 - y) %*% log(1 - h)) / m\rJ\r# ----------------------------------------------------\r}\r}\r\rGradient descent\rRemember that the general form of gradient descent is:\n\\[\\begin{align*}\u0026amp; Repeat \\; \\lbrace \\newline \u0026amp; \\; \\theta_j := \\theta_j - \\alpha \\dfrac{\\partial}{\\partial \\theta_j}J(\\theta) \\newline \u0026amp; \\rbrace\\end{align*}\\]\nWe can do the derivative using calculus to get:\n\\[\\begin{align*} \u0026amp; Repeat \\; \\lbrace \\newline \u0026amp; \\; \\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\newline \u0026amp; \\rbrace \\end{align*}\\]\nA vectorized implementation is:\n\\[\\begin{align*} \\newline \u0026amp; \\; \\theta := \\theta - \\frac{\\alpha}{m} {X^T (g(X\\theta}) - y^ \\rightarrow )\\end{align*}\\]\ngrad \u0026lt;- function(X, y) {\r#COSTFUNCTION Compute gradient for logistic regression\r# J \u0026lt;- COSTFUNCTION(theta, X, y) computes the gradient of the cost\r# w.r.t. to the parameters.\rfunction(theta) {\r# You need to return the following variable correctly\rgrad \u0026lt;- matrix(0,dim(as.matrix(theta)))\rm \u0026lt;- length(y)\rh \u0026lt;- sigmoid(X %*% theta)\r# calculate grads\rgrad \u0026lt;- (t(X) %*% (h - y)) / m\rgrad\r# ----------------------------------------------------\r}\r}\rPseudocode\nRepeat {\r1. Calculate gradient average\r2. Multiply by learning rate\r3. Subtract from weights\r}\rCost history\nimage\n\rAccuracy\nAccuracy measures how correct our predictions\rwere. In this case we simply compare predicted labels to true labels and\rdivide by the total.\nDecision boundary\nAnother helpful technique is to plot the decision boundary on top of our\rpredictions to see how our labels compare to the actual labels. This\rinvolves plotting our predicted probabilities and coloring them with\rtheir true labels.\nimage\n\r\r\rMulticlass logistic regression\rInstead of \\(y = {0,1}\\) we will expand our definition so that\r\\(y = {0,1...n}\\). Basically we re-run binary classification multiple\rtimes, once for each class.\nProcedure\r\rDivide the problem into n+1 binary classification problems (+1\rbecause the index starts at 0?).\rFor each class…\rPredict the probability the observations are in that single class.\rprediction = \u0026lt;math\u0026gt;max(probability of the classes)\r\r\rFor each sub-problem, we select one class (YES) and lump all the others\rinto a second class (NO). Then we take the class with the highest\rpredicted value.\nSince y = {0,1…n}, we divide our problem into n+1 (+1 because the index starts at 0) binary classification problems; in each one, we predict the probability that ‘y’ is a member of one of our classes.\n\\[\\begin{align*}\u0026amp; y \\in \\lbrace0, 1 ... n\\rbrace \\newline\u0026amp; h_\\theta^{(0)}(x) = P(y = 0 | x ; \\theta) \\newline\u0026amp; h_\\theta^{(1)}(x) = P(y = 1 | x ; \\theta) \\newline\u0026amp; \\cdots \\newline\u0026amp; h_\\theta^{(n)}(x) = P(y = n | x ; \\theta) \\newline\u0026amp; \\mathrm{prediction} = \\max_i( h_\\theta ^{(i)}(x) )\\newline\\end{align*}\\]\nSlide show\nknitr::include_url(\u0026#39;/slides/LogisticRegression.html\u0026#39;)\r\r\r\r\r",
    "ref": "/blog/logistic-regression/"
  },{
    "title": "Classification",
    "date": "",
    "description": "Classifcation models a function to predict a specific outcome out of mulitple possible outcomes.",
    "body": "\r\rClassification\rWith regression we now predicted specific output-values for certain given input-values. Sometimes, however, we are not trying to predict outputs but to categorize or classify our elements. For this, we use classification algorithms\nIn the figure above, we see one specific kind of classification algorithm, namely the K-Nearest-Neighbor classifier. Here we already have a decent amount of classified elements. We then add a new one (represented by the stars) and try to predict its class by looking at its nearest neighbors.\n\rCLASSIFICATION ALGORITHMS\rThere are various different classification algorithms and they are often used for predicting medical data or other real life use-cases. For example, by providing a large amount of tumor samples, we can classify if a tumor is benign or malignant with a pretty high certainty.\n\rK-NEAREST-NEIGHBORS\rAs already mentioned, by using the K-Nearest-Neighbors classifier, we assign the class of the new object, based on its nearest neighbors. The K specifies the amount of neighbors to look at. For example, we could say that we only want to look at the one neighbor who is nearest but we could also say that we want to factor in 100 neighbors.\rNotice that K shouldn’t be a multiple of the number of classes since it might cause conflicts when we have an equal amount of elements from one class as from the other.\n\rNAIVE-BAYES\rThe Naive Bayes algorithm might be a bit confusing when you encounter it the first time. However, we are only going to discuss the basics and focus more on the implementation in Python later on.\n\r\rOutlook\rTemperture\rHumidity\rWindy\rPlay\r\r\r\rSunny\rHot\rHigh\rFALSE\rNo\r\rSunny\rHot\rHigh\rTRUE\rNo\r\rRainy\rMild\rHigh\rFALSE\rNo\r\rRainy\rHot\rHigh\rTRUE\rNo\r\rOvercast\rHot\rNormal\rTRUE\rYes\r\rSunny\rHot\rNormal\rTRUE\rYes\r\rSunny\rMild\rHigh\rTRUE\rYes\r\rOvercast\rCold\rNormal\rTRUE\rNo\r\r\r\rImagine that we have a table like the one above. We have four input values (which we would have to make numerical of course) and one label or output. The two classes are Yes and No and they indicate if we are going to play outside or not.\nWhat Naive Bayes now does is to write down all the probabilities for the individual scenarios. So we would start by writing the general probability of playing and not playing. In this case, we only play three out of eight times and thus our probability of playing will be 3/8 and the probability of not playing will be 5/8.\nAlso, out of the five times we had a high humidity we only played once, whereas out of the three times it was normal, we played twice. So our probability for playing when we have a high humidity is 1/5 and for playing when we have a medium humidity is 2/3. We go on like that and note all the probabilities we have in our table. To then get the classification for a new entry, we multiply the probabilities together and end up with a prediction.\n\rLOGISTIC REGRESSION\rAnother popular classification algorithm is called logistic regression . Even though the name says regression , this is actually a classification algorithm. It looks at probabilities and determines how likely it is that a certain event happens (or a certain class is the right one), given the input data. This is done by plotting something similar to a logistic growth curve and splitting the data into two.\n\rDECISION TREES\rWith decision tree classifiers, we construct a decision tree out of our training data and use it to predict the classes of new elements.\nSince we are not using a line (and thus our model is not linear), we are also preventing mistakes caused by outliers.\nThis classification algorithm requires very little data preparation and it is also very easy to understand and visualize. On the other hand, it is very easy to be overfitting the model. Here, the model is very closely matched to the training data and thus has worse chances to make a correct prediction on new data.\n\rRANDOM FOREST\rRndom forest classifier is based on decision trees. What it does is creating a forest of multiple decision trees. To classify a new object, all the various trees determine a class and the most frequent result gets chosen. This makes the result more accurate and it also prevents overfitting. It is also more suited to handle data sets with higher dimensions. On the other hand, since the generation of the forest is random , you have very little control over your model.\n\rLOADING DATA\rNow let us get into the code. In this example, we will get our data directly from the sklearn module. For the program we need the following imports:\nimport numpy as np\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.neighbors import KNeighborsClassifier\rfrom sklearn.datasets import load_breast_cancer \rAt the last import, we import a dataset containing data on breast cancer. Also notice that we are only importing the KNeighborsClassifier for now.\ndata = load_breast_cancer()\rprint (data.feature_names)\r## [\u0026#39;mean radius\u0026#39; \u0026#39;mean texture\u0026#39; \u0026#39;mean perimeter\u0026#39; \u0026#39;mean area\u0026#39;\r## \u0026#39;mean smoothness\u0026#39; \u0026#39;mean compactness\u0026#39; \u0026#39;mean concavity\u0026#39;\r## \u0026#39;mean concave points\u0026#39; \u0026#39;mean symmetry\u0026#39; \u0026#39;mean fractal dimension\u0026#39;\r## \u0026#39;radius error\u0026#39; \u0026#39;texture error\u0026#39; \u0026#39;perimeter error\u0026#39; \u0026#39;area error\u0026#39;\r## \u0026#39;smoothness error\u0026#39; \u0026#39;compactness error\u0026#39; \u0026#39;concavity error\u0026#39;\r## \u0026#39;concave points error\u0026#39; \u0026#39;symmetry error\u0026#39; \u0026#39;fractal dimension error\u0026#39;\r## \u0026#39;worst radius\u0026#39; \u0026#39;worst texture\u0026#39; \u0026#39;worst perimeter\u0026#39; \u0026#39;worst area\u0026#39;\r## \u0026#39;worst smoothness\u0026#39; \u0026#39;worst compactness\u0026#39; \u0026#39;worst concavity\u0026#39;\r## \u0026#39;worst concave points\u0026#39; \u0026#39;worst symmetry\u0026#39; \u0026#39;worst fractal dimension\u0026#39;]\rprint (data.target_names)\r## [\u0026#39;malignant\u0026#39; \u0026#39;benign\u0026#39;]\rtargets, we have two options in this dataset: malignant and benign .\n\rPREPARING DATA\rAgain, we convert our data back into NumPy arrays and split them into training and test data.\nX = np.array(data.data)\rY = np.array(data.target)\rX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.1 )\rThe data attribute refers to our features and the target attribute points to the classes or labels. We again choose a test size of ten percent.\n\rTRAINING AND TESTING\rWe start by first defining our K-Nearest-Neighbors classifier and then training it.\nknn = KNeighborsClassifier( n_neighbors = 5 )\rknn.fit(X_train, Y_train)\r## KNeighborsClassifier(algorithm=\u0026#39;auto\u0026#39;, leaf_size=30, metric=\u0026#39;minkowski\u0026#39;,\r## metric_params=None, n_jobs=None, n_neighbors=5, p=2,\r## weights=\u0026#39;uniform\u0026#39;)\rThe n_neighbors parameter specifies how many neighbor points we want to consider. In this case, we take five. Then we test our model again for its accuracy.\naccuracy = knn.score(X_test, Y_test)\rprint (accuracy)\r## 0.9298245614035088\rWe get a pretty decent accuracy for such a complex task.\n0.9649122807017544\nTHE BEST ALGORITHM\rNow let’s put all the classification algorithms that we’ve discussed up until now to use and see which one performs best.\nfrom sklearn.neighbors import KNeighborsClassifier\rfrom sklearn.naive_bayes import GaussianNB\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.tree import DecisionTreeClassifier\rfrom sklearn.ensemble import RandomForestClassifier \rclf1 = KNeighborsClassifier( n_neighbors = 5 )\rclf2 = GaussianNB()\rclf3 = LogisticRegression()\rclf4 = DecisionTreeClassifier()\rclf5 = RandomForestClassifier()\rclf1.fit(X_train, Y_train)\r## KNeighborsClassifier(algorithm=\u0026#39;auto\u0026#39;, leaf_size=30, metric=\u0026#39;minkowski\u0026#39;,\r## metric_params=None, n_jobs=None, n_neighbors=5, p=2,\r## weights=\u0026#39;uniform\u0026#39;)\rclf2.fit(X_train, Y_train)\r## GaussianNB(priors=None, var_smoothing=1e-09)\rclf3.fit(X_train, Y_train)\r## LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\r## intercept_scaling=1, l1_ratio=None, max_iter=100,\r## multi_class=\u0026#39;auto\u0026#39;, n_jobs=None, penalty=\u0026#39;l2\u0026#39;,\r## random_state=None, solver=\u0026#39;lbfgs\u0026#39;, tol=0.0001, verbose=0,\r## warm_start=False)\r## ## C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\r## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\r## ## Increase the number of iterations (max_iter) or scale the data as shown in:\r## https://scikit-learn.org/stable/modules/preprocessing.html\r## Please also refer to the documentation for alternative solver options:\r## https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\r## extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\rclf4.fit(X_train, Y_train)\r## DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion=\u0026#39;gini\u0026#39;,\r## max_depth=None, max_features=None, max_leaf_nodes=None,\r## min_impurity_decrease=0.0, min_impurity_split=None,\r## min_samples_leaf=1, min_samples_split=2,\r## min_weight_fraction_leaf=0.0, presort=\u0026#39;deprecated\u0026#39;,\r## random_state=None, splitter=\u0026#39;best\u0026#39;)\rclf5.fit(X_train, Y_train)\r## RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\r## criterion=\u0026#39;gini\u0026#39;, max_depth=None, max_features=\u0026#39;auto\u0026#39;,\r## max_leaf_nodes=None, max_samples=None,\r## min_impurity_decrease=0.0, min_impurity_split=None,\r## min_samples_leaf=1, min_samples_split=2,\r## min_weight_fraction_leaf=0.0, n_estimators=100,\r## n_jobs=None, oob_score=False, random_state=None,\r## verbose=0, warm_start=False)\rprint (clf1.score(X_test, Y_test))\r## 0.9298245614035088\rprint (clf2.score(X_test, Y_test))\r## 0.9473684210526315\rprint (clf3.score(X_test, Y_test))\r## 0.9473684210526315\rprint (clf4.score(X_test, Y_test))\r## 0.9298245614035088\rprint (clf5.score(X_test, Y_test))\r## 0.9649122807017544\rWhen you run this program a couple of times, you will notice that we can’t really say which algorithm is the best. Every time we run this script, we will see different results, at least for this specific data set.\n\rPREDICTING LABELS\rAgain, we can again make predictions for new, unknown data. The chance of success in the classification is even very high. We just need to pass an array of input values and use the predict function .\nX_new = np.array([[...]])\rY_new = clf.predict(X_new)\r\r\r",
    "ref": "/blog/classification/"
  },{
    "title": "Student Grade Prediction",
    "date": "",
    "description": "Predicting student grade using Linear Regression",
    "body": "\r\rLinear Regression\rThe easiest and most basic machine learning algorithm is linear regression . It will be the first one that we are going to look at and it is a supervised learning algorithm. That means that we need both – inputs and outputs – to train the model.\nIf we are applying this model to data of schools and we try to find a relation between missing hours, learning time and the resulting grade, we will probably get a less accurate result than by including 30 parameters. Logically, however, we then no longer have a straight line or flat surface but a hyperplane. This is the equivalent to a straight line, in higher dimensions.\n\rLoading Data\rto get started with our code, we first need data that we want to work with. Here we use a dataset from UCI.\nLink: https://archive.ics.uci.edu/ml/datasets/student+performance\nThis is a dataset which contains a lot of information about student performance. We will use it as sample data for our models.\nWe download the ZIP-file from the Data Folder and extract the file student-mat.csv from there into the folder in which we code our script.\nNow we can start with our code. First of all, we will import the necessary libraries.\nimport numpy as np\rimport pandas as pd\rimport matplotlib.pyplot as plt\rfrom sklearn.linear_model import LinearRegression\rfrom sklearn.model_selection import train_test_split\rBesides the imports of the first three libraries that we already know, we have two more imports that are new to us. First, we import the LinearRegression module . This is the module that we will use for creating, training and using our regression model. Additionally, we import the train_test_split module, which we will use to prepare and split our data.\rOur first action is to load the data from the CSV file into a Pandas DataFrame. We do this with the function read_csv.\ndata = pd.read_csv( \u0026#39;../data/student/student-mat.csv\u0026#39; , sep = \u0026#39;;\u0026#39; )\rprint(data.tail())\r## school sex age address famsize Pstatus ... Walc health absences G1 G2 G3\r## 390 MS M 20 U LE3 A ... 5 4 11 9 9 9\r## 391 MS M 17 U LE3 T ... 4 2 3 14 16 16\r## 392 MS M 21 R GT3 T ... 3 3 3 10 8 7\r## 393 MS M 18 R LE3 T ... 4 5 0 11 12 10\r## 394 MS M 19 U LE3 T ... 3 5 5 8 9 9\r## ## [5 rows x 33 columns]\rIt is important that we change our separator to semicolon, since the default separator for CSV files is a comma and our file is separated by semicolons.\nIn the next step, we think about which features (i.e. columns) are relevant for us, and what exactly we want to predict. A description of all features can be found on the previously mentioned website. In this example, we will limit ourselves to the following columns:\nAge, Sex, Studytime, Absences, G1, G2, G3 (label)\ndata = data[[ \u0026#39;age\u0026#39; , \u0026#39;sex\u0026#39; , \u0026#39;studytime\u0026#39; ,\u0026#39;absences\u0026#39; , \u0026#39;G1\u0026#39; , \u0026#39;G2\u0026#39; , \u0026#39;G3\u0026#39; ]]\rThe columns G1, G2 and G3 are the three grades that the students get. Our goal is to predict the\rthird and final grade by looking at the other values like first grade, age, sex and so on.\nSummarized that means that we only select these columns from our DataFrame, out of the 33 possible.\nG3 is our label and the rest are our features. Each feature is an axis in the coordinate system and each point is a record, that is, one row in the table.\rBut we have a little problem here. The sex feature is not numeric, but stored as F (for female) or M (for male) . But for us to work with it and register it in the coordinate system, we have to convert it into numbers.\ndata[ \u0026#39;sex\u0026#39; ] = data[ \u0026#39;sex\u0026#39; ].map({ \u0026#39;F\u0026#39; : 0 , \u0026#39;M\u0026#39; : 1 })\rprediction = \u0026#39;G3\u0026#39; \rWe do this by using the map function. Here, we map a dictionary to our feature. Each F becomes a zero and every M becomes a one. Now we can work with it.\rFinally, we define the column of the desired label as a variable to make it easier to work with.\n\rPREPARING DATA\rOur data is now fully loaded and selected. However, in order to use it as training and testing data for our model, we have to reformat them. The sklearn models do not accept Pandas data frames, but only NumPy arrays. That’s why we turn our features into an x-array and our label into a y-array.\nX = np.array(data.drop([prediction], 1 ))\rY = np.array(data[prediction])\rThe method np.array converts the selected columns into an array. The drop function returns the data frame without the specified column. Our X array now contains all of our columns, except for the final grade. The final grade is in the Y array.\rIn order to train and test our model, we have to split our available data. The first part is used to get the hyperplane to fit our data as well as possible. The second part then checks the accuracy of the prediction, with previously unknown data.\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1 )\rWith the function train_test_split , we divide our X and Y arrays into four arrays. The order must be exactly as shown here. The test_size parameter specifies what percentage of records to use for testing. In this case, it is 10%. This is also a good and recommended value. We do this to test how accurate it is with data that our model has never seen before.\n\rTRAINING AND TESTING\rNow we can start training and testing our model. For that, we first define our model.\nmodel = LinearRegression()\rmodel.fit(X_train, Y_train)\r## LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\rBy using the constructor of the LinearRegression class, we create our model. We then use the fit function and pass our training data. Now our model is already trained. It has now adjusted its hyperplane so that it fits all of our values.\rIn order to test how well our model performs, we can use the score method and pass our testing data.\naccuracy = model.score(X_test, Y_test)\rprint (accuracy)\rSince the splitting of training and test data is always random, we will have slightly different results on each run. An average result could look like this:\r0.9130676521162756\rActually, 91 percent is a pretty high and good accuracy. Now that we know that our model is somewhat reliable, we can enter new data and predict the final grade.\nX_new = np.array([[ 18 , 1 , 3 , 40 , 15 , 16 ]])\rY_new = model.predict(X_new)\rprint (Y_new)\r## [17.47637459]\rHere we define a new NumPy array with values for our features in the right order. Then we use the predict method, to calculate the likely final grade for our inputs.\n[17.12142363]\nIn this case, the final grade would probably be 17.\n\r",
    "ref": "/blog/student-grade-prediction/"
  },{
    "title": "Linear Regression",
    "date": "",
    "description": "A linear equation that models a function such that if we give any `x` to it, it will predict a value `y` , where both `x and y` are input and output variables respectively",
    "body": "\r\rDefinition\rA linear equation that models a function such that if we give any x to it, it will predict a value y , where both x and y are input and output variables respectively. These are numerical and continous values.\rIt is the most simple and well known algorithm used in machine learning.\n\rFlowchart\r\nThe above Flowchart represents that we choose our training set, feed it to an algorithm, it will learn the patterns and will output a function called Hypothesis function 'H(x)'. We then give any x value to that function and it will output an estimated y value for it.\nFor historical reasons, this function H(x) is called hypothesis function.\n\rCost Function\rThe best fit line to our data will be where we have least distance between the predicted 'y' value and trained 'y' value.\n\rFormula for Cost Function\r\rWhere :\r- h(xi) 👉 hypothesis function\r- yi 👉 actual values of y\r- 1/m 👉 gives Mean of Squared Errors\r- 1/2 👉 Mean is halved as a convenience for the computation of the Gradient Descent.\n\rcomputeCost \u0026lt;- function (X, y, theta){\r# number of training examples\rm \u0026lt;- length(y);\r# need to return\rJ \u0026lt;- 0;\rpredictions \u0026lt;- X %*% theta;\rsqerrors = (predictions - y)^2;\rJ = 1/(2*m)* sum(sqerrors);\rJ\r}\rThis formula inputs the sum of the distances between predicted values and actual values of training set, does sqaure it and take the average and multiply it by 0.5\rThis cost function is also called as Squared Error Function or Mean Squared Error.\rWhy do we take squares of the error’s?\nThe MSE function is commonly used and is a reasonable choice and works well for most Regression problems.\rLet’s subsititute MSE function to function J :\r\n\rGradient Descent\rSo now we have our hypothesis function and we have a way of measuring how well it fits into the data. Now we need to estimate the parameters in the hypothesis function. That’s where Gradient Descent comes in.\nGradient Descent is used to minimize the cost function J, minimizing J is same as minimizing MSE to get best possible fit line to our data.\n\\[\\displaystyle \\min_{\\theta_0,\\theta_1}\\frac{1}{2m}\\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)^2\\]\n\rFormula for Gradient Descent\r\rWhere :\r- := Is the Assignment Operator\r- α is Alpha, it’s the number which is called learning rate. If its too high it may fail to converge and if too low then descending will be slow.\r- ‘θj’ Taking Gradient Descent of a feature or a column of a dataset.\r- ∂/(∂θj) J(θ0,θ1) Taking partial derivative of MSE cost function.\n\r\n gradientDescent \u0026lt;- function(X, y, theta, alpha, num_iters){\rm \u0026lt;- length(y); J_history = rep(0, num_iters);\rfor (iter in 1:num_iters){\rpredictions \u0026lt;- X %*% theta;\rupdates = t(X) %*% (predictions - y);\rtheta = theta - alpha * (1/m) * updates;\rJ_history[iter] \u0026lt;- computeCost(X, y, theta);\r}\rlist(\u0026quot;theta\u0026quot; = theta, \u0026quot;J_history\u0026quot; = J_history) }\rNow Let’s apply Gradient Descend to minmize our MSE function.\rIn order to apply Gradient Descent, we need to figure out the partial derivative term.\nSo lets solve partial derivative of cost function J.\n\n\nNow let’s plug these two values to our Gradient Descent:\n\n\nApplications\r\rSales Forecasting\rDemand Supply Forecasting\rOperations cost optimization\rInsurance industry - claim prediction\rBanking\rHealthcare industry - cost prediction\rEcommerce industry - Recommandation System\r\r\rKey Points\r\rIf sample is small ( \u0026lt; 10000) then normal equation can be used to get the theta values\n\rAs the training set size increases it is better to use gradient descent algorithm instead of normal equation\n\rIf sample data contains large digits for x, y values then it is better to scale the values around mean before applying cost function and gradient descent\n\rIn R language lm(x~y) can be used directly for determining theta values which is more efficient than using gradient descent algorithm\n\rInitially it is better to calculate correlation coeficient to ensure that variables are related in some way\n\rNormalizing data is important to deal with when individual values are numerically large ( \u0026gt; 4 digits)\n\r\r\rSlide show\rknitr::include_url(\u0026#39;/slides/GradientDescentLR.html\u0026#39;)\r\r\rSlide show (Multiple Features)\rknitr::include_url(\u0026#39;/slides/MultipleFeaturesLR.html\u0026#39;)\r\r\r\r",
    "ref": "/blog/linear-regression/"
  },{
    "title": "Time-Series Forecasting",
    "date": "",
    "description": "Introduction to time-series forecasting",
    "body": "\r\rWhat is time series?\rA series of observations collected during some time intervals in know as time series. The observations which are collected are primarily dependent on the time at which it is collected. The sale of the items in a retail store like Walmart will be a time series. The periodicity of the the sale could be at daily level or weekly level. The number of people flying from Bangalore to Delhi on daily basis is a time series.\rTime-series forecasting uses historical time based data to forecast a variable for example\n\rAt Supermarket the demand for a grocery product each day\rCount of the downloads of an App in a country for the coming month\rProduct sales in units sold each day in a retail store\rYearly unemployment rate\rThe closing price of a stock\rExpected yearly yield of a crop\rPopulation growth in a country\rDaily demand for a bus/airline/train\r\rTime series and a normal series?\rTime component is important in time series. The time series is primarily dependent on the time. But a normal series say 1, 2, 3…100 has no time component to it. When a value that a series will take depends on the time it was recorded, it is called as time series.\n\rHow to define a time series object using R language\rThe function ts() helps to create time-series objects. As.ts() and is.ts() coerce an object to a time-series and test whether an object is a time series or not\rfor example:\n\rFrequency\rThe count of number of observations per unit of time is called as frequency. Frequency can further be distinguished as Frequency for yearly data is 1, Frequency for weekly data is 7, Frequency for quarterly data is 4 and so on…\n\rSome useful softwares\rforecast: For forecasting functions\n\rtseries: For unit root tests and GARC models\n\rMcomp: Time series data from forecasting competitions\n\rfma: For data\n\rexpsmooth: For data\n\rfpp: For data\n\rARIMA\n\rprophet\n\rSPSS\n\r\r\rprophet\rProphet represents a time-series as a combination of trend, seasonality and holidays. This decomposed time series model can be represented by the following equation:\nCapture10\n\rwhere:\n\rg(t) defines the trend function that models the non-periodic changes in the time series\n\rs(t) defines the periodic changes (e.g., weekly and yearly seasonality)\n\rh(t) means the effect of holidays\n\rεt: is the error notation representing information that was missed by the model\n\r\r\rTrend\rProphet provides two variants for the trend function, g(t) which are logistic growth modeling non-linear growth and constant rate of growth or Piece wise Linear Model.\n\rSeasonality\rs(t) is called as the seasonal component which allows to flexibly model the periodic changes due to weekly and yearly seasonability. For example, a 5-day work week can produce effects on a time series that repeat each week, while vacation schedules and school breaks can produce effects that repeat each year.\n\rHolidays and Events\rThere are events which can provide predictable outcome to business. These events are holidays or any other important ocasion such as black friday.Prophet provides list of such past and future events for modeling into time-series component.\nlibrary(Quandl)\rlibrary(quantmod)\rlibrary(prophet)\rQuandl.api_key(\u0026quot;YBM9uPgpnsaDYPkAn539\u0026quot;)\r#daily_gold = Quandl(\u0026quot;WGC/GOLD_DAILY_INR\u0026quot;,collapse=\u0026quot;daily\u0026quot;,start_date=\u0026quot;2000-01-01\u0026quot;,end_date = \u0026quot;2020-08-11\u0026quot;,type=\u0026quot;raw\u0026quot;)\rdaily_gold = Quandl(\u0026quot;LBMA/SILVER\u0026quot;,collapse=\u0026quot;daily\u0026quot;,start_date=\u0026quot;2020-01-01\u0026quot;,end_date = \u0026quot;2020-08-12\u0026quot;,type=\u0026quot;raw\u0026quot;)\rkeeps \u0026lt;- c(\u0026quot;Date\u0026quot;, \u0026quot;USD\u0026quot;)\rdaily_gold = daily_gold[keeps]\rnames(daily_gold) \u0026lt;- c(\u0026#39;ds\u0026#39;,\u0026#39;y\u0026#39;)\rdaily_gold[2] \u0026lt;- daily_gold[2] model = prophet(daily_gold)\rfuture \u0026lt;- make_future_dataframe(model, periods = 10)\rforecast \u0026lt;- predict(model, future)\rprint(forecast)\r## ds trend additive_terms additive_terms_lower\r## 1 2020-01-02 18.67220 0.1719716 0.1719716\r## 2 2020-01-03 18.63388 0.1920116 0.1920116\r## 3 2020-01-06 18.51890 0.1161517 0.1161517\r## 4 2020-01-07 18.48058 0.1067298 0.1067298\r## 5 2020-01-08 18.44225 0.2188640 0.2188640\r## 6 2020-01-09 18.40392 0.1719716 0.1719716\r## 7 2020-01-10 18.36560 0.1920116 0.1920116\r## 8 2020-01-13 18.25062 0.1161517 0.1161517\r## 9 2020-01-14 18.21230 0.1067298 0.1067298\r## 10 2020-01-15 18.17397 0.2188640 0.2188640\r## 11 2020-01-16 18.13565 0.1719716 0.1719716\r## 12 2020-01-17 18.09732 0.1920116 0.1920116\r## 13 2020-01-20 17.98235 0.1161517 0.1161517\r## 14 2020-01-21 17.94402 0.1067298 0.1067298\r## 15 2020-01-22 17.90570 0.2188640 0.2188640\r## 16 2020-01-23 17.86737 0.1719716 0.1719716\r## 17 2020-01-24 17.82905 0.1920116 0.1920116\r## 18 2020-01-27 17.71407 0.1161517 0.1161517\r## 19 2020-01-28 17.67574 0.1067298 0.1067298\r## 20 2020-01-29 17.63742 0.2188640 0.2188640\r## 21 2020-01-30 17.59909 0.1719716 0.1719716\r## 22 2020-01-31 17.56077 0.1920116 0.1920116\r## 23 2020-02-03 17.44579 0.1161517 0.1161517\r## 24 2020-02-04 17.40747 0.1067298 0.1067298\r## 25 2020-02-05 17.36914 0.2188640 0.2188640\r## 26 2020-02-06 17.33082 0.1719716 0.1719716\r## 27 2020-02-07 17.29176 0.1920116 0.1920116\r## 28 2020-02-10 17.17459 0.1161517 0.1161517\r## 29 2020-02-11 17.13553 0.1067298 0.1067298\r## 30 2020-02-12 17.09647 0.2188640 0.2188640\r## 31 2020-02-13 17.05742 0.1719716 0.1719716\r## 32 2020-02-14 17.00788 0.1920116 0.1920116\r## 33 2020-02-17 16.85928 0.1161517 0.1161517\r## 34 2020-02-18 16.80975 0.1067298 0.1067298\r## 35 2020-02-19 16.76021 0.2188640 0.2188640\r## 36 2020-02-20 16.70263 0.1719716 0.1719716\r## 37 2020-02-21 16.64505 0.1920116 0.1920116\r## 38 2020-02-24 16.47231 0.1161517 0.1161517\r## 39 2020-02-25 16.41472 0.1067298 0.1067298\r## 40 2020-02-26 16.35714 0.2188640 0.2188640\r## 41 2020-02-27 16.29913 0.1719716 0.1719716\r## 42 2020-02-28 16.24113 0.1920116 0.1920116\r## 43 2020-03-02 16.06710 0.1161517 0.1161517\r## 44 2020-03-03 16.00909 0.1067298 0.1067298\r## 45 2020-03-04 15.95108 0.2188640 0.2188640\r## 46 2020-03-05 15.89307 0.1719716 0.1719716\r## 47 2020-03-06 15.83507 0.1920116 0.1920116\r## 48 2020-03-09 15.66104 0.1161517 0.1161517\r## 49 2020-03-10 15.60303 0.1067298 0.1067298\r## 50 2020-03-11 15.54502 0.2188640 0.2188640\r## 51 2020-03-12 15.48701 0.1719716 0.1719716\r## 52 2020-03-13 15.42900 0.1920116 0.1920116\r## 53 2020-03-16 15.25498 0.1161517 0.1161517\r## 54 2020-03-17 15.19697 0.1067298 0.1067298\r## 55 2020-03-18 15.13896 0.2188640 0.2188640\r## 56 2020-03-19 15.08098 0.1719716 0.1719716\r## 57 2020-03-20 15.02300 0.1920116 0.1920116\r## 58 2020-03-23 14.84905 0.1161517 0.1161517\r## 59 2020-03-24 14.79107 0.1067298 0.1067298\r## 60 2020-03-25 14.73308 0.2188640 0.2188640\r## 61 2020-03-26 14.70644 0.1719716 0.1719716\r## 62 2020-03-27 14.67979 0.1920116 0.1920116\r## 63 2020-03-30 14.59985 0.1161517 0.1161517\r## 64 2020-03-31 14.57320 0.1067298 0.1067298\r## 65 2020-04-01 14.54656 0.2188640 0.2188640\r## 66 2020-04-02 14.55587 0.1719716 0.1719716\r## 67 2020-04-03 14.56517 0.1920116 0.1920116\r## 68 2020-04-06 14.59310 0.1161517 0.1161517\r## 69 2020-04-07 14.60241 0.1067298 0.1067298\r## 70 2020-04-08 14.61172 0.2188640 0.2188640\r## 71 2020-04-09 14.64302 0.1719716 0.1719716\r## 72 2020-04-14 14.79949 0.1067298 0.1067298\r## 73 2020-04-15 14.83079 0.2188640 0.2188640\r## 74 2020-04-16 14.86208 0.1719716 0.1719716\r## 75 2020-04-17 14.89338 0.1920116 0.1920116\r## 76 2020-04-20 15.00470 0.1161517 0.1161517\r## 77 2020-04-21 15.04181 0.1067298 0.1067298\r## 78 2020-04-22 15.07892 0.2188640 0.2188640\r## 79 2020-04-23 15.11603 0.1719716 0.1719716\r## 80 2020-04-24 15.15314 0.1920116 0.1920116\r## 81 2020-04-27 15.26628 0.1161517 0.1161517\r## 82 2020-04-28 15.30399 0.1067298 0.1067298\r## 83 2020-04-29 15.34170 0.2188640 0.2188640\r## 84 2020-04-30 15.37942 0.1719716 0.1719716\r## 85 2020-05-01 15.41713 0.1920116 0.1920116\r## 86 2020-05-04 15.53329 0.1161517 0.1161517\r## 87 2020-05-05 15.57200 0.1067298 0.1067298\r## 88 2020-05-06 15.61072 0.2188640 0.2188640\r## 89 2020-05-07 15.64944 0.1719716 0.1719716\r## 90 2020-05-11 15.80432 0.1161517 0.1161517\r## 91 2020-05-12 15.84304 0.1067298 0.1067298\r## 92 2020-05-13 15.88176 0.2188640 0.2188640\r## 93 2020-05-14 15.92048 0.1719716 0.1719716\r## 94 2020-05-15 15.95920 0.1920116 0.1920116\r## 95 2020-05-18 16.07536 0.1161517 0.1161517\r## 96 2020-05-19 16.11408 0.1067298 0.1067298\r## 97 2020-05-20 16.15280 0.2188640 0.2188640\r## 98 2020-05-21 16.19152 0.1719716 0.1719716\r## 99 2020-05-22 16.23024 0.1920116 0.1920116\r## 100 2020-05-26 16.38512 0.1067298 0.1067298\r## 101 2020-05-27 16.42384 0.2188640 0.2188640\r## 102 2020-05-28 16.46256 0.1719716 0.1719716\r## 103 2020-05-29 16.50128 0.1920116 0.1920116\r## 104 2020-06-01 16.61744 0.1161517 0.1161517\r## 105 2020-06-02 16.65616 0.1067298 0.1067298\r## 106 2020-06-03 16.69488 0.2188640 0.2188640\r## 107 2020-06-04 16.73360 0.1719716 0.1719716\r## 108 2020-06-05 16.77232 0.1920116 0.1920116\r## 109 2020-06-08 16.88848 0.1161517 0.1161517\r## 110 2020-06-09 16.92720 0.1067298 0.1067298\r## 111 2020-06-10 16.96592 0.2188640 0.2188640\r## 112 2020-06-11 17.00464 0.1719716 0.1719716\r## 113 2020-06-12 17.04335 0.1920116 0.1920116\r## 114 2020-06-15 17.15951 0.1161517 0.1161517\r## 115 2020-06-16 17.19823 0.1067298 0.1067298\r## 116 2020-06-17 17.23695 0.2188640 0.2188640\r## 117 2020-06-18 17.27567 0.1719716 0.1719716\r## 118 2020-06-19 17.31439 0.1920116 0.1920116\r## 119 2020-06-22 17.43055 0.1161517 0.1161517\r## 120 2020-06-23 17.51737 0.1067298 0.1067298\r## 121 2020-06-24 17.60418 0.2188640 0.2188640\r## 122 2020-06-25 17.69100 0.1719716 0.1719716\r## 123 2020-06-26 17.77781 0.1920116 0.1920116\r## 124 2020-06-29 18.03825 0.1161517 0.1161517\r## 125 2020-06-30 18.21636 0.1067298 0.1067298\r## 126 2020-07-01 18.39447 0.2188640 0.2188640\r## 127 2020-07-02 18.57258 0.1719716 0.1719716\r## 128 2020-07-03 18.75069 0.1920116 0.1920116\r## 129 2020-07-06 19.28501 0.1161517 0.1161517\r## 130 2020-07-07 19.46312 0.1067298 0.1067298\r## 131 2020-07-08 19.64123 0.2188640 0.2188640\r## 132 2020-07-09 19.81934 0.1719716 0.1719716\r## 133 2020-07-10 19.99745 0.1920116 0.1920116\r## 134 2020-07-13 20.53177 0.1161517 0.1161517\r## 135 2020-07-14 20.70988 0.1067298 0.1067298\r## 136 2020-07-15 20.88799 0.2188640 0.2188640\r## 137 2020-07-16 21.06610 0.1719716 0.1719716\r## 138 2020-07-17 21.24421 0.1920116 0.1920116\r## 139 2020-07-20 21.77853 0.1161517 0.1161517\r## 140 2020-07-21 21.95664 0.1067298 0.1067298\r## 141 2020-07-22 22.13475 0.2188640 0.2188640\r## 142 2020-07-23 22.31286 0.1719716 0.1719716\r## 143 2020-07-24 22.49097 0.1920116 0.1920116\r## 144 2020-07-27 23.02529 0.1161517 0.1161517\r## 145 2020-07-28 23.20340 0.1067298 0.1067298\r## 146 2020-07-29 23.38151 0.2188640 0.2188640\r## 147 2020-07-30 23.55962 0.1719716 0.1719716\r## 148 2020-07-31 23.73773 0.1920116 0.1920116\r## 149 2020-08-03 24.27206 0.1161517 0.1161517\r## 150 2020-08-04 24.45016 0.1067298 0.1067298\r## 151 2020-08-05 24.62827 0.2188640 0.2188640\r## 152 2020-08-06 24.80638 0.1719716 0.1719716\r## 153 2020-08-07 24.98449 0.1920116 0.1920116\r## 154 2020-08-10 25.51882 0.1161517 0.1161517\r## 155 2020-08-11 25.69692 0.1067298 0.1067298\r## 156 2020-08-12 25.87503 0.2188640 0.2188640\r## 157 2020-08-13 26.05314 0.1719716 0.1719716\r## 158 2020-08-14 26.23125 0.1920116 0.1920116\r## 159 2020-08-15 26.40936 -0.4028644 -0.4028644\r## 160 2020-08-16 26.58747 -0.4028644 -0.4028644\r## 161 2020-08-17 26.76558 0.1161517 0.1161517\r## 162 2020-08-18 26.94368 0.1067298 0.1067298\r## 163 2020-08-19 27.12179 0.2188640 0.2188640\r## 164 2020-08-20 27.29990 0.1719716 0.1719716\r## 165 2020-08-21 27.47801 0.1920116 0.1920116\r## 166 2020-08-22 27.65612 -0.4028644 -0.4028644\r## additive_terms_upper weekly weekly_lower weekly_upper\r## 1 0.1719716 0.1719716 0.1719716 0.1719716\r## 2 0.1920116 0.1920116 0.1920116 0.1920116\r## 3 0.1161517 0.1161517 0.1161517 0.1161517\r## 4 0.1067298 0.1067298 0.1067298 0.1067298\r## 5 0.2188640 0.2188640 0.2188640 0.2188640\r## 6 0.1719716 0.1719716 0.1719716 0.1719716\r## 7 0.1920116 0.1920116 0.1920116 0.1920116\r## 8 0.1161517 0.1161517 0.1161517 0.1161517\r## 9 0.1067298 0.1067298 0.1067298 0.1067298\r## 10 0.2188640 0.2188640 0.2188640 0.2188640\r## 11 0.1719716 0.1719716 0.1719716 0.1719716\r## 12 0.1920116 0.1920116 0.1920116 0.1920116\r## 13 0.1161517 0.1161517 0.1161517 0.1161517\r## 14 0.1067298 0.1067298 0.1067298 0.1067298\r## 15 0.2188640 0.2188640 0.2188640 0.2188640\r## 16 0.1719716 0.1719716 0.1719716 0.1719716\r## 17 0.1920116 0.1920116 0.1920116 0.1920116\r## 18 0.1161517 0.1161517 0.1161517 0.1161517\r## 19 0.1067298 0.1067298 0.1067298 0.1067298\r## 20 0.2188640 0.2188640 0.2188640 0.2188640\r## 21 0.1719716 0.1719716 0.1719716 0.1719716\r## 22 0.1920116 0.1920116 0.1920116 0.1920116\r## 23 0.1161517 0.1161517 0.1161517 0.1161517\r## 24 0.1067298 0.1067298 0.1067298 0.1067298\r## 25 0.2188640 0.2188640 0.2188640 0.2188640\r## 26 0.1719716 0.1719716 0.1719716 0.1719716\r## 27 0.1920116 0.1920116 0.1920116 0.1920116\r## 28 0.1161517 0.1161517 0.1161517 0.1161517\r## 29 0.1067298 0.1067298 0.1067298 0.1067298\r## 30 0.2188640 0.2188640 0.2188640 0.2188640\r## 31 0.1719716 0.1719716 0.1719716 0.1719716\r## 32 0.1920116 0.1920116 0.1920116 0.1920116\r## 33 0.1161517 0.1161517 0.1161517 0.1161517\r## 34 0.1067298 0.1067298 0.1067298 0.1067298\r## 35 0.2188640 0.2188640 0.2188640 0.2188640\r## 36 0.1719716 0.1719716 0.1719716 0.1719716\r## 37 0.1920116 0.1920116 0.1920116 0.1920116\r## 38 0.1161517 0.1161517 0.1161517 0.1161517\r## 39 0.1067298 0.1067298 0.1067298 0.1067298\r## 40 0.2188640 0.2188640 0.2188640 0.2188640\r## 41 0.1719716 0.1719716 0.1719716 0.1719716\r## 42 0.1920116 0.1920116 0.1920116 0.1920116\r## 43 0.1161517 0.1161517 0.1161517 0.1161517\r## 44 0.1067298 0.1067298 0.1067298 0.1067298\r## 45 0.2188640 0.2188640 0.2188640 0.2188640\r## 46 0.1719716 0.1719716 0.1719716 0.1719716\r## 47 0.1920116 0.1920116 0.1920116 0.1920116\r## 48 0.1161517 0.1161517 0.1161517 0.1161517\r## 49 0.1067298 0.1067298 0.1067298 0.1067298\r## 50 0.2188640 0.2188640 0.2188640 0.2188640\r## 51 0.1719716 0.1719716 0.1719716 0.1719716\r## 52 0.1920116 0.1920116 0.1920116 0.1920116\r## 53 0.1161517 0.1161517 0.1161517 0.1161517\r## 54 0.1067298 0.1067298 0.1067298 0.1067298\r## 55 0.2188640 0.2188640 0.2188640 0.2188640\r## 56 0.1719716 0.1719716 0.1719716 0.1719716\r## 57 0.1920116 0.1920116 0.1920116 0.1920116\r## 58 0.1161517 0.1161517 0.1161517 0.1161517\r## 59 0.1067298 0.1067298 0.1067298 0.1067298\r## 60 0.2188640 0.2188640 0.2188640 0.2188640\r## 61 0.1719716 0.1719716 0.1719716 0.1719716\r## 62 0.1920116 0.1920116 0.1920116 0.1920116\r## 63 0.1161517 0.1161517 0.1161517 0.1161517\r## 64 0.1067298 0.1067298 0.1067298 0.1067298\r## 65 0.2188640 0.2188640 0.2188640 0.2188640\r## 66 0.1719716 0.1719716 0.1719716 0.1719716\r## 67 0.1920116 0.1920116 0.1920116 0.1920116\r## 68 0.1161517 0.1161517 0.1161517 0.1161517\r## 69 0.1067298 0.1067298 0.1067298 0.1067298\r## 70 0.2188640 0.2188640 0.2188640 0.2188640\r## 71 0.1719716 0.1719716 0.1719716 0.1719716\r## 72 0.1067298 0.1067298 0.1067298 0.1067298\r## 73 0.2188640 0.2188640 0.2188640 0.2188640\r## 74 0.1719716 0.1719716 0.1719716 0.1719716\r## 75 0.1920116 0.1920116 0.1920116 0.1920116\r## 76 0.1161517 0.1161517 0.1161517 0.1161517\r## 77 0.1067298 0.1067298 0.1067298 0.1067298\r## 78 0.2188640 0.2188640 0.2188640 0.2188640\r## 79 0.1719716 0.1719716 0.1719716 0.1719716\r## 80 0.1920116 0.1920116 0.1920116 0.1920116\r## 81 0.1161517 0.1161517 0.1161517 0.1161517\r## 82 0.1067298 0.1067298 0.1067298 0.1067298\r## 83 0.2188640 0.2188640 0.2188640 0.2188640\r## 84 0.1719716 0.1719716 0.1719716 0.1719716\r## 85 0.1920116 0.1920116 0.1920116 0.1920116\r## 86 0.1161517 0.1161517 0.1161517 0.1161517\r## 87 0.1067298 0.1067298 0.1067298 0.1067298\r## 88 0.2188640 0.2188640 0.2188640 0.2188640\r## 89 0.1719716 0.1719716 0.1719716 0.1719716\r## 90 0.1161517 0.1161517 0.1161517 0.1161517\r## 91 0.1067298 0.1067298 0.1067298 0.1067298\r## 92 0.2188640 0.2188640 0.2188640 0.2188640\r## 93 0.1719716 0.1719716 0.1719716 0.1719716\r## 94 0.1920116 0.1920116 0.1920116 0.1920116\r## 95 0.1161517 0.1161517 0.1161517 0.1161517\r## 96 0.1067298 0.1067298 0.1067298 0.1067298\r## 97 0.2188640 0.2188640 0.2188640 0.2188640\r## 98 0.1719716 0.1719716 0.1719716 0.1719716\r## 99 0.1920116 0.1920116 0.1920116 0.1920116\r## 100 0.1067298 0.1067298 0.1067298 0.1067298\r## 101 0.2188640 0.2188640 0.2188640 0.2188640\r## 102 0.1719716 0.1719716 0.1719716 0.1719716\r## 103 0.1920116 0.1920116 0.1920116 0.1920116\r## 104 0.1161517 0.1161517 0.1161517 0.1161517\r## 105 0.1067298 0.1067298 0.1067298 0.1067298\r## 106 0.2188640 0.2188640 0.2188640 0.2188640\r## 107 0.1719716 0.1719716 0.1719716 0.1719716\r## 108 0.1920116 0.1920116 0.1920116 0.1920116\r## 109 0.1161517 0.1161517 0.1161517 0.1161517\r## 110 0.1067298 0.1067298 0.1067298 0.1067298\r## 111 0.2188640 0.2188640 0.2188640 0.2188640\r## 112 0.1719716 0.1719716 0.1719716 0.1719716\r## 113 0.1920116 0.1920116 0.1920116 0.1920116\r## 114 0.1161517 0.1161517 0.1161517 0.1161517\r## 115 0.1067298 0.1067298 0.1067298 0.1067298\r## 116 0.2188640 0.2188640 0.2188640 0.2188640\r## 117 0.1719716 0.1719716 0.1719716 0.1719716\r## 118 0.1920116 0.1920116 0.1920116 0.1920116\r## 119 0.1161517 0.1161517 0.1161517 0.1161517\r## 120 0.1067298 0.1067298 0.1067298 0.1067298\r## 121 0.2188640 0.2188640 0.2188640 0.2188640\r## 122 0.1719716 0.1719716 0.1719716 0.1719716\r## 123 0.1920116 0.1920116 0.1920116 0.1920116\r## 124 0.1161517 0.1161517 0.1161517 0.1161517\r## 125 0.1067298 0.1067298 0.1067298 0.1067298\r## 126 0.2188640 0.2188640 0.2188640 0.2188640\r## 127 0.1719716 0.1719716 0.1719716 0.1719716\r## 128 0.1920116 0.1920116 0.1920116 0.1920116\r## 129 0.1161517 0.1161517 0.1161517 0.1161517\r## 130 0.1067298 0.1067298 0.1067298 0.1067298\r## 131 0.2188640 0.2188640 0.2188640 0.2188640\r## 132 0.1719716 0.1719716 0.1719716 0.1719716\r## 133 0.1920116 0.1920116 0.1920116 0.1920116\r## 134 0.1161517 0.1161517 0.1161517 0.1161517\r## 135 0.1067298 0.1067298 0.1067298 0.1067298\r## 136 0.2188640 0.2188640 0.2188640 0.2188640\r## 137 0.1719716 0.1719716 0.1719716 0.1719716\r## 138 0.1920116 0.1920116 0.1920116 0.1920116\r## 139 0.1161517 0.1161517 0.1161517 0.1161517\r## 140 0.1067298 0.1067298 0.1067298 0.1067298\r## 141 0.2188640 0.2188640 0.2188640 0.2188640\r## 142 0.1719716 0.1719716 0.1719716 0.1719716\r## 143 0.1920116 0.1920116 0.1920116 0.1920116\r## 144 0.1161517 0.1161517 0.1161517 0.1161517\r## 145 0.1067298 0.1067298 0.1067298 0.1067298\r## 146 0.2188640 0.2188640 0.2188640 0.2188640\r## 147 0.1719716 0.1719716 0.1719716 0.1719716\r## 148 0.1920116 0.1920116 0.1920116 0.1920116\r## 149 0.1161517 0.1161517 0.1161517 0.1161517\r## 150 0.1067298 0.1067298 0.1067298 0.1067298\r## 151 0.2188640 0.2188640 0.2188640 0.2188640\r## 152 0.1719716 0.1719716 0.1719716 0.1719716\r## 153 0.1920116 0.1920116 0.1920116 0.1920116\r## 154 0.1161517 0.1161517 0.1161517 0.1161517\r## 155 0.1067298 0.1067298 0.1067298 0.1067298\r## 156 0.2188640 0.2188640 0.2188640 0.2188640\r## 157 0.1719716 0.1719716 0.1719716 0.1719716\r## 158 0.1920116 0.1920116 0.1920116 0.1920116\r## 159 -0.4028644 -0.4028644 -0.4028644 -0.4028644\r## 160 -0.4028644 -0.4028644 -0.4028644 -0.4028644\r## 161 0.1161517 0.1161517 0.1161517 0.1161517\r## 162 0.1067298 0.1067298 0.1067298 0.1067298\r## 163 0.2188640 0.2188640 0.2188640 0.2188640\r## 164 0.1719716 0.1719716 0.1719716 0.1719716\r## 165 0.1920116 0.1920116 0.1920116 0.1920116\r## 166 -0.4028644 -0.4028644 -0.4028644 -0.4028644\r## multiplicative_terms multiplicative_terms_lower multiplicative_terms_upper\r## 1 0 0 0\r## 2 0 0 0\r## 3 0 0 0\r## 4 0 0 0\r## 5 0 0 0\r## 6 0 0 0\r## 7 0 0 0\r## 8 0 0 0\r## 9 0 0 0\r## 10 0 0 0\r## 11 0 0 0\r## 12 0 0 0\r## 13 0 0 0\r## 14 0 0 0\r## 15 0 0 0\r## 16 0 0 0\r## 17 0 0 0\r## 18 0 0 0\r## 19 0 0 0\r## 20 0 0 0\r## 21 0 0 0\r## 22 0 0 0\r## 23 0 0 0\r## 24 0 0 0\r## 25 0 0 0\r## 26 0 0 0\r## 27 0 0 0\r## 28 0 0 0\r## 29 0 0 0\r## 30 0 0 0\r## 31 0 0 0\r## 32 0 0 0\r## 33 0 0 0\r## 34 0 0 0\r## 35 0 0 0\r## 36 0 0 0\r## 37 0 0 0\r## 38 0 0 0\r## 39 0 0 0\r## 40 0 0 0\r## 41 0 0 0\r## 42 0 0 0\r## 43 0 0 0\r## 44 0 0 0\r## 45 0 0 0\r## 46 0 0 0\r## 47 0 0 0\r## 48 0 0 0\r## 49 0 0 0\r## 50 0 0 0\r## 51 0 0 0\r## 52 0 0 0\r## 53 0 0 0\r## 54 0 0 0\r## 55 0 0 0\r## 56 0 0 0\r## 57 0 0 0\r## 58 0 0 0\r## 59 0 0 0\r## 60 0 0 0\r## 61 0 0 0\r## 62 0 0 0\r## 63 0 0 0\r## 64 0 0 0\r## 65 0 0 0\r## 66 0 0 0\r## 67 0 0 0\r## 68 0 0 0\r## 69 0 0 0\r## 70 0 0 0\r## 71 0 0 0\r## 72 0 0 0\r## 73 0 0 0\r## 74 0 0 0\r## 75 0 0 0\r## 76 0 0 0\r## 77 0 0 0\r## 78 0 0 0\r## 79 0 0 0\r## 80 0 0 0\r## 81 0 0 0\r## 82 0 0 0\r## 83 0 0 0\r## 84 0 0 0\r## 85 0 0 0\r## 86 0 0 0\r## 87 0 0 0\r## 88 0 0 0\r## 89 0 0 0\r## 90 0 0 0\r## 91 0 0 0\r## 92 0 0 0\r## 93 0 0 0\r## 94 0 0 0\r## 95 0 0 0\r## 96 0 0 0\r## 97 0 0 0\r## 98 0 0 0\r## 99 0 0 0\r## 100 0 0 0\r## 101 0 0 0\r## 102 0 0 0\r## 103 0 0 0\r## 104 0 0 0\r## 105 0 0 0\r## 106 0 0 0\r## 107 0 0 0\r## 108 0 0 0\r## 109 0 0 0\r## 110 0 0 0\r## 111 0 0 0\r## 112 0 0 0\r## 113 0 0 0\r## 114 0 0 0\r## 115 0 0 0\r## 116 0 0 0\r## 117 0 0 0\r## 118 0 0 0\r## 119 0 0 0\r## 120 0 0 0\r## 121 0 0 0\r## 122 0 0 0\r## 123 0 0 0\r## 124 0 0 0\r## 125 0 0 0\r## 126 0 0 0\r## 127 0 0 0\r## 128 0 0 0\r## 129 0 0 0\r## 130 0 0 0\r## 131 0 0 0\r## 132 0 0 0\r## 133 0 0 0\r## 134 0 0 0\r## 135 0 0 0\r## 136 0 0 0\r## 137 0 0 0\r## 138 0 0 0\r## 139 0 0 0\r## 140 0 0 0\r## 141 0 0 0\r## 142 0 0 0\r## 143 0 0 0\r## 144 0 0 0\r## 145 0 0 0\r## 146 0 0 0\r## 147 0 0 0\r## 148 0 0 0\r## 149 0 0 0\r## 150 0 0 0\r## 151 0 0 0\r## 152 0 0 0\r## 153 0 0 0\r## 154 0 0 0\r## 155 0 0 0\r## 156 0 0 0\r## 157 0 0 0\r## 158 0 0 0\r## 159 0 0 0\r## 160 0 0 0\r## 161 0 0 0\r## 162 0 0 0\r## 163 0 0 0\r## 164 0 0 0\r## 165 0 0 0\r## 166 0 0 0\r## yhat_lower yhat_upper trend_lower trend_upper yhat\r## 1 17.44778 20.22142 18.67220 18.67220 18.84417\r## 2 17.43241 20.28847 18.63388 18.63388 18.82589\r## 3 17.28549 20.00058 18.51890 18.51890 18.63505\r## 4 17.26297 19.99705 18.48058 18.48058 18.58730\r## 5 17.22867 19.94185 18.44225 18.44225 18.66111\r## 6 17.17586 19.88467 18.40392 18.40392 18.57590\r## 7 17.13467 20.01034 18.36560 18.36560 18.55761\r## 8 16.97079 19.77026 18.25062 18.25062 18.36678\r## 9 16.94057 19.76627 18.21230 18.21230 18.31903\r## 10 16.92739 19.90654 18.17397 18.17397 18.39284\r## 11 16.88827 19.75330 18.13565 18.13565 18.30762\r## 12 16.83198 19.75430 18.09732 18.09732 18.28933\r## 13 16.74752 19.44475 17.98235 17.98235 18.09850\r## 14 16.65187 19.36566 17.94402 17.94402 18.05075\r## 15 16.72063 19.61430 17.90570 17.90570 18.12456\r## 16 16.67929 19.56262 17.86737 17.86737 18.03934\r## 17 16.62662 19.44383 17.82905 17.82905 18.02106\r## 18 16.45845 19.24106 17.71407 17.71407 17.83022\r## 19 16.34847 19.19172 17.67574 17.67574 17.78247\r## 20 16.50903 19.26053 17.63742 17.63742 17.85628\r## 21 16.32932 19.26126 17.59909 17.59909 17.77107\r## 22 16.34559 19.11223 17.56077 17.56077 17.75278\r## 23 16.07759 18.97360 17.44579 17.44579 17.56194\r## 24 16.10047 18.88417 17.40747 17.40747 17.51420\r## 25 16.17766 19.08119 17.36914 17.36914 17.58801\r## 26 16.14597 18.80305 17.33082 17.33082 17.50279\r## 27 16.03030 18.84447 17.29176 17.29176 17.48377\r## 28 15.95264 18.71911 17.17459 17.17459 17.29074\r## 29 15.71507 18.66804 17.13553 17.13553 17.24226\r## 30 15.87808 18.88037 17.09647 17.09647 17.31534\r## 31 15.79268 18.63847 17.05742 17.05742 17.22939\r## 32 15.75938 18.54281 17.00788 17.00788 17.19989\r## 33 15.46774 18.20355 16.85928 16.85928 16.97543\r## 34 15.58209 18.28693 16.80975 16.80975 16.91648\r## 35 15.59161 18.45315 16.76021 16.76021 16.97908\r## 36 15.40852 18.20813 16.70263 16.70263 16.87460\r## 37 15.48073 18.36596 16.64505 16.64505 16.83706\r## 38 15.10453 18.02908 16.47231 16.47231 16.58846\r## 39 15.20348 18.05467 16.41472 16.41472 16.52145\r## 40 15.14251 18.12385 16.35714 16.35714 16.57601\r## 41 15.18171 17.90445 16.29913 16.29913 16.47111\r## 42 15.02480 17.87908 16.24113 16.24113 16.43314\r## 43 14.74788 17.50482 16.06710 16.06710 16.18325\r## 44 14.67348 17.49058 16.00909 16.00909 16.11582\r## 45 14.83439 17.50482 15.95108 15.95108 16.16995\r## 46 14.71564 17.49835 15.89307 15.89307 16.06505\r## 47 14.62256 17.41079 15.83507 15.83507 16.02708\r## 48 14.37267 17.25332 15.66104 15.66104 15.77719\r## 49 14.21488 16.98432 15.60303 15.60303 15.70976\r## 50 14.23265 17.09242 15.54502 15.54502 15.76389\r## 51 14.19268 17.02797 15.48701 15.48701 15.65898\r## 52 14.12328 16.98674 15.42900 15.42900 15.62102\r## 53 13.84621 16.75491 15.25498 15.25498 15.37113\r## 54 13.93380 16.78323 15.19697 15.19697 15.30370\r## 55 13.77968 16.77591 15.13896 15.13896 15.35782\r## 56 13.91433 16.73188 15.08098 15.08098 15.25295\r## 57 13.80074 16.68831 15.02300 15.02300 15.21501\r## 58 13.62004 16.36845 14.84905 14.84905 14.96520\r## 59 13.55952 16.25524 14.79107 14.79107 14.89780\r## 60 13.53938 16.40495 14.73308 14.73308 14.95195\r## 61 13.36967 16.30452 14.70644 14.70644 14.87841\r## 62 13.50567 16.23266 14.67979 14.67979 14.87180\r## 63 13.27999 16.09325 14.59985 14.59985 14.71600\r## 64 13.33199 16.15539 14.57320 14.57320 14.67993\r## 65 13.30382 16.20999 14.54656 14.54656 14.76542\r## 66 13.35069 16.12102 14.55587 14.55587 14.72784\r## 67 13.28872 16.21607 14.56517 14.56517 14.75719\r## 68 13.35429 16.24595 14.59310 14.59310 14.70925\r## 69 13.27950 16.03611 14.60241 14.60241 14.70914\r## 70 13.51020 16.33761 14.61172 14.61172 14.83058\r## 71 13.37259 16.35241 14.64302 14.64302 14.81499\r## 72 13.47708 16.33532 14.79949 14.79949 14.90622\r## 73 13.64369 16.44462 14.83079 14.83079 15.04965\r## 74 13.57634 16.33475 14.86208 14.86208 15.03405\r## 75 13.52913 16.38701 14.89338 14.89338 15.08539\r## 76 13.70679 16.60941 15.00470 15.00470 15.12086\r## 77 13.87465 16.58755 15.04181 15.04181 15.14854\r## 78 13.89576 16.80329 15.07892 15.07892 15.29779\r## 79 13.89814 16.72030 15.11603 15.11603 15.28800\r## 80 13.95269 16.63747 15.15314 15.15314 15.34515\r## 81 13.99310 16.73444 15.26628 15.26628 15.38243\r## 82 14.02559 16.74393 15.30399 15.30399 15.41072\r## 83 14.22720 16.97987 15.34170 15.34170 15.56057\r## 84 14.10355 16.95703 15.37942 15.37942 15.55139\r## 85 14.24563 17.04522 15.41713 15.41713 15.60914\r## 86 14.26290 16.98616 15.53329 15.53329 15.64944\r## 87 14.32209 17.16729 15.57200 15.57200 15.67873\r## 88 14.55135 17.25366 15.61072 15.61072 15.82959\r## 89 14.43457 17.20230 15.64944 15.64944 15.82141\r## 90 14.46165 17.28146 15.80432 15.80432 15.92047\r## 91 14.50991 17.37962 15.84304 15.84304 15.94977\r## 92 14.64713 17.50104 15.88176 15.88176 16.10062\r## 93 14.66202 17.44816 15.92048 15.92048 16.09245\r## 94 14.72705 17.52702 15.95920 15.95920 16.15121\r## 95 14.69081 17.57231 16.07536 16.07536 16.19151\r## 96 14.87747 17.74015 16.11408 16.11408 16.22081\r## 97 15.01973 17.67419 16.15280 16.15280 16.37166\r## 98 15.03259 17.73246 16.19152 16.19152 16.36349\r## 99 14.91905 17.82160 16.23024 16.23024 16.42225\r## 100 15.08854 17.88573 16.38512 16.38512 16.49185\r## 101 15.25723 17.99700 16.42384 16.42384 16.64270\r## 102 15.18762 17.97871 16.46256 16.46256 16.63453\r## 103 15.27493 18.16499 16.50128 16.50128 16.69329\r## 104 15.40605 18.06707 16.61744 16.61744 16.73359\r## 105 15.40776 18.21721 16.65616 16.65616 16.76289\r## 106 15.49340 18.26576 16.69488 16.69488 16.91374\r## 107 15.45116 18.30469 16.73360 16.73360 16.90557\r## 108 15.55970 18.27788 16.77232 16.77232 16.96433\r## 109 15.55013 18.46831 16.88848 16.88848 17.00463\r## 110 15.56869 18.37102 16.92720 16.92720 17.03393\r## 111 15.75998 18.63572 16.96592 16.96592 17.18478\r## 112 15.75387 18.61332 17.00464 17.00464 17.17661\r## 113 15.92186 18.66839 17.04335 17.04335 17.23537\r## 114 15.86662 18.68059 17.15951 17.15951 17.27567\r## 115 15.93082 18.60874 17.19823 17.19823 17.30496\r## 116 16.07517 18.85065 17.23695 17.23695 17.45582\r## 117 16.12646 18.96967 17.27567 17.27567 17.44765\r## 118 16.11043 19.06855 17.31439 17.31439 17.50641\r## 119 16.11602 18.95763 17.43055 17.43055 17.54671\r## 120 16.23126 19.03163 17.51737 17.51737 17.62410\r## 121 16.36662 19.17643 17.60418 17.60418 17.82305\r## 122 16.43104 19.14733 17.69100 17.69100 17.86297\r## 123 16.49898 19.49096 17.77781 17.77781 17.96982\r## 124 16.80538 19.51999 18.03825 18.03825 18.15440\r## 125 16.94528 19.71755 18.21636 18.21636 18.32309\r## 126 17.18071 20.04743 18.39447 18.39447 18.61333\r## 127 17.24111 20.22863 18.57258 18.57258 18.74455\r## 128 17.49884 20.44018 18.75069 18.75069 18.94270\r## 129 17.96514 20.76104 19.28501 19.28501 19.40116\r## 130 18.15894 20.95952 19.46312 19.46312 19.56985\r## 131 18.47844 21.27093 19.64123 19.64123 19.86009\r## 132 18.46319 21.31218 19.81934 19.81934 19.99131\r## 133 18.79689 21.56733 19.99745 19.99745 20.18946\r## 134 19.41992 21.98921 20.53177 20.53177 20.64793\r## 135 19.38809 22.26116 20.70988 20.70988 20.81661\r## 136 19.72331 22.50962 20.88799 20.88799 21.10685\r## 137 19.77703 22.63044 21.06610 21.06610 21.23807\r## 138 20.00423 22.81751 21.24421 21.24421 21.43622\r## 139 20.32251 23.30499 21.77853 21.77853 21.89469\r## 140 20.62993 23.48383 21.95664 21.95664 22.06337\r## 141 20.93529 23.80905 22.13475 22.13475 22.35362\r## 142 21.15972 23.85299 22.31286 22.31286 22.48483\r## 143 21.36129 24.13376 22.49097 22.49097 22.68298\r## 144 21.81611 24.58861 23.02529 23.02529 23.14145\r## 145 21.87296 24.75975 23.20340 23.20340 23.31013\r## 146 22.24532 24.88232 23.38151 23.38151 23.60038\r## 147 22.40830 25.09896 23.55962 23.55962 23.73159\r## 148 22.49427 25.36519 23.73773 23.73773 23.92974\r## 149 22.97439 25.86402 24.27206 24.27206 24.38821\r## 150 23.16056 26.05026 24.45016 24.45016 24.55689\r## 151 23.42662 26.26819 24.62827 24.62827 24.84714\r## 152 23.59497 26.32284 24.80638 24.80638 24.97835\r## 153 23.75788 26.62782 24.98449 24.98449 25.17650\r## 154 24.16046 27.10557 25.51882 25.51882 25.63497\r## 155 24.40026 27.24595 25.69692 25.69692 25.80365\r## 156 24.69517 27.49652 25.87503 25.87503 26.09390\r## 157 24.73743 27.69032 26.05314 26.05314 26.22511\r## 158 25.05069 27.80720 26.23125 26.23184 26.42326\r## 159 24.55005 27.45016 26.40602 26.41376 26.00649\r## 160 24.75928 27.54985 26.57850 26.59822 26.18460\r## 161 25.53491 28.32366 26.74999 26.78625 26.88173\r## 162 25.61810 28.44542 26.91817 26.97712 27.05041\r## 163 25.83413 28.69525 27.08515 27.16716 27.34066\r## 164 26.15025 28.87360 27.25361 27.36012 27.47187\r## 165 26.15238 29.16152 27.42123 27.55540 27.67002\r## 166 25.93321 28.65315 27.58171 27.74742 27.25326\rplot(model,forecast)\rprophet_plot_components(model,forecast)\rImplementing forcasting using TimeSeries object in R\rlibrary(tseries)\rlibrary(zoo)\rlibrary(forecast)\rlibrary(normwhn.test)\rdat = structure(\rc(1100, 1150, 1200, 1150, 1150, 1200, 1300, 1100, 1200,1250, 1200, 1250, 1300,1400,1550,1400,1450,1450,1450,1450,1500,1550,1600,1650,\r1650,1650,1650,1650,1750,1850,1750,1800,1850,1950,1950,2000,\r1950,2000,2050,2100,2150,2200,2250,2450,2650,2700,2850,2850,\r2750,2750,2750,2750,2800,2800,2850,2850,3000,2950,3000,2950,\r2900,2850,2750,2750,2450,2400,2450,2500,2750,2550,2600,2300,\r2470,2570,2570,2550,2500,2470,2550,2550,2470,2470,2370,2400,\r2870,2648,2656,2600,2656,2608,2460,2644,2564,2508,2524,2508,\r3006,3027,3129,3142,3119,2920,2902,2875,2880,2726,2543,2836,\r2887,2978,2901,2946,2890,2947,2811,2899,3017,2989,2967,2902,\r2994,3046,3074,3100,3141,3002,2934,2901,2908,3003,3097,3083,\r3205,3253,3221,3192,3170,3240,3345,3480,3750,3725,3800,4000,\r3800,4000,4200,4500,4800,5000,5100,5500,5500,5800,6000,6200), .Tsp = c(2008, 2020.91666666667, 12), class = \u0026quot;ts\u0026quot;)\rarimaModel \u0026lt;- arima(dat, order=c(0, 1, 1), list(order=c(0, 1, 0), period = 12))\rsummary(arimaModel)\r## ## Call:\r## arima(x = dat, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 0), period = 12))\r## ## Coefficients:\r## ma1\r## -0.1067\r## s.e. 0.0720\r## ## sigma^2 estimated as 22135: log likelihood = -918.26, aic = 1840.53\r## ## Training set error measures:\r## ME RMSE MAE MPE MAPE MASE ACF1\r## Training set 14.32495 142.4436 105.2636 0.3245391 3.827641 1.189896 -0.02882344\rpreds \u0026lt;- predict(arimaModel, n.ahead = 12)\rprint(preds)\r## $pred\r## Jan Feb Mar Apr May Jun Jul Aug\r## 2021 5998.214 6198.214 6398.214 6698.214 6998.214 7198.214 7298.214 7698.214\r## Sep Oct Nov Dec\r## 2021 7698.214 7998.214 8198.214 8398.214\r## ## $se\r## Jan Feb Mar Apr May Jun Jul Aug\r## 2021 148.7772 199.4956 239.7131 274.0916 304.6146 332.3461 357.9354 381.8136\r## Sep Oct Nov Dec\r## 2021 404.2839 425.5694 445.8398 465.2279\rplot(dat, col = \u0026#39;green\u0026#39;, main = \u0026#39;Acutal vs ARIMA Model\u0026#39;,\rylab = \u0026#39;Gold Rates\u0026#39;, lwd = 3)\rlines(arimaModel$fitted, col = \u0026#39;black\u0026#39;, lwd = 3, lty = 2)\rlegend(\u0026#39;topleft\u0026#39;, legend = c(\u0026#39;Actual\u0026#39;, \u0026#39;ARIMA Fit\u0026#39;),\rcol = c(\u0026#39;green\u0026#39;,\u0026#39;black\u0026#39;), lwd = c(3,3), lty = c(1,2))\rts.plot(dat, preds$pred, main = \u0026#39;Actual vs ARIMA Predictions\u0026#39;,\rcol = c(\u0026#39;green\u0026#39;,\u0026#39;black\u0026#39;), lty = c(1,2), lwd = c(3,3))\rlegend(\u0026#39;topleft\u0026#39;, legend = c(\u0026#39;Actual Data\u0026#39;, \u0026#39;ARIMA Predictions\u0026#39;),\rcol = c(\u0026#39;green\u0026#39;,\u0026#39;black\u0026#39;), lwd = c(3,3), lty = c(1,2))\r\rSummary\rThe main aim of time-series modeling is to carefully collect and regorously study the past observations of the time-series to develop an appropriate model which describes the inherent structure of the series. The model is then used to generate the future values for the series.\rTime series forecasting is thus can be termed as act of predicting the future by understanding the past.\n\r\r\r",
    "ref": "/blog/time-series/"
  },{
    "title": "Text analytics",
    "date": "",
    "description": "The analysis of text data gives useful insigths. This post uses news group data set to investigate text data",
    "body": "\r\rProcessing large amounts text data is an important area in natural language processing. The analysis of text data with machine learning tools can give us important insights. Given a text data such as a book, posts or tweets, one may ask questions such as list of common words.\nIn this post we are going to analyse 20 news groups dataset. The Newsgroups dataset comprises around 18000 newsgroups posts on 20 topics. The dataset can by obtained by using fetch_20newsgroups in sklearn.datasets as fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\n1: First step is to get the dataset and look into it to get understanding about how it is organized…\nfrom sklearn.datasets import fetch_20newsgroups\rnewsgroups_full = fetch_20newsgroups(subset=\u0026#39;all\u0026#39;, remove=(\u0026#39;headers\u0026#39;, \u0026#39;footers\u0026#39;, \u0026#39;quotes\u0026#39;), shuffle=True, random_state=42)\rprint(newsgroups_full.keys())\r## dict_keys([\u0026#39;data\u0026#39;, \u0026#39;filenames\u0026#39;, \u0026#39;target_names\u0026#39;, \u0026#39;target\u0026#39;, \u0026#39;DESCR\u0026#39;])\rThe newsgroups_full dataset has properties and function such as keys() which important keys for fetching the details of different types.\rFor example target_names specifies various names of the newsgroups, target is 20 different unique index corresponding to target_names\rthe key data is used to get actual data stored in different files having some filenames. Lets see how go use different keys\n# The target names are the names of the news groups\rprint(newsgroups_full.target_names)\r## [\u0026#39;alt.atheism\u0026#39;, \u0026#39;comp.graphics\u0026#39;, \u0026#39;comp.os.ms-windows.misc\u0026#39;, \u0026#39;comp.sys.ibm.pc.hardware\u0026#39;, \u0026#39;comp.sys.mac.hardware\u0026#39;, \u0026#39;comp.windows.x\u0026#39;, \u0026#39;misc.forsale\u0026#39;, \u0026#39;rec.autos\u0026#39;, \u0026#39;rec.motorcycles\u0026#39;, \u0026#39;rec.sport.baseball\u0026#39;, \u0026#39;rec.sport.hockey\u0026#39;, \u0026#39;sci.crypt\u0026#39;, \u0026#39;sci.electronics\u0026#39;, \u0026#39;sci.med\u0026#39;, \u0026#39;sci.space\u0026#39;, \u0026#39;soc.religion.christian\u0026#39;, \u0026#39;talk.politics.guns\u0026#39;, \u0026#39;talk.politics.mideast\u0026#39;, \u0026#39;talk.politics.misc\u0026#39;, \u0026#39;talk.religion.misc\u0026#39;]\r# The data is actual data stred as list\rprint(newsgroups_full.target_names[newsgroups_full.target[1]])\r## comp.sys.ibm.pc.hardware\rprint(newsgroups_full.data[1])\r## My brother is in the market for a high-performance video card that supports\r## VESA local bus with 1-2MB RAM. Does anyone have suggestions/ideas on:\r## ## - Diamond Stealth Pro Local Bus\r## ## - Orchid Farenheit 1280\r## ## - ATI Graphics Ultra Pro\r## ## - Any other high-performance VLB card\r## ## ## Please post or email. Thank you!\r## ## - Matt\rAs we can se the above two statements give us the data about post belonging to comp.sys.ibm.pc.hardware which contains:\n\r# Putting the words in the dictionary\rnewsgroups_full_dnry = dict()\rfor ind in range(len(newsgroups_full.data)):\rgrp_name = newsgroups_full.target_names[newsgroups_full.target[ind]]\rif grp_name in newsgroups_full_dnry:\rnewsgroups_full_dnry[grp_name] += 1\relse:\rnewsgroups_full_dnry[grp_name] = 1\rprint(\u0026quot;Total number of articles in dataset \u0026quot; + str(len(newsgroups_full.data))) \r## Total number of articles in dataset 18846\rprint(\u0026quot;Number of articles category wise: \u0026quot;)\r## Number of articles category wise:\rprint(newsgroups_full_dnry)\r## {\u0026#39;rec.sport.hockey\u0026#39;: 999, \u0026#39;comp.sys.ibm.pc.hardware\u0026#39;: 982, \u0026#39;talk.politics.mideast\u0026#39;: 940, \u0026#39;comp.sys.mac.hardware\u0026#39;: 963, \u0026#39;sci.electronics\u0026#39;: 984, \u0026#39;talk.religion.misc\u0026#39;: 628, \u0026#39;sci.crypt\u0026#39;: 991, \u0026#39;sci.med\u0026#39;: 990, \u0026#39;alt.atheism\u0026#39;: 799, \u0026#39;rec.motorcycles\u0026#39;: 996, \u0026#39;rec.autos\u0026#39;: 990, \u0026#39;comp.windows.x\u0026#39;: 988, \u0026#39;comp.graphics\u0026#39;: 973, \u0026#39;sci.space\u0026#39;: 987, \u0026#39;talk.politics.guns\u0026#39;: 910, \u0026#39;misc.forsale\u0026#39;: 975, \u0026#39;rec.sport.baseball\u0026#39;: 994, \u0026#39;talk.politics.misc\u0026#39;: 775, \u0026#39;comp.os.ms-windows.misc\u0026#39;: 985, \u0026#39;soc.religion.christian\u0026#39;: 997}\rPie chart of distribution of the articles\nimport matplotlib.pyplot as plt\rlabels = newsgroups_full.target_names\rslices = []\rfor key in newsgroups_full_dnry:\rslices.append(newsgroups_full_dnry[key])\rfig , ax = plt.subplots()\rax.pie(slices, labels = labels , autopct = \u0026#39;%1.1f%%\u0026#39;, shadow = True, startangle = 90)\rax.axis(\u0026quot;equal\u0026quot;)\rax.set_title(\u0026quot;News groups messages distribution\u0026quot;)\rThe distribution of messages posted in different newsgroups is almost similar. The sports groups have most number of messages\nViewing the data as tabular form. We can put the data in the dataframe and see the top ten records\nimport pandas as pd\rdata_labels_map = dict(enumerate(newsgroups_full.target_names))\rmessage, target_labels, target_names = (newsgroups_full.data, newsgroups_full.target, [data_labels_map[label] for label in newsgroups_full.target])\rnewsgroups_full_df = pd.DataFrame({\u0026#39;text\u0026#39;: message, \u0026#39;source\u0026#39;: target_labels, \u0026#39;source_name\u0026#39;: target_names})\rprint(newsgroups_full_df.shape)\r## (18846, 3)\rnewsgroups_full_df.head(10)\r## text ... source_name\r## 0 \\n\\nI am sure some bashers of Pens fans are pr... ... rec.sport.hockey\r## 1 My brother is in the market for a high-perform... ... comp.sys.ibm.pc.hardware\r## 2 \\n\\n\\n\\n\\tFinally you said what you dream abou... ... talk.politics.mideast\r## 3 \\nThink!\\n\\nIt\u0026#39;s the SCSI card doing the DMA t... ... comp.sys.ibm.pc.hardware\r## 4 1) I have an old Jasmine drive which I cann... ... comp.sys.mac.hardware\r## 5 \\n\\nBack in high school I worked as a lab assi... ... sci.electronics\r## 6 \\n\\nAE is in Dallas...try 214/241-6060 or 214/... ... comp.sys.mac.hardware\r## 7 \\n[stuff deleted]\\n\\nOk, here\u0026#39;s the solution t... ... rec.sport.hockey\r## 8 \\n\\n\\nYeah, it\u0026#39;s the second one. And I believ... ... rec.sport.hockey\r## 9 \\nIf a Christian means someone who believes in... ... talk.religion.misc\r## ## [10 rows x 3 columns]\r2: Next step is cleaning the text…\nTo clean the large amounts of text we use nltk tools such as WordNetLemmatizer, PorterStemmer, stopwords, names.\rLets import them first\nimport nltk\rfrom nltk.corpus import names\rfrom nltk.stem import WordNetLemmatizer\rfrom nltk.stem import PorterStemmer\rfrom nltk.corpus import stopwords\rfrom nltk.tokenize import word_tokenize\rimport re\rstopWords = set(stopwords.words(\u0026#39;english\u0026#39;))\rvalidwords = set(nltk.corpus.words.words())\rre is regular expression library in python. We need to first define few functions such as text_tokenizer. The main aim is to clean the posts first by removing the alpha-numeric, numeric and non-alphabatic characters then by applying stemming and lemmmatizing techiniques so that we are left with only the words which are meaningful for the analysis. Lets write the functions for the same\nporter_stemmer = PorterStemmer()\rlemmatizer = WordNetLemmatizer()\rdef text_tokenizer(str_input):\rwords = re.sub(r\u0026quot;[^A-Za-z\\-]\u0026quot;, \u0026quot; \u0026quot;, str_input).lower().split()\rwords = [porter_stemmer.stem(word) for word in words if len(word) \u0026gt; 2 ]\rwords = [lemmatizer.lemmatize(word) for word in words if len(word) \u0026gt; 2 and word in validwords and word not in stopWords]\rreturn \u0026#39; \u0026#39;.join(words)\r2.1: Next is to apply text_tokenizer function to get a new column having clean text…\nnewsgroups_full_df[\u0026#39;clean_text\u0026#39;] = newsgroups_full_df.text.apply(lambda x: text_tokenizer(x))\rnewsgroups_full_df.sort_values(by=[\u0026#39;source\u0026#39;],inplace=True)\rnewsgroups_full_df.head(5)\r## text ... clean_text\r## 8501 \\nI could give much the same testimonial about... ... could give much scout back gay thank well put ...\r## 14285 \\nFine... THE ILLIAD IS THE WORD OF GOD(tm) (... ... fine word god matter prove wrong west\r## 17533 Hello Gang,\\n\\nThere have been some notes rece... ... hello gang note recent ask obtain fish questio...\r## 1527 \\n Sorry, gotta disagree with you on this one... ... one bill prefer half bake bob vice said queen ...\r## 14271 The latest news seems to be that Koresh will g... ... latest news seem give finish write sequel\r## ## [5 rows x 4 columns]\r2.3:Creating a dictionary of newsgroup cleaned text\nwordlst = list()\rnewsgroup_dic = dict()\rlabel = \u0026#39;\u0026#39;\rfor i in range(0,20):\rnewsgroups_full_df_1 = newsgroups_full_df.loc[newsgroups_full_df[\u0026#39;source\u0026#39;] == i]\rfor row in newsgroups_full_df_1[[\u0026#39;source_name\u0026#39;, \u0026#39;clean_text\u0026#39;]].iterrows():\rr = row[1]\rlabel = r.source_name\rwordlst.append(\u0026#39;\u0026#39;.join(map(str,r.clean_text)))\rwordstr = \u0026#39; \u0026#39;.join(map(str, wordlst))\rnewsgroup_dic[label] = wordstr\rlabel = \u0026#39;\u0026#39;\rwordstr = \u0026#39;\u0026#39;\rwordlst.clear() \rNext steps will create the features out of the dictionary of the newsgroups words just created in the previous steps. In natural language processing feature extraction is an important step. In this case the words themselves becomes the features. To extract the features python provides an important library called CountVectorizer. We need to transform our cleaned_text using sklearn.feature_extraction.text and CountVectorizer library. Lets apply it to our newsgroup data.\n3: Feature extraction…\nThe feature vector can be created with sklearn CountVectorizer. When creating the feature vectors we can decide the number of features, as well as set limits for the minimum and maximum number of documents a word can appear.\nNote that the transformed data is stored in a sparse matrix (which is much more efficient for large data sets).\n# First lets import it\rfrom sklearn.feature_extraction.text import CountVectorizer\rcount_vectorizer = CountVectorizer(stop_words = \u0026#39;english\u0026#39;)\rThe function get_word_freq_dict_sorted returns a sorted dictionary of words counts. It taks a dataframe as its argument.\ndef get_word_freq_dict_sorted(ng_X_df):\rwordfreq = ng_X_df.sum(axis=0)\rfeatures = ng_X_df.columns.tolist()\rcounts = wordfreq.tolist()\rwordfreq_df = pd.DataFrame()\rwordfreq_df[\u0026#39;word\u0026#39;] = features\rwordfreq_df[\u0026#39;count\u0026#39;] = counts\rwordfreq_dict = dict(wordfreq_df.values.tolist())\rwordfreqdict_sorted = dict(sorted(wordfreq_dict.items(), key=lambda x: x[1],reverse=True))\rreturn wordfreqdict_sorted\rNow iterate over the newsgroup dictionary obtained from the newsgroups dataframe and create another dictionary where keys are the newsgroups and values are another dictionary of word counts in that newsgroup.\nng_dict_of_words = dict()\rfor key in newsgroup_dic:\rng_X = count_vectorizer.fit_transform([newsgroup_dic[key]])\rng_X_df = pd.DataFrame(ng_X.toarray(), columns=count_vectorizer.get_feature_names())\rng_dict_of_words[key] = get_word_freq_dict_sorted(ng_X_df)\r\r4: Exploring words in the news groups..\nQUESTION: What are the top words in newsgroup comp.sys.ibm.pc.hardware by their count ?\nANSWER: Iterating over the dictionary corresponding to comp.sys.ibm.pc.hardware we get the top ten words as {space orbit launch use like time mission year earth moon}. Like wise we get the most common words in each newsgroup by their count.\nword_dic = ng_dict_of_words[\u0026#39;comp.sys.ibm.pc.hardware\u0026#39;] word_df = pd.DataFrame.from_dict(word_dic, orient=\u0026#39;index\u0026#39;)\rprint(word_df.T.iloc[0:1,0:10])\r## drive use card ani control disk work problem know ide\r## 0 990 792 537 476 441 384 369 356 333 309\rVarious other approaches to explore words in news groups include graphical methods, which help us visualize the distribution of words across news groups. We can use matplotlib.pyplot to draw differnt graphs.\nNext we will explore various algorithms for text classification.\n5 Text Classification…\nText classification is done using various machine learning algorithms. The most popular ones are\n\rMultinomialNB\rLogisticRegression\rSVC\r\rThe goal of the text classification is to predict which newsgroup a post belongs to based on the post text.\nBOW and TF-IDF are two different techniques for text classification\nBag of Words (BoW) is an algorithm that counts frequency of a word in newsgroups. Those word counts allow us to compare different newsgroups and gauge their similarities for applications like search, topic modeling etc.\nIn TF-IDF, words are given weight. TF-IDF measures relevance, not frequency. That is, wordcounts are replaced with TF-IDF scores across the whole dataset.\nTo use text classification algorithm we need to randomly separates data into training and testing dataset and fit the classifier with selected training data. A classifer defines model for text classification. The score gives us the accuracy for testing data.\nDifferent classifiers can give us different results for accuracy. Accuracy depends on the specific problem, number of categories and differences between them, etc.\n6 Evaluation…\nEvaluation of the model can be done using the confusion matrix which can be ploted using the heatmap plot. A basic heatmap is shown below\nnewgroupsheatmap.png\n\rThe confusion matrix depicts the wrongly classified records. For example 4 articles from comp.graphics are wrongly classified as comp.windows.x.\n***7 Slide show\nknitr::include_url(\u0026#39;/slides/NewsGroupsAnalysis.html\u0026#39;)\r\rSummary: Text classifcation has usefull applications in detection of spam pages, personal email sorting, tagging products or document filtering, automatic classification of the text based on its contents, sentiment analysis etc. There are different methods and models availble in sklearn and nltp libraries in python which can be utilized for text classification and natural language processing applications.\n",
    "ref": "/blog/text-analytics/"
  },{
    "title": "EDA with Iris dataset",
    "date": "",
    "description": "Exploring datasets is an important topic in data science. To achieve this task EDA i.e Exploratory data analysis helps",
    "body": "\r\rLoading the dataset\rExploring datasets is an important topic in data science. To achieve this task EDA i.e Exploratory data analysis helps by means of summary statistics and other infographics. In this post we will take iris dataset and apply EDA techiniqes to better gain an insight into the dataset.\nfrom sklearn.datasets import load_iris\riris_dataset = load_iris()\rprint(type(iris_dataset))\r## \u0026lt;class \u0026#39;sklearn.utils.Bunch\u0026#39;\u0026gt;\rprint(iris_dataset.keys())\r## dict_keys([\u0026#39;data\u0026#39;, \u0026#39;target\u0026#39;, \u0026#39;target_names\u0026#39;, \u0026#39;DESCR\u0026#39;, \u0026#39;feature_names\u0026#39;, \u0026#39;filename\u0026#39;])\rThe dataset is of Bunch datatypes having keys 'data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'.\nBefore we go ahead we need to convert Bunch to pandas DataFrame.\n\rcreating dataframe from raw data\r\rimport pandas as pd\riris_df = pd.DataFrame(iris_dataset.data, columns = iris_dataset.feature_names)\rprint(iris_df.head())\r## sepal length (cm) sepal width (cm) petal length (cm) petal width (cm)\r## 0 5.1 3.5 1.4 0.2\r## 1 4.9 3.0 1.4 0.2\r## 2 4.7 3.2 1.3 0.2\r## 3 4.6 3.1 1.5 0.2\r## 4 5.0 3.6 1.4 0.2\rThe columns shows the length, width but does not show the group to which this length or width belongs. The group to which these values belong is stored in target_names which can stored in seperate column in our dataframe as.\n\rgroup_names = pd.Series([iris_dataset.target_names[ind] for ind in iris_dataset.target], dtype = \u0026#39;category\u0026#39;)\riris_df[\u0026#39;group\u0026#39;] = group_names\rprint(iris_df.head())\r## sepal length (cm) sepal width (cm) ... petal width (cm) group\r## 0 5.1 3.5 ... 0.2 setosa\r## 1 4.9 3.0 ... 0.2 setosa\r## 2 4.7 3.2 ... 0.2 setosa\r## 3 4.6 3.1 ... 0.2 setosa\r## 4 5.0 3.6 ... 0.2 setosa\r## ## [5 rows x 5 columns]\r\rGetting summary statistical measures\rTo start with EDA, mean and median should be calculated for the numeric variables. To get the summary statistics we use describe function.\niris_sumry = iris_df.describe().transpose()\riris_sumry[\u0026#39;std\u0026#39;] = iris_df.std()\riris_sumry.head()\r## count mean std min 25% 50% 75% max\r## sepal length (cm) 150.0 5.843333 0.828066 4.3 5.1 5.80 6.4 7.9\r## sepal width (cm) 150.0 3.057333 0.435866 2.0 2.8 3.00 3.3 4.4\r## petal length (cm) 150.0 3.758000 1.765298 1.0 1.6 4.35 5.1 6.9\r## petal width (cm) 150.0 1.199333 0.762238 0.1 0.3 1.30 1.8 2.5\rThere is no feature which is having a mean as zero. The average sepal width and petal length are not much different. The median of petal length is much different from the mean.\nFor sepal length 25% quartile is 5.1 i.e 25% of the dataset values for this feature are less than 5.1\n\rChecking skewness and kurtosis\rTo check whether the petal length is normaly distributed or not we find the skewness and kurtosis and perform the test as…\nfrom scipy.stats import skew, skewtest\rpetal_length = iris_df[\u0026#39;petal length (cm)\u0026#39;]\rsk = skew(petal_length)\rz_score, p_value = skewtest(petal_length)\rprint(\u0026#39;Skewness %0.3f z-score %0.3f p-value %0.3f\u0026#39; % (sk, z_score, p_value))\r## Skewness -0.272 z-score -1.400 p-value 0.162\r\rfrom scipy.stats import kurtosis, kurtosistest\rpetal_length = iris_df[\u0026#39;petal length (cm)\u0026#39;]\rku = kurtosis(petal_length)\rz_score, p_value = kurtosistest(petal_length)\rprint(\u0026#39;Kurtosis %0.3f z-score %0.3f p-value %0.3f\u0026#39; % (ku, z_score, p_value))\r## Kurtosis -1.396 z-score -14.823 p-value 0.000\rFrom the values of skewness and kurtosis we see that the petal length of plants is skewed to the left.\n\rCreating categorical dataframe\rTo create a categorical dataframe from the quantitative data we can make use of the binning as. Binning transforms the numeric data into categorical data. In EDA this helps in reducing outliers.\n\rperct = [0,.25,.5,.75,1]\riris_bin = pd.concat(\r[pd.qcut(iris_df.iloc[:,0], perct, precision=1),\rpd.qcut(iris_df.iloc[:,1], perct, precision=1),\rpd.qcut(iris_df.iloc[:,2], perct, precision=1),\rpd.qcut(iris_df.iloc[:,3], perct, precision=1)],\rjoin=\u0026#39;outer\u0026#39;, axis = 1)\r\rFrequencies and Contingency tables\rThe resulting frequencies of each class of species in iris dataset can be obtained as …\n\rprint(iris_df[\u0026#39;group\u0026#39;].value_counts())\r## virginica 50\r## versicolor 50\r## setosa 50\r## Name: group, dtype: int64\rThe resultant frequencies for the binned dataframe\n\rprint(iris_bin[\u0026#39;petal length (cm)\u0026#39;].value_counts())\r## (0.9, 1.6] 44\r## (4.4, 5.1] 41\r## (5.1, 6.9] 34\r## (1.6, 4.4] 31\r## Name: petal length (cm), dtype: int64\rWe can describe the binned dataframe using the describe function\niris_bin.describe().transpose()\r## count unique top freq\r## sepal length (cm) 150 4 (4.2, 5.1] 41\r## sepal width (cm) 150 4 (1.9, 2.8] 47\r## petal length (cm) 150 4 (0.9, 1.6] 44\r## petal width (cm) 150 4 (0.0, 0.3] 41\rContingency tables based on groups and binning can be obtained as…\nprint(pd.crosstab(iris_df[\u0026#39;group\u0026#39;], iris_bin[\u0026#39;petal length (cm)\u0026#39;]))\r## petal length (cm) (0.9, 1.6] (1.6, 4.4] (4.4, 5.1] (5.1, 6.9]\r## group ## setosa 44 6 0 0\r## versicolor 0 25 25 0\r## virginica 0 0 16 34\rCross tabulation can further used to apply chi-square test to determine which feature has the effect on the species of the plant. Further chi-square test can help us understand the relationship between target outcome (plant group) and other independant variables (length and width). For example one can setup a chi-squre test to check if the petal length is statistically different from each other i.e values are significantly different across class of species.\n\rApplying t-test to check statistical signifcance\rfrom scipy.stats import ttest_ind\rgrp0 = iris_df[\u0026#39;group\u0026#39;] == \u0026#39;setosa\u0026#39;\rgrp1 = iris_df[\u0026#39;group\u0026#39;] == \u0026#39;versicolor\u0026#39;\rgrp2 = iris_df[\u0026#39;group\u0026#39;] == \u0026#39;virginica\u0026#39;\rpetal_length = iris_df[\u0026#39;petal length (cm)\u0026#39;]\rprint(\u0026#39;var1 %0.3f var2 %03f\u0026#39; % (petal_length[grp1].var(),\rpetal_length[grp2].var()))\r## var1 0.221 var2 0.304588\rsepal_width = iris_df[\u0026#39;sepal width (cm)\u0026#39;]\rt, p_value = ttest_ind(sepal_width[grp1], sepal_width[grp2], axis=0, equal_var=False)\rprint(\u0026#39;t statistic %0.3f p-value %0.3f\u0026#39; % (t, p_value))\r## t statistic -3.206 p-value 0.002\rThe p-value shows that group means are significantly different.\nFurther we can check it among more than 2 groups using ANOVA\n\r\r\rfrom scipy.stats import f_oneway\rsepal_width = iris_df[\u0026#39;sepal width (cm)\u0026#39;]\rf, p_value = f_oneway(sepal_width[grp0],\rsepal_width[grp1],\rsepal_width[grp2])\rprint(\u0026#39;One-way ANOVA F-value %0.3f p-value %0.3f\u0026#39; % (f,p_value))\r## One-way ANOVA F-value 49.160 p-value 0.000\rApplying chi-square to cagegorical variables\r\rfrom scipy.stats import chi2_contingency\rtable = pd.crosstab(iris_df[\u0026#39;group\u0026#39;],\riris_bin[\u0026#39;petal length (cm)\u0026#39;])\rchi2, p, dof, expected = chi2_contingency(table.values)\rprint(\u0026#39;Chi-square %0.2f p-value %0.3f\u0026#39; % (chi2, p))\r## Chi-square 212.43 p-value 0.000\rThe p-value and chi-square value indicates that petal length variable can be effectively used for distinguishing between iris groups.\n\rVisualising data\rCreating box plot\rimport seaborn as sns\rimport matplotlib.pyplot as plt\rsns.boxplot(x=\u0026quot;group\u0026quot;,y=\u0026quot;petal length (cm)\u0026quot;,data=iris_df)\rplt.show()\rThe box plot shows that the 3 groups, setosa, versicolor, and virginica, have different petal lengths.\n\r\rConclusion\rIn this artical we hv seen how to apply to do exploratary data analysis with iris dataset. We also learned the tools that help us understand the relationship between outcome variable and independent variables. We learned various techniqes in EDA that can be used before building the machine learning models.\n\r\r",
    "ref": "/blog/eda-with-iris-dataset/"
  },{
    "title": "Understanding frequency tables",
    "date": "",
    "description": "Frequency tables are a great tool to help explore datasets and get an idea about relationships between variables",
    "body": "\r\rIntroduction\rTo discover the relationship between variables is the main task of data analysis. Tools like frequency tables helps to explore the data and get an idea of the relationships between variables. A frequency table is just a data table that shows the counts of one or more categorical variables.\nTo explore frequency tables, we will take the titanic dataset\n\rimport numpy as np\rimport pandas as pd\rimport os\rtit_train = pd.read_csv(\u0026quot;../data/titanic/train.csv\u0026quot;)\rcabin_as_text = tit_train[\u0026#39;Cabin\u0026#39;].astype(str)\rnew_cabin = np.array([cabin[0] for cabin in cabin_as_text])\rtit_train[\u0026quot;Cabin\u0026quot;] = pd.Categorical(new_cabin)\r\rOne-Way Tables\rIn pandas frequency tables are known as crosstabs. Using the pd.crosstab() function we can get the frequency tables. This function takes one or more array type objects as indexes or columns and then constructs a new dataframe of the variable counts based on the supplied arrays.\n\rcross_tab_survived = pd.crosstab(index=tit_train[\u0026quot;Survived\u0026quot;], columns=\u0026quot;count\u0026quot;) print(cross_tab_survived)\r## col_0 count\r## Survived ## 0 549\r## 1 342\rWe can make a more crosstabs to explore other variables\ncross_tab_plcass = pd.crosstab(index=tit_train[\u0026quot;Pclass\u0026quot;], columns=\u0026quot;count\u0026quot;) print(cross_tab_plcass)\r## col_0 count\r## Pclass ## 1 216\r## 2 184\r## 3 491\rcross_tab_sex = pd.crosstab(index=tit_train[\u0026quot;Sex\u0026quot;], columns=\u0026quot;count\u0026quot;) print(cross_tab_sex)\r## col_0 count\r## Sex ## female 314\r## male 577\rcross_tab_cab = pd.crosstab(index=tit_train[\u0026quot;Cabin\u0026quot;], columns=\u0026quot;count\u0026quot;) print(cross_tab_cab)\r## col_0 count\r## Cabin ## A 15\r## B 47\r## C 59\r## D 33\r## E 32\r## F 13\r## G 4\r## T 1\r## n 687\rThe one-way tables give us useful insights. We can see the distribution of records across the categories. For example we find that males are more than females by a significant margin and there were more third class travelers than first and second class combined.\nSince the crosstab function produces DataFrames, the DataFrame operations we’ve learned work on crosstabs:\nprint (cross_tab_cab.sum(), \u0026quot;\\n\u0026quot;) # Sum the counts\r## col_0\r## count 891\r## dtype: int64\rprint (cross_tab_cab.shape, \u0026quot;\\n\u0026quot;) # Check number of rows and cols\r## (9, 1)\rOne of the most useful feature of frequency tables is that they allow to extract the proportion of the data that belongs to each category. With a one-way table, we can do this by dividing each table value by the total number of records in the table.\ncross_tab_cab / cross_tab_cab.sum()\r## col_0 count\r## Cabin ## A 0.016835\r## B 0.052750\r## C 0.066218\r## D 0.037037\r## E 0.035915\r## F 0.014590\r## G 0.004489\r## T 0.001122\r## n 0.771044\rcross_tab_cab\r## col_0 count\r## Cabin ## A 15\r## B 47\r## C 59\r## D 33\r## E 32\r## F 13\r## G 4\r## T 1\r## n 687\r\r\rTwo-Way Tables\rTwo-way frequency tables, also called contingency tables, are tables of counts with two dimensions where each dimension is a different variable. Two-way tables can give us insight into the relationship between two variables. To create a two way table, pass two variables to the pd.crosstab() function instead of one.\n\rTable of survival vs. sex\r\rsurvive_sex = pd.crosstab(index=tit_train[\u0026quot;Survived\u0026quot;], columns=tit_train[\u0026quot;Sex\u0026quot;])\rsurvive_sex.index= [\u0026quot;died\u0026quot;,\u0026quot;survived\u0026quot;]\rsurvive_sex\r## Sex female male\r## died 81 468\r## survived 233 109\r\rTable of survival vs passenger class\r\rsurvive_class = pd.crosstab(index=tit_train[\u0026quot;Survived\u0026quot;], columns=tit_train[\u0026quot;Pclass\u0026quot;])\rsurvive_class.columns = [\u0026quot;class1\u0026quot;,\u0026quot;class2\u0026quot;,\u0026quot;class3\u0026quot;]\rsurvive_class.index= [\u0026quot;died\u0026quot;,\u0026quot;survived\u0026quot;]\rsurvive_class\r## class1 class2 class3\r## died 80 97 372\r## survived 136 87 119\r\rTable of survival vs passenger class\r\r\rsurvived_class = pd.crosstab(index=tit_train[\u0026quot;Survived\u0026quot;], columns=tit_train[\u0026quot;Pclass\u0026quot;],\rmargins=True) # Include row and column totals\rsurvived_class.columns = [\u0026quot;class1\u0026quot;,\u0026quot;class2\u0026quot;,\u0026quot;class3\u0026quot;,\u0026quot;rowtotal\u0026quot;]\rsurvived_class.index= [\u0026quot;died\u0026quot;,\u0026quot;survived\u0026quot;,\u0026quot;coltotal\u0026quot;]\rsurvived_class\r## class1 class2 class3 rowtotal\r## died 80 97 372 549\r## survived 136 87 119 342\r## coltotal 216 184 491 891\rTo get the proportion of counts along each column (in this case, the survival rate within each passenger class) divide by the column totals:\nsurvived_class/survived_class.loc[\u0026quot;coltotal\u0026quot;]\r## class1 class2 class3 rowtotal\r## died 0.37037 0.527174 0.757637 0.616162\r## survived 0.62963 0.472826 0.242363 0.383838\r## coltotal 1.00000 1.000000 1.000000 1.000000\rTo get the proportion of counts along each row divide by the row totals. Use the df.div() to the get division to on a column by column basis:\nsurvived_class.div(survived_class[\u0026quot;rowtotal\u0026quot;],\raxis=0)\r## class1 class2 class3 rowtotal\r## died 0.145719 0.176685 0.677596 1.0\r## survived 0.397661 0.254386 0.347953 1.0\r## coltotal 0.242424 0.206510 0.551066 1.0\rAlternatively, you can transpose the table with df.T to swap rows and columns and perform row by row division as normal:\nsurvived_class.T/survived_class[\u0026quot;rowtotal\u0026quot;]\r## died survived coltotal\r## class1 0.145719 0.397661 0.242424\r## class2 0.176685 0.254386 0.206510\r## class3 0.677596 0.347953 0.551066\r## rowtotal 1.000000 1.000000 1.000000\rHigher Dimensional Tables\rThe crosstab() function lets you create tables out of more than two categories. Higher dimensional tables can be a little confusing to look at, but they can also yield finer-grained insight into interactions between multiple variables. Let’s create a 3-way table inspecting survival, sex and passenger class:\nsurv_sex_class = pd.crosstab(index=tit_train[\u0026quot;Survived\u0026quot;], columns=[tit_train[\u0026quot;Pclass\u0026quot;],\rtit_train[\u0026quot;Sex\u0026quot;]],\rmargins=True) # Include row and column totals\rsurv_sex_class\r## Pclass 1 2 3 All\r## Sex female male female male female male ## Survived ## 0 3 77 6 91 72 300 549\r## 1 91 45 70 17 72 47 342\r## All 94 122 76 108 144 347 891\rNotice that by passing a second variable to the columns argument, the resulting table has columns categorized by both Pclass and Sex.\nsurv_sex_class[2] \r## Sex female male\r## Survived ## 0 6 91\r## 1 70 17\r## All 76 108\rsurv_sex_class[2][\u0026quot;female\u0026quot;] \r## Survived\r## 0 6\r## 1 70\r## All 76\r## Name: female, dtype: int64\rDue to the convenient hierarchical structure of the table, we still use one division to get the proportion of survival across each column:\nsurv_sex_class/surv_sex_class.loc[\u0026quot;All\u0026quot;] # Divide by column totals\r## Pclass 1 2 3 All\r## Sex female male female male female male ## Survived ## 0 0.031915 0.631148 0.078947 0.842593 0.5 0.864553 0.616162\r## 1 0.968085 0.368852 0.921053 0.157407 0.5 0.135447 0.383838\r## All 1.000000 1.000000 1.000000 1.000000 1.0 1.000000 1.000000\rHere we see that over 90% of women in 1st class and 2nd class survived, but only 50% of women in 3rd class survived. Men in 1st class also survived with a greater rate than men in lower classes. Passenger class seems to have a significant impact on survival, so it would likely be useful to include as a feature in a predictive model.\n\r\rSummary\rFrequency tables are effective tools for understanding relationships between features of a dataset. It is easy to inspect the data in the frequency tables. Sometimes creating plots from the frequency tables helps in detecting the patterns in the data.\n\r",
    "ref": "/blog/exploring-frequency-tables/"
  },{
    "title": "Exploring and preparing data",
    "date": "",
    "description": "Exploring and preparing data with titanic dataset",
    "body": "\r\r.badCode {\rbackground-color: black;\r}\r.badCode {\rbackground-color: black;\r}\r\rIntroduction\rThe first a part of any data analysis or predictive modeling task is an initial exploration of the datasets. Albeit we collected the datasets ourself and we have already got an inventory of questions in mind that we simply want to answer, it’s important to explore the datasets before doing any serious analysis, since oddities within the datasets can cause bugs and muddle your results. Before exploring deeper questions, we got to answer many simpler ones about the shape and quality of datasets . That said, it’s important to travel into initial data exploration with an enormous picture question in mind.\nThis post aims to boost a number of the questions we ought to consider once we check out a replacement data set for the primary time and show the way to perform various Python operations associated with those questions.\nIn this post, we’ll explore the Titanic disaster training set available from Kaggle.co. The dataset consists of 889 passengers who rode aboard the Titanic.\n\rGetting the dataset\rTo get the dataset into pandas dataframe simply call the function read_csv.\nimport pandas as pd\rimport numpy as np\rtit_train = pd.read_csv(\u0026quot;../data/titanic/train.csv\u0026quot;) # Read the data\r\rChecking the dimensions of the dataset with df.shape and the variable data types of df.dtypes.\ntit_train.shape\r## (891, 12)\rtit_train.dtypes\r## PassengerId int64\r## Survived int64\r## Pclass int64\r## Name object\r## Sex object\r## Age float64\r## SibSp int64\r## Parch int64\r## Ticket object\r## Fare float64\r## Cabin object\r## Embarked object\r## dtype: object\rThe output displays that we re working with a set of 891 records and 12 columns. Most of the column variables are encoded as numeric data types (ints and floats) but a some of them are encoded as “object”.\nCheck the head of the data to get a better sense of what the variables look like:\ntit_train.head(5)\r## PassengerId Survived Pclass ... Fare Cabin Embarked\r## 0 1 0 3 ... 7.2500 NaN S\r## 1 2 1 1 ... 71.2833 C85 C\r## 2 3 1 3 ... 7.9250 NaN S\r## 3 4 1 1 ... 53.1000 C123 S\r## 4 5 0 3 ... 8.0500 NaN S\r## ## [5 rows x 12 columns]\rWe have a combination of numeric columns and columns with text data.\nIn dataset analysis, variables or features that split records into a fixed number of unique categories, such as Sex, are called as categorical variables.\nPandas can be used to interpret categorical variables as such when we load dataset, but we can convert a variable to categorical if necessary\nAfter getting a sense of the datasets structure, it is a good practice to look at a statistical summary of the features with df.describe():\ntit_train.describe().transpose()\r## count mean std ... 50% 75% max\r## PassengerId 891.0 446.000000 257.353842 ... 446.0000 668.5 891.0000\r## Survived 891.0 0.383838 0.486592 ... 0.0000 1.0 1.0000\r## Pclass 891.0 2.308642 0.836071 ... 3.0000 3.0 3.0000\r## Age 714.0 29.699118 14.526497 ... 28.0000 38.0 80.0000\r## SibSp 891.0 0.523008 1.102743 ... 0.0000 1.0 8.0000\r## Parch 891.0 0.381594 0.806057 ... 0.0000 0.0 6.0000\r## Fare 891.0 32.204208 49.693429 ... 14.4542 31.0 512.3292\r## ## [7 rows x 8 columns]\rwe notice that the non-numeric columns are omitted from the statistical summary provided by df.describe().\nWe can find the summary of the categorical variables by passing only those columns to describe():\n\rcat_vars = tit_train.dtypes[tit_train.dtypes == \u0026quot;object\u0026quot;].index\rprint(cat_vars)\r\r## Index([\u0026#39;Name\u0026#39;, \u0026#39;Sex\u0026#39;, \u0026#39;Ticket\u0026#39;, \u0026#39;Cabin\u0026#39;, \u0026#39;Embarked\u0026#39;], dtype=\u0026#39;object\u0026#39;)\rtit_train[cat_vars].describe().transpose()\r## count unique top freq\r## Name 891 891 Coleff, Mr. Satio 1\r## Sex 891 2 male 577\r## Ticket 891 681 347082 7\r## Cabin 204 147 C23 C25 C27 4\r## Embarked 889 3 S 644\rThe summary of the categorical features shows the count of non-NaN records, the number of unique categories, the most frequent occurring value and the number of occurrences of the most frequent value.\nAlthough describe() gives a concise overview of each variable, it does not necessarily give us enough information to determine what each variable means.\nCertain features like “Age” and “Fare” are easy to understand, while others like “SibSp” and “Parch” are not. The details of these are provided by kaggle on the data download page.\n# VARIABLE DESCRIPTIONS:\r# survival Survival\r# (0 = No; 1 = Yes)\r# pclass Passenger Class\r# (1 = 1st; 2 = 2nd; 3 = 3rd)\r# name Name\r# sex Sex\r# age Age\r# sibsp Number of Siblings/Spouses Aboard\r# parch Number of Parents/Children Aboard\r# ticket Ticket Number\r# fare Passenger Fare\r# cabin Cabin\r# embarked Port of Embarkation\r# (C = Cherbourg; Q = Queenstown; S = Southampton)\rAfter looking at the data we ask yourself a few questions:\n\r\rQUESTIONS\r\r\r\rDo we require all of the variables ?\r\rShould we transform any variables ?\r\rCheck if there are any NA values, outliers etc ?\r\rShould we create new variables?\r\r\r\rDo we require all of the Variables?\rRemoval of unnecessary variables is a first step when dealing with any data set, since removing variables reduces complexity and can make computation on the data faster.\nWhether we should get rid of a variable or not will depend on size of the data set and the goal of the analysis. With a dataset like the titanic data, there’s no requirement to remove variables from a computing perspective.\rBut it can be helpful to drop variables that will only distract from your goal.\nLet’s go through each variable and consider whether we should keep it or not in the context of survival prediction.\n“PassengerId” is just a number assigned to each passenger. We can remove it\ndel tit_train[\u0026#39;PassengerId\u0026#39;]\rVariable “Survived” shows whether each passenger lived or died. Since survival prediction is our goal, we definitely need to keep it.\nFeatures describing passengers numerically or grouping them into a few broad categories could be useful for survival prediction. Therefore variables Pclass, Sex, Age, SibSp, Parch, Fare and Embarked can be kept.\nfurther, “Name” appears to be a character string of the name of each passenger and it will also help in identifying passenger so we can keep it\nNext, let’s see at “Ticket”\ntit_train[\u0026#39;Ticket\u0026#39;][0:10]\r## 0 A/5 21171\r## 1 PC 17599\r## 2 STON/O2. 3101282\r## 3 113803\r## 4 373450\r## 5 330877\r## 6 17463\r## 7 349909\r## 8 347742\r## 9 237736\r## Name: Ticket, dtype: object\rtit_train[\u0026#39;Ticket\u0026#39;].describe()\r## count 891\r## unique 681\r## top 347082\r## freq 7\r## Name: Ticket, dtype: object\rTicket has 681 unique values: almost as many as there are passengers. Categorical variables with this many levels are generally not very useful for prediction. Let’s remove it\ndel tit_train[\u0026#39;Ticket\u0026#39;]\rLastly let’s see the “Cabin” variable\ntit_train[\u0026#39;Cabin\u0026#39;][0:10]\r## 0 NaN\r## 1 C85\r## 2 NaN\r## 3 C123\r## 4 NaN\r## 5 NaN\r## 6 E46\r## 7 NaN\r## 8 NaN\r## 9 NaN\r## Name: Cabin, dtype: object\rtit_train[\u0026#39;Cabin\u0026#39;].describe()\r## count 204\r## unique 147\r## top C23 C25 C27\r## freq 4\r## Name: Cabin, dtype: object\rCabin also has 147 unique values, which shows it may not be useful for prediction. On the other hand, the names of the different levels for the cabin variable seem to have a some structure, each starts with a capital letter followed by a number. We can use that structure to reduce the number of levels to make categories large enough that they might be useful for prediction later on. So Lets Keep Cabin for now.\n\rShould we transform Any Variables?\rPclass is an integer variable that indicates a passenger’s class, with 1 being first class, 2 as second class and 3 as third class. We can transform this by transforming Pclass into an ordered categorical variable\n\rpclass_new = pd.Categorical(tit_train[\u0026#39;Pclass\u0026#39;], ordered=True)\rpclass_new = pclass_new.rename_categories([\u0026quot;class1\u0026quot;,\u0026quot;class2\u0026quot;,\u0026quot;class3\u0026quot;])\rpclass_new.describe()\r## counts freqs\r## categories ## class1 216 0.242424\r## class2 184 0.206510\r## class3 491 0.551066\rtit_train[\u0026#39;Pclass\u0026#39;] = pclass_new\rNow see the Cabin variable. It appears that each Cabin is in a general section of the ship indicated by the capital letter at the start of each factor level\ntit_train[\u0026#39;Cabin\u0026#39;].unique()\r## array([nan, \u0026#39;C85\u0026#39;, \u0026#39;C123\u0026#39;, \u0026#39;E46\u0026#39;, \u0026#39;G6\u0026#39;, \u0026#39;C103\u0026#39;, \u0026#39;D56\u0026#39;, \u0026#39;A6\u0026#39;,\r## \u0026#39;C23 C25 C27\u0026#39;, \u0026#39;B78\u0026#39;, \u0026#39;D33\u0026#39;, \u0026#39;B30\u0026#39;, \u0026#39;C52\u0026#39;, \u0026#39;B28\u0026#39;, \u0026#39;C83\u0026#39;, \u0026#39;F33\u0026#39;,\r## \u0026#39;F G73\u0026#39;, \u0026#39;E31\u0026#39;, \u0026#39;A5\u0026#39;, \u0026#39;D10 D12\u0026#39;, \u0026#39;D26\u0026#39;, \u0026#39;C110\u0026#39;, \u0026#39;B58 B60\u0026#39;, \u0026#39;E101\u0026#39;,\r## \u0026#39;F E69\u0026#39;, \u0026#39;D47\u0026#39;, \u0026#39;B86\u0026#39;, \u0026#39;F2\u0026#39;, \u0026#39;C2\u0026#39;, \u0026#39;E33\u0026#39;, \u0026#39;B19\u0026#39;, \u0026#39;A7\u0026#39;, \u0026#39;C49\u0026#39;, \u0026#39;F4\u0026#39;,\r## \u0026#39;A32\u0026#39;, \u0026#39;B4\u0026#39;, \u0026#39;B80\u0026#39;, \u0026#39;A31\u0026#39;, \u0026#39;D36\u0026#39;, \u0026#39;D15\u0026#39;, \u0026#39;C93\u0026#39;, \u0026#39;C78\u0026#39;, \u0026#39;D35\u0026#39;,\r## \u0026#39;C87\u0026#39;, \u0026#39;B77\u0026#39;, \u0026#39;E67\u0026#39;, \u0026#39;B94\u0026#39;, \u0026#39;C125\u0026#39;, \u0026#39;C99\u0026#39;, \u0026#39;C118\u0026#39;, \u0026#39;D7\u0026#39;, \u0026#39;A19\u0026#39;,\r## \u0026#39;B49\u0026#39;, \u0026#39;D\u0026#39;, \u0026#39;C22 C26\u0026#39;, \u0026#39;C106\u0026#39;, \u0026#39;C65\u0026#39;, \u0026#39;E36\u0026#39;, \u0026#39;C54\u0026#39;,\r## \u0026#39;B57 B59 B63 B66\u0026#39;, \u0026#39;C7\u0026#39;, \u0026#39;E34\u0026#39;, \u0026#39;C32\u0026#39;, \u0026#39;B18\u0026#39;, \u0026#39;C124\u0026#39;, \u0026#39;C91\u0026#39;, \u0026#39;E40\u0026#39;,\r## \u0026#39;T\u0026#39;, \u0026#39;C128\u0026#39;, \u0026#39;D37\u0026#39;, \u0026#39;B35\u0026#39;, \u0026#39;E50\u0026#39;, \u0026#39;C82\u0026#39;, \u0026#39;B96 B98\u0026#39;, \u0026#39;E10\u0026#39;, \u0026#39;E44\u0026#39;,\r## \u0026#39;A34\u0026#39;, \u0026#39;C104\u0026#39;, \u0026#39;C111\u0026#39;, \u0026#39;C92\u0026#39;, \u0026#39;E38\u0026#39;, \u0026#39;D21\u0026#39;, \u0026#39;E12\u0026#39;, \u0026#39;E63\u0026#39;, \u0026#39;A14\u0026#39;,\r## \u0026#39;B37\u0026#39;, \u0026#39;C30\u0026#39;, \u0026#39;D20\u0026#39;, \u0026#39;B79\u0026#39;, \u0026#39;E25\u0026#39;, \u0026#39;D46\u0026#39;, \u0026#39;B73\u0026#39;, \u0026#39;C95\u0026#39;, \u0026#39;B38\u0026#39;,\r## \u0026#39;B39\u0026#39;, \u0026#39;B22\u0026#39;, \u0026#39;C86\u0026#39;, \u0026#39;C70\u0026#39;, \u0026#39;A16\u0026#39;, \u0026#39;C101\u0026#39;, \u0026#39;C68\u0026#39;, \u0026#39;A10\u0026#39;, \u0026#39;E68\u0026#39;,\r## \u0026#39;B41\u0026#39;, \u0026#39;A20\u0026#39;, \u0026#39;D19\u0026#39;, \u0026#39;D50\u0026#39;, \u0026#39;D9\u0026#39;, \u0026#39;A23\u0026#39;, \u0026#39;B50\u0026#39;, \u0026#39;A26\u0026#39;, \u0026#39;D48\u0026#39;,\r## \u0026#39;E58\u0026#39;, \u0026#39;C126\u0026#39;, \u0026#39;B71\u0026#39;, \u0026#39;B51 B53 B55\u0026#39;, \u0026#39;D49\u0026#39;, \u0026#39;B5\u0026#39;, \u0026#39;B20\u0026#39;, \u0026#39;F G63\u0026#39;,\r## \u0026#39;C62 C64\u0026#39;, \u0026#39;E24\u0026#39;, \u0026#39;C90\u0026#39;, \u0026#39;C45\u0026#39;, \u0026#39;E8\u0026#39;, \u0026#39;B101\u0026#39;, \u0026#39;D45\u0026#39;, \u0026#39;C46\u0026#39;, \u0026#39;D30\u0026#39;,\r## \u0026#39;E121\u0026#39;, \u0026#39;D11\u0026#39;, \u0026#39;E77\u0026#39;, \u0026#39;F38\u0026#39;, \u0026#39;B3\u0026#39;, \u0026#39;D6\u0026#39;, \u0026#39;B82 B84\u0026#39;, \u0026#39;D17\u0026#39;, \u0026#39;A36\u0026#39;,\r## \u0026#39;B102\u0026#39;, \u0026#39;B69\u0026#39;, \u0026#39;E49\u0026#39;, \u0026#39;C47\u0026#39;, \u0026#39;D28\u0026#39;, \u0026#39;E17\u0026#39;, \u0026#39;A24\u0026#39;, \u0026#39;C50\u0026#39;, \u0026#39;B42\u0026#39;,\r## \u0026#39;C148\u0026#39;], dtype=object)\rIf we grouped the cabin just by this letter, we could lesser the number of levels while getting some useful information.\n\rchr_cabin = tit_train[\u0026quot;Cabin\u0026quot;].astype(str)\rn_Cabin = np.array([cabin[0] for cabin in chr_cabin]) n_Cabin = pd.Categorical(n_Cabin)\rn_Cabin.describe()\r## counts freqs\r## categories ## A 15 0.016835\r## B 47 0.052750\r## C 59 0.066218\r## D 33 0.037037\r## E 32 0.035915\r## F 13 0.014590\r## G 4 0.004489\r## T 1 0.001122\r## n 687 0.771044\rThe output of describe() shows we can group Cabin into broader categories, but we also discovered something interesting: 688 of the records have Cabin are “n” which is shortened from “nan”. In other words, more than 2/3 of the passengers do not have a cabin.\nA missing cabin variable could be an indication that a passenger died.\nWe can keep the new variable cabin\ntit_train[\u0026quot;Cabin\u0026quot;] = n_Cabin\r\rChecking to if there are null Values, Outliers or Other garbage Values?\rTo check the missing values we us pd.isnull() function for example\n\rmock_vector = pd.Series([1,None,3,None,7,8])\rmock_vector.isnull()\r## 0 False\r## 1 True\r## 2 False\r## 3 True\r## 4 False\r## 5 False\r## dtype: bool\rIf the missing values are numeric then they can be simple deleted. If missing values are categorical then they can be treated as additional category with value as NA.\nTo check if there is missing age values in titanic dataset\ntit_train[\u0026quot;Age\u0026quot;].describe()\r## count 714.000000\r## mean 29.699118\r## std 14.526497\r## min 0.420000\r## 25% 20.125000\r## 50% 28.000000\r## 75% 38.000000\r## max 80.000000\r## Name: Age, dtype: float64\rWe see that the count of age (712) is less than the total row count of dataset(889).\rTo check indexes of the missing ages we use np.where()\nmissvalues = np.where(tit_train[\u0026quot;Age\u0026quot;].isnull() == True)\rmissvalues\r## (array([ 5, 17, 19, 26, 28, 29, 31, 32, 36, 42, 45, 46, 47,\r## 48, 55, 64, 65, 76, 77, 82, 87, 95, 101, 107, 109, 121,\r## 126, 128, 140, 154, 158, 159, 166, 168, 176, 180, 181, 185, 186,\r## 196, 198, 201, 214, 223, 229, 235, 240, 241, 250, 256, 260, 264,\r## 270, 274, 277, 284, 295, 298, 300, 301, 303, 304, 306, 324, 330,\r## 334, 335, 347, 351, 354, 358, 359, 364, 367, 368, 375, 384, 388,\r## 409, 410, 411, 413, 415, 420, 425, 428, 431, 444, 451, 454, 457,\r## 459, 464, 466, 468, 470, 475, 481, 485, 490, 495, 497, 502, 507,\r## 511, 517, 522, 524, 527, 531, 533, 538, 547, 552, 557, 560, 563,\r## 564, 568, 573, 578, 584, 589, 593, 596, 598, 601, 602, 611, 612,\r## 613, 629, 633, 639, 643, 648, 650, 653, 656, 667, 669, 674, 680,\r## 692, 697, 709, 711, 718, 727, 732, 738, 739, 740, 760, 766, 768,\r## 773, 776, 778, 783, 790, 792, 793, 815, 825, 826, 828, 832, 837,\r## 839, 846, 849, 859, 863, 868, 878, 888], dtype=int64),)\rlen(missvalues)\r## 1\rBefore we do anything with missing values its good to check the distribution of the missing values to know the central tendency of age.\ntit_train.hist(column=\u0026#39;Age\u0026#39;, figsize=(9,6), bins=20) \rThe histogram shows that couple of passengers are near age 80.\nTo check the fare variable we create the box plot\ntit_train[\u0026quot;Fare\u0026quot;].plot(kind=\u0026quot;box\u0026quot;,\rfigsize=(9,9))\r50% of the data in the box plot represents the median. There are outliers in the data. There are passengers who paid double the amount than any other passenger. We can check this using np.where() function\nind = np.where(tit_train[\u0026quot;Fare\u0026quot;] == max(tit_train[\u0026quot;Fare\u0026quot;]) )\rtit_train.loc[ind]\r## Survived Pclass ... Cabin Embarked\r## 258 1 class1 ... n C\r## 679 1 class1 ... B C\r## 737 1 class1 ... B C\r## ## [3 rows x 10 columns]\rBefore modeling datasets using ML models it is better to address missing values, outliers, mislabeled data, bad data because they can corrupt the analysis and lead to wrong results.\n\rShould we Create New Variables?\rThe decision to create new variables should be taken while preparing the data. The new variable could represent aggregate of existing variables, for example in titanic dataset we can create a new variable called family which stores the number of members in that family.\ntit_train[\u0026quot;Family\u0026quot;] = tit_train[\u0026quot;SibSp\u0026quot;] + tit_train[\u0026quot;Parch\u0026quot;]\rwe can check who has most family members on the board\nmostfamily = np.where(tit_train[\u0026quot;Family\u0026quot;] == max(tit_train[\u0026quot;Family\u0026quot;]))\rtit_train.loc[mostfamily]\r## Survived Pclass Name ... Cabin Embarked Family\r## 159 0 class3 Sage, Master. Thomas Henry ... n S 10\r## 180 0 class3 Sage, Miss. Constance Gladys ... n S 10\r## 201 0 class3 Sage, Mr. Frederick ... n S 10\r## 324 0 class3 Sage, Mr. George John Jr ... n S 10\r## 792 0 class3 Sage, Miss. Stella Anna ... n S 10\r## 846 0 class3 Sage, Mr. Douglas Bullen ... n S 10\r## 863 0 class3 Sage, Miss. Dorothy Edith \u0026quot;Dolly\u0026quot; ... n S 10\r## ## [7 rows x 11 columns]\r\r\rSummary\rThere are question that should be answered while investing any dataset. Once the basic questions are answered one can move further to find relationship between variables/features and build the machine learning models.\n\r",
    "ref": "/blog/exploring-and-preparing-data/"
  },{
    "title": "Handling numeric data",
    "date": "",
    "description": "Handling numeric data using mtcars dataset. Numeric data is easiar to deal with in Data analysis projects",
    "body": "\r\rIntroduction\rIn data analysis projects Numeric data present is very different from the text data. The numeric data is relatively clean than the text data that is why it is easiear to deal with them. In this post, we’ll learn few common operations used to prepare numeric data for use in analysis and predictive models using mtcars dataset.\nGetting the dataset\rTo get the dataset into pandas dataframe simply call the function read_csv.\nimport pandas as pd\rimport numpy as np\rmt_car = pd.read_csv(\u0026quot;../data/mtcars/mt_car.csv\u0026quot;) # Read the data\r\rCenter and Scale\rTo center and scale the dataset we substract the mean value from each data point. Subtracting the mean centers the data around zero and sets the new mean to zero. Lets try do it with mtcars dataset.\n\rprint (mt_car.head() )\r## m_pg n_cyl disp_ment n_hp dra w_t q_sec v_s a_m n_gear n_carb\r## 0 21.0 6.0 160.0 110.0 3.90 2.62 16.46 0.0 1.0 4.0 4.0\r## 1 21.0 6.0 160.0 110.0 3.90 2.88 17.02 0.0 1.0 4.0 4.0\r## 2 22.8 4.0 108.0 93.0 3.85 2.32 18.61 1.0 1.0 4.0 1.0\r## 3 21.4 6.0 258.0 110.0 3.08 3.22 19.44 1.0 0.0 3.0 1.0\r## 4 18.7 8.0 360.0 175.0 3.15 3.44 17.02 0.0 0.0 3.0 2.0\rcol_means = mt_car.sum()/mt_car.shape[0] # Get column means\rcol_means\r## m_pg 20.090625\r## n_cyl 6.187500\r## disp_ment 230.721875\r## n_hp 146.687500\r## dra 3.596563\r## w_t 3.218437\r## q_sec 17.848750\r## v_s 0.437500\r## a_m 0.406250\r## n_gear 3.687500\r## n_carb 2.812500\r## dtype: float64\rNow we need to subtract the means of the column from each row in element-wise way to zero center the data. Pandas can peform math operations involving dataframews and columns on element-wise row-by-row basis by default so it can be simply subtracted from column means series from the dataset to center it.\n\rcenter_ed = mt_car - col_means\rprint(center_ed.describe())\r## m_pg n_cyl disp_ment ... a_m n_gear n_carb\r## count 3.200000e+01 32.000000 3.200000e+01 ... 32.000000 32.000000 32.0000\r## mean 3.996803e-15 0.000000 -3.907985e-14 ... 0.000000 0.000000 0.0000\r## std 6.026948e+00 1.785922 1.239387e+02 ... 0.498991 0.737804 1.6152\r## min -9.690625e+00 -2.187500 -1.596219e+02 ... -0.406250 -0.687500 -1.8125\r## 25% -4.665625e+00 -2.187500 -1.098969e+02 ... -0.406250 -0.687500 -0.8125\r## 50% -8.906250e-01 -0.187500 -3.442188e+01 ... -0.406250 0.312500 -0.8125\r## 75% 2.709375e+00 1.812500 9.527812e+01 ... 0.593750 0.312500 1.1875\r## max 1.380938e+01 1.812500 2.412781e+02 ... 0.593750 1.312500 5.1875\r## ## [8 rows x 11 columns]\rAfter centering the data we see that negative values are below average while positive values are above average. Next we can put it on common scale using the standard deviation as.\n\rcol_deviations = mt_car.std(axis=0) # Get column standard deviations\rcenter_n_scale = center_ed/col_deviations print(center_n_scale.describe())\r## m_pg n_cyl ... n_gear n_carb\r## count 3.200000e+01 3.200000e+01 ... 3.200000e+01 3.200000e+01\r## mean 6.678685e-16 -6.938894e-18 ... -5.030698e-17 1.387779e-17\r## std 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\r## min -1.607883e+00 -1.224858e+00 ... -9.318192e-01 -1.122152e+00\r## 25% -7.741273e-01 -1.224858e+00 ... -9.318192e-01 -5.030337e-01\r## 50% -1.477738e-01 -1.049878e-01 ... 4.235542e-01 -5.030337e-01\r## 75% 4.495434e-01 1.014882e+00 ... 4.235542e-01 7.352031e-01\r## max 2.291272e+00 1.014882e+00 ... 1.778928e+00 3.211677e+00\r## ## [8 rows x 11 columns]\rWe see that after dividing by the standard deviation, every variable now has a standard deviation of 1. At this point, all the columns have roughly the same mean and scale of spread about the mean.\nManually centering and scaling is a good exercise, but it is often possible to perform common data preprocessing using functions available in the Python libraries. The Python library scikit-learn, a package for predictive modeling and data analysis, has pre-processing tools including a scale() function for centering and scaling data:\n\rfrom sklearn import preprocessing\rscale_data = preprocessing.scale(mt_car) scale_car = pd.DataFrame(scale_data, index = mt_car.index,\rcolumns=mt_car.columns)\rprint(scale_car.describe() )\r## m_pg n_cyl ... n_gear n_carb\r## count 3.200000e+01 3.200000e+01 ... 3.200000e+01 3.200000e+01\r## mean -5.481726e-16 4.163336e-17 ... -1.144917e-16 -6.938894e-18\r## std 1.016001e+00 1.016001e+00 ... 1.016001e+00 1.016001e+00\r## min -1.633610e+00 -1.244457e+00 ... -9.467293e-01 -1.140108e+00\r## 25% -7.865141e-01 -1.244457e+00 ... -9.467293e-01 -5.110827e-01\r## 50% -1.501383e-01 -1.066677e-01 ... 4.303315e-01 -5.110827e-01\r## 75% 4.567366e-01 1.031121e+00 ... 4.303315e-01 7.469671e-01\r## max 2.327934e+00 1.031121e+00 ... 1.807392e+00 3.263067e+00\r## ## [8 rows x 11 columns]\rpreprocessing.scale() returns ndarrays which needs to be converted back to dataframe.\n\rHandling Skewed-data\rTo understand whether the data is skewed or not we need to plot it. The overall shape and how the data is spread out can have a significant impact on the analysis and modeling\nnorm_dist = np.random.normal(size=10000) norm_dist= pd.DataFrame(norm_dist)\rnorm_dist.hist(figsize=(8,8), bins=30)\rNotice how the normally distributed data looks roughly symmetric with a bell-shaped curve. Now let’s generate some skewed data\nskew = np.random.exponential(scale=2, size= 10000) skew = pd.DataFrame(skew)\rskew.hist(figsize=(8,8),bins=50)\r\rCorrelation\rThe model that we use in predictive modeling have features and each feature is related to other features in some way or the other. Using corr() we can find how these features are related with each other\n\r\r",
    "ref": "/blog/handling-numeric-data/"
  },{
    "title": "Data Science",
    "date": "",
    "description": "A Data science process is a life cycle process for delivering a data science project that helps a data consumer to gain insights from the data.",
    "body": "\r\rData Science Process?\rA Data science process is a life cycle process for delivering a data science project that helps a data consumer to gain insights from the data.\nIt involves the following steps. They are as follows:\nDescribing the data problem by asking the right questions\rCollecting the data\rTransforming the raw data in to the format required applying Data munging techniques\rExploring Data by using EDA techniques\rAdvanced Exploratory data analysis.\rBuilding model and perform model assessment\rPresentation and Automation\r\r1 Describing the data problem by asking the right questions\rWhen the user presents their question, for example “What are my expected findings after the project is finished?”, or “What kind of information can I extract through the data science process?,” different results will be given. Therefore, asking the right\rquestion and setting a research goal is essential in the first place, for the question itself determines the objective and target of the data science project.\n## Examples of specific questions ?\r# - For future farming technique for a specified plant which treatment condition gives the largest plant yields on average\r# - Which web-site design is most viewed by customers ?\r# - What will the temperature be next Thurseday ?\r# - What will my fourth quarter sales in region C be ?\r# - Which viewers like the same types of movies?\r# - Which plant is in this image ?\r# - What will be the Share price next month ?\r\r2 Collecting the data\rNext step is to collect the relevent data from the data source. The collected data is known as raw data. The data can be extracted by file IO, JDBC/ODBC or using web crawler. The data is collected from the conerned stakeholders over a period of time and may be unstructured/raw format.\n\r3 Data munging or Data Preparation\rThe next step is data munging (or wrangling), a step to help map raw data into a more convenient format for further processing. During this step, there are many processes, such as data parsing, sorting, splitting, merging, filtering, missing value completion, and other processes to convert and organize the data, and enable it to fit into a consumable structure. Later, the mapped data can be further used for data aggregation, analysis, or visualization.\rThis step involves Data Cleansing, Data tranformation and combining data. For example during the cleansing process it is necessary to check for null values, during transformation it is necessary to combine and reduce number of variables, combing data includes creating data views and merging or joining data sets. Once data is clean we look for statistical properties such as distributions,correlations, and outliers.\nCommon data errors found during this phase are\n\rMistakes during data entry\rRedundant white space: Present of white spaces\rImpossible values: values which are not possible in the given context\rMissing values: features or data not labeled or having null values\rOutliers: Data points that are significantly spread out by the majority of the observations\r\rFor example to detect outliers statistical methods such as IQR and boxplot is used.\rDescribe the data as unimodal, bimodal, multimodal; symmetric, right-skewed, left-skewed to find out the outliers.\n\r4 Exploring Data by using EDA techniques\rAfter the data munging step, users can do further analysis toward data processing. The most basic analysis is to perform exploratory data analysis. Exploratory data analysis involves analyzing a datamatrix by summarizing its characteristics. Performing basic statistical, aggregation, and visual methods are also crucial tasks to help the user understand data characteristics, which are beneficial for the user to capture the majority, trends, and outliers easily through plots. Basic exploratory analysis includes creating simple graphs, combined graphs and summarising the findings based on the graphs.\nExperimental Data Analysis is a process of looking at a data set to see what are the appropriate statistical inferences that can possibly be learned. For univariate data, we can ask if the data is approximately normal, longer tailed, or shorter tailed? Does it have symmetry, or is it skewed? Is it unimodal, bimodal or multi-modal. The main tool is the proper use of computer graphics.\n\rbarplots for categorical data\n\rhistogram, dot plots, stem and leaf plots to see the shape of numerical distributions\n\rboxplots to see summaries of a numerical distribution, useful in comparing distributions and identifying long and short-tailed distributions.\n\rnormal probability plots To see if data is approximately normal\n\r\r\r5 Advanced exploratory data analysis\rUntil now, the descriptive statistic gives a general description of data features. However, one would like to generate an inference rule for the user to prepare and predict data features based on input parameters. Therefore, the application of machine learning enables the user to generate an inferential model, where the user can input a training dataset to generate a predictive model. After this, the prediction model can be utilized to predict the output value or label based on given parameters.\nExample: Applying anova to plantgrowth dataset\nrequire(stats); require(graphics)\rboxplot(weight ~ group, data = PlantGrowth, main = \u0026quot;PlantGrowth data\u0026quot;,\rylab = \u0026quot;Dried weight of plants\u0026quot;, col = \u0026quot;lightgray\u0026quot;,\rnotch = TRUE, varwidth = TRUE)\ranova(lm(weight ~ group, data = PlantGrowth))\r## Analysis of Variance Table\r## ## Response: weight\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## group 2 3.7663 1.8832 4.8461 0.01591 *\r## Residuals 27 10.4921 0.3886 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r\r6 Model building and assessment\rTo assess whether the generating model performs the best in the data estimation of a given problem, one must perform a model selection. The selection method here involves many steps, including data preprocessing, tuning parameters, and even switching the machine learning algorithm. However, one thing that is important to keep in mind is that the simplest model frequently achieves the best results in predictive or exploratory power whereas complex models often result in over fitting.\nExample: Simple prediction using confidence and prediction intervals\n## Predictions\rx \u0026lt;- rnorm(15)\ry \u0026lt;- x + rnorm(15)\rpredict(lm(y ~ x))\r## 1 2 3 4 5 6 ## 0.59811639 0.60050528 -0.10926162 0.26184090 -0.14078942 0.72495537 ## 7 8 9 10 11 12 ## -0.67607117 -0.05058166 0.73899499 -1.97569695 0.34840145 0.06887359 ## 13 14 15 ## -0.18436236 0.15126159 0.81552839\rnew \u0026lt;- data.frame(x = seq(-3, 3, 0.5))\rpredict(lm(y ~ x), new, se.fit = TRUE)\r## $fit\r## 1 2 3 4 5 6 ## -2.09914109 -1.73955851 -1.37997592 -1.02039333 -0.66081075 -0.30122816 ## 7 8 9 10 11 12 ## 0.05835443 0.41793701 0.77751960 1.13710219 1.49668477 1.85626736 ## 13 ## 2.21584995 ## ## $se.fit\r## 1 2 3 4 5 6 7 8 ## 1.0916365 0.9289568 0.7703412 0.6189220 0.4815366 0.3739847 0.3271479 0.3651863 ## 9 10 11 12 13 ## 0.4678359 0.6029543 0.7532673 0.9112866 1.0736150 ## ## $df\r## [1] 13\r## ## $residual.scale\r## [1] 1.266509\rpred.w.plim \u0026lt;- predict(lm(y ~ x), new, interval = \u0026quot;prediction\u0026quot;)\rpred.w.clim \u0026lt;- predict(lm(y ~ x), new, interval = \u0026quot;confidence\u0026quot;)\rmatplot(new$x, cbind(pred.w.clim, pred.w.plim[,-1]),\rlty = c(1,2,2,3,3), type = \u0026quot;l\u0026quot;, ylab = \u0026quot;predicted y\u0026quot;)\r\r7 Presentation and Automation\rFinally,The last step of the data science model is presenting your results and automating the analysis, if needed. One goal of a project is to change a process and/or make better decisions. We may still need to convince the business that our findings will indeed change the business process as expected. This is where we can shine in as influencer role. The importance of this step is more apparent in\rprojects on a strategic and tactical level. Certain projects require to perform the business process over and over again, so automating the project will save time.\n\r\r",
    "ref": "/blog/data-science-overview/"
  },{
    "title": "Machine Learning Overview",
    "date": "",
    "description": "Machine learning is the science, in which we focus on teaching machines or computers to perform certain tasks without being given specific instructions",
    "body": "\r\r\rMachine Learning ?\r\rSupervised Learning\rUnsupervised Learning\rReinforcement Learning\rDeep Learning\rFields of Application\rMachine Learning Frameworks\r\rTorch\rTheano\rCaffe\rKeras\rTensorFlow\r\rMachine Learning Algorithms\r\rLinear Regression\rClassification\rK-Nearest-Neighbors\rNaive-Bayes\rLogistic Regression\rDecision Trees\rRandom Forest\rSupport Vector Machines\r\r\r\r\rMachine Learning ?\rMachine learning is fundamentally is just the science, in which we focus on teaching machines or computers to perform certain tasks without being given specific instructions. We want our machines to learn how to do something themselves without explaining it to them.\nIn order to do this, we oftentimes look at how the human brain works and try to design virtual brains that work in a similar manner.\nMachine learning and artificial intelligence are different. Artificial intelligence is a broad field and every system that can learn and solve problems might be considered an AI. Machine learning is one specific approach to this broad field.\nSupervised Learning\rIn machine learning we have different approaches or types. The two main approaches are supervised learning and unsupervised learning . So let’s first talk about supervised learning.\nHere, we give our model a set of inputs and also the corresponding outputs, which are the desired results. In this way, the model learns to match certain inputs to certain outputs and it adjusts its structure. It learns to make connections between what was put in and what the desired output is. It understands the correlation. When trained well enough, we can use the model to make predictions for inputs that we don’t know the results for.\nClassic supervised learning algorithms are regressions, classifications and support vector machines.\n\rUnsupervised Learning\rWith unsupervised learning on the other hand, we don’t give our model the desired results while training. Not because we don’t want to but because we don’t know them. This approach is more like a kind of pattern recognition. We give our model a set of input data and it then has to look for patterns in it. Once the model is trained, we can put in new data and our model will need to make decisions. Since the model doesn’t get any information about classes or results, it has to work with similarities and patterns in the data and categorize or cluster it by itself.\nClassic unsupervised learning algorithms are clustering, anomaly detection and some applications of neural networks.\n\rReinforcement Learning\rThen there is a third type of machine learning called reinforcement learning . Here we create some model with a random structure. Then we just observe what it does and reinforce or encourage it, when we like what it does. Otherwise, we can also give some negative feedback. The more our model does what we want it to do, the more we reinforce it and the more “rewards” it gets. This might happen in form of a number or a grade, which represents the so-called fitness of the model.\nIn this way, our model learns what is right and what is wrong. You can imagine it a little bit like natural selection and survival of the fittest. We can create 100 random models and kill the 50 models that perform worst. Then the remaining 50 reproduce and the same process repeats. These kinds of algorithms are called genetic algorithms.\rClassic reinforcement learning algorithms are genetic or evolutional algorithms.\n\rDeep Learning\rAnother term that is always confused with machine learning is deep learning . Deep learning however is just one area of machine learning, namely the one, which works with neural networks. Neural networks are a very comprehensive and complex topic.\n\rFields of Application\rActually, it would be easier to list all the areas in which machine learning doesn’t get applied rather than the fields of application. Despite that, we will take a quick look at some of the major areas, in which machine learning gets applied.\n· Research\n· Autonomous Cars\n· Spacecraft\n· Economics and Finance\n· Medical and Healthcare\n· Physics, Biology, Chemistry\n· Engineering\n· Mathematics\n· Robotics\n· Education\n· Forensics\n· Police and Military\n· Marketing\n· Search Engines\n· GPS and Pathfinding Systems\n\rMachine Learning Frameworks\rThis five of the most popular frameworks are:\n\rTorch\n\rTheano\n\rCaffe\n\rKeras\n\rTensorFlow\n\r\rTorch\rTorch originally released in 2002 by Ronan Collobert for the purpose of numeric computing. The computations of Torch involves multidimensional arrays called tensors. Tensors can be processed with regular vector or matrix operations. Torch acquired routines for building, training, and evaluating neural networks.Corporations like IBM and Facebook had great deal of interest in Torch. The other frameworks Theano, Caffe, Keras, and TensorFlow — can be interfaced through Python, which has emerged as the language of choice in the machine learning domain.\n\rTheano\rTheano was developed in 2010 by machile learning group at the University of Montreal. It was released as a library for numeric computation.\rLike NumPy, Theano also provides a wide range of Python modules for operating on multi-dimensional arrays. but theano stores operations in a data structure called a graph, which it compiles into high performance code. Theano supports symbolic differentiation, which makes it possible to find derivatives of functions automatically.\n\rCaffe\rThis framework as developed at UC Berkeley. It is a framework for developing image recognition applications. Caffe is written in C++, and like Theano, it supports GPU acceleration.\n\rKeras\rKeras is modular and simple machine learning framework. Keras acts as an interface to other machine learning frameworks.Keras’s simplicity stems from its small API and intuitive set of functions.\n\rTensorFlow\rThe Google Brain team released TensorFlow 1.0 in 2015, the current version is 1.4. It’s provided under the Apache 2.0 open source license, which means anyone is free to use it modify it and distribute modifications. Python is the primary interface in TensorFlow, but like Caffe, its core functionality is written in C++ for performance. TensorFlow applications can be executed on the Google Cloud Platform (GCP).\n\r\rMachine Learning Algorithms\rLinear Regression\rThe easiest and most basic machine learning algorithm is linear regression .It is a supervised learning algorithm. It is an approach to model the relation between a response i.e. dependent variable ‘Y’ and one or more independent or explanatory variables ‘X1,X2..’ .\n\rClassification\rIn linear regression we now predicted specific output-values for certain given input-values. Sometimes, however, we are not trying to predict outputs but to categorize or classify our elements. For this, we use classification algorithms.\n\rK-Nearest-Neighbors\rIn K-Nearest-Neighbors classifier, we assign the class of the new object, based on its nearest neighbors. The K specifies the amount of neighbors to look at. For example, we could say that we only want to look at the one neighbor who is nearest but we could also say that we want to factor in 100 neighbors.\n\rNaive-Bayes\rNaive-Bayes is a classification algorithm. It helps in finding the probability of one or more outcomes. It is used in decision making such that the outcome with higher probability is more likey to occur than outcome with lower probabilities.\n\rLogistic Regression\rAnother popular classification algorithm is called logistic regression. It looks at probabilities and determines how likely it is that a certain event happens (or a certain class is the right one), given the input data. This is done by plotting a logistic growth curve and splitting the data into two.\n\rDecision Trees\rWith decision tree classifiers, we construct a decision tree out of our training data and use it to predict the classes of new elements.\n\rRandom Forest\rThis classification is based on decision trees. What it does is creating a forest of multiple decision trees. To classify a new object, all the various trees determine a class and the most frequent result gets chosen. This makes the result more accurate and it also prevents overfitting\n\rSupport Vector Machines\rIn machine learning classification, SVM finds an optimal hyperplane that best segregates observations from different classes. A hyperplane is a plane of n -1 dimension that separates the n dimensional feature space of the observations into two spaces. For example, the hyperplane in a two-dimensional feature space is a line, and a surface in a three-dimensional feature space. The optimal hyperplane is picked so that the distance from its nearest points in each space to itself is maximized. And these nearest points are the so-called support vectors.\n\r\r\r",
    "ref": "/blog/machine-learning-overview/"
  },{
    "title": "Getting Started with Hadoop",
    "date": "",
    "description": "Big-data helps to come to conclusion based on the data points arrived from the Big-data analysis, for instance Big-data can be in the form of Petabytes or hexabytes.",
    "body": "\r\r“Hadoop” is a Big Data Management software of the 21st century which is powered by the distributed processing and data storage and the opportunity that such distributed processing offers in terms of cost effectiveness and efficiency.\nHowever, it is imperative to understand what it means to create a Hadoop Ecosystem. Such an effort requires an entire ecosystem of support and a platform for implementation of that has to be developed and matured over a period of time. While from developers viewpoint, the face of Hadoop may be a set of jar files deployed over a set of machines providing services for navigating and working with distributed files thus enabling communication between client and Hadoop cluster for data analysis, the thinking, vision and systems that produce this end product are implemented over several years. The seeds of the Hadoop were first planted in 2002 when Doug cutting and Mike Cafarella took steps in creating a better open-source search engine. This project was as Nutch which can search over 700 million web sites and over a trillion web pages. In essence this is the first piece vision of the Hadoop Ecosystem – Search Engine.\nThe first piece is Core Engine which has its roots in Nutch project, which was later evolved based on the Google’s File System Paper in October 2003 and MapReduce paper in December 2004. These papers later helped in automating steps which were done manually in Nutch project hence in early 2006, when Doug Cutting joined Yahoo and set up a 300-node research cluster there, the storage and processing parts of the Nutch were automated based on the Google’s File System and MapReduce framework to form Hadoop as an open-source Apache Software Foundation project and the Nutch web crawler remained its own separate project.\nMapReduce was developed by Google in 2004. It represents a programming framework for processing, generating and indexing large sets of data on the web. Google developed MapReduce as a general-purpose execution engine that handles the complexities of network communication, parallel programming and fault-tolerance for any kind of analytic application (hand-coded or analytics tool based).\nMapReduce is a framework for processing highly distributable problems across huge datasets. It is an algorithm based framework for computing distributed problems using divide and conquer approach cluster of nodes. MapReduce jobs are divided into two parts. The “Map” function divides a query into multiple parts and processes data at the node level. The “Reduce” function aggregates the results of the “Map” function to determine the “answer” to the query. It consists of Master node which maps input into smaller sub problems/distributes work to clusters, where these worker nodes process smaller problems, return answers back to master node and the master node reduces set of answers back to master node.\nIt divides the basic problem into a set of smaller manageable tasks and assigns them to a large number of computers (nodes). An ideal MapReduce task is too large for any one node to process, but can be accomplished by multiple nodes efficiently.\nMapReduce is named for the two steps at the heart of the framework.\n Map step – The master node takes the input, divides it into smaller sub-problems, and distributes them to worker nodes. Each worker node processes its smaller problem, and passes the result back to its master node. There can be multiple levels of workers.\n Reduce step – The master node collects the results from all of the sub-problems, combines the results into groups based on the key and then assigns them to worker nodes called reducers. Each reducer processes those values and sends the result back to the master node.\nMapReduce can be a huge help in analyzing and processing large chunks of data: buying pattern analysis, customer usage and interest patterns in e-commerce, processing the large amount of data generated in the fields of science and medicine, and processing and analyzing security data, credit scores and other large data-sets in the financial industry.\nAlthough in 2006-era, the Hadoop was not able to handle production search workloads at web scale and only worked on 5 to 20 nodes at that point. Horizontal scalability was an issue because Hadoop was originally required to handle petabytes and exabytes of data distributed over multiple nodes in parallel.\nIt took couple of years for Yahoo to move its web index into Hadoop. This transformation to Hadoop was completed by end of 2008. In 2008 Yahoo has created “Production search index” which was based on 10,000 core Hadoop clusters. Hadoop is used both in Production and research environments and many organisations are using Hadoop. Few examples are Facebook, AOL, Yahoo, IBM.\nIDC predicts that Hadoop software market will be worth 813 million in 2016. Hadoop has created many start-ups and spurred hundreds of millions in venture capital investment since 2008. Hadoop software is driving the big data market today and it will hit more than 23 billion by 2016.\nThe second piece of Hadoop is the creation of Big-Data Hadoop platforms that would enhance efficiency and can help in analysing large datasets and provide meaningful information to help organizations in making business decisions. Two core platforms of the Big Data Analytics involving Hadoop as core component are Cloudera and Hottonworks both providing Hadoop Ecosystem capabilities which enables analysis of more data at lower cost, Horizontal scalability, fault tolerance, improvement in programmability and data access, Co-ordination, workflow, management and deployment. Everyone selling any type of database, business intelligence software or anything else related to data at least connects to Hadoop in some capacity.\nCloudera was the first commercial Hadoop company launched in march 2008, which helped enterprises adopt Hadoop which otherwise would have taken longer time for them to adopt.\nThe third piece of Hadoop is the industry specific vertical applications that can ride on the aforesaid Core engine and platforms. This is where the opportunity lies for example in eCommerce “Analysing customer behaviour in real time”. There is huge scope for Big Data Hadoop applications in healthcare, education, eCommerce industries. The applications in BI, DW and analytics can be developed on a technology stack involving Hadoop HDFS and MapReduce (with HBase \u0026amp; Hive).\nThe fourth piece of Hadoop is the creation and administration of data centres which can help in Big Data processing and over which all the three pieces mentioned above can be deployed. Such data centres should protect the privacy of data and provide adequate security measures.\n",
    "ref": "/blog/hadoop/"
  },{
    "title": "Big Data Analytics",
    "date": "",
    "description": "Big-data helps to come to conclusion based on the data points arrived from the Big-data analysis, for instance Big-data can be in the form of Petabytes or hexabytes.",
    "body": "\r\r\rBig-data and Analytics:\r\r\rBig-data and Analytics:\rDo we know how we can reach at a decision based on mobile data or computer data or Aadhar number? How to find that the new accounts that are opened in the Pradhan Mantri’s Jan Dhan Yojana does not have enough money ?\nHow does company know that their brand is making consumers happy or not? OK, this works based on the Big-data Analytics. Lets us understand what this is, When we call it Big Data Analytics then this directly links us i.e. consumer. Many companies are selling their products online. To find the righteousness of the sells Big-data helps.\nLet us assume that someone is claiming that they have a certain number of hits of the consumer on their web-site, but those number of hits cannot be true unless the consumer have done those number of purchases because someone cannot be claimed as consumer unless he/she purchases something, now this indicates towards unstructured data. When data is structured and some conclusion is drawn from it then it will be known as Big Data Analysis.\nLets us understand it with the help of an example:-\nThere is a manufacturer of jewellery in Surat in India. He has started his website recently. Now he wants to increase the sales on the online platform only. He comes to Delhi and gets a very beautiful website. He has also made arrangements that his website shows up on the top of the google search results, now this kind or arrangement is known as search engine optimization. He is very excited from this. He was very excited that all over the world his website was visited by around 5000 people daily but within a month this excitement went down because out of those only 20 percent used to purchase. He was not able to understand the fact and was trying to increase the sales. Then one of his friends told him about Big data analytics firm in Bangalore. He was not able to understand that what that company will do. His friend told him that Big Data analysis will help him know the trends of the web site visited by the consumers. This will help him get the future strategy of the online jewelry business.\nThe Meaning of Big Data\rBig-data helps to come to conclusion based on the data points arrived from the Big-data analysis, for instance Big-data can be in the form of Petabytes or hexabytes. This may contain the data or information about lakhs, crores or millions of people. This data may come from sources such as customers connect center, social media, mobile data etc. These kinds of numbers are known as structured data but these are not complete. To get to the roots of these is difficult.\nBig-data \u0026amp; Analytics helps companies to arrive at a decision. This will help not only to get new information about the business but this helps in getting the big insights or trends which cannot be identified based on just numbers.\n\rHow it started\rIn September 1956 IBM has presented Random access memory accounting machine. This was the first disk storage product of the world. This has two kinds of machines. One was 305 with a capacity of 5 Megabyte and weight of 1 ton. Second was 650. Actually the use of these was done by telecommunications company to find whether the customers are happy or not. It can be identified with the help of an analysis.\nIn a report by Nasscom in partnership with Blue ocean market intelligence the market of big data is 1 Arab dollar i.e. around 62 Arab rupees.this will be more than double in 2017.\nBig data helps to get different kinds of ideas. With the help of this we can bring the change in management. Regardless of the organization Big-data analysis helps to get insights. With the help of these insights company can form the future strategy.\nFrom the point of view of the consumer it is important because it can increase the number of consumers.\n\rUse\rBig Data helps in decision making. Along with that it helps in getting the direction in which to put the efforts. In financial sector manufacturer of consumer products and telecommunications companies can identify “what the customer wants”. Companies working in the banks and financial sector can find which kinds of loan are being taken more, how the loan is getting repaid. How much debt is getting lost and what are the reasons of that.\nIn the era of supply chain and new brands the interest of the consumer is getting identified with the help of Big-data. Crores of people give their opinion on social media on something and when it is about some brand then with the help of Big-data it is considered to find further insights. Similarly Big Data helps in getting an opinion based on the text, audio and video. What is making consumers happy can be identified with the help of big data analytics.\nPeople use Big-data driven analytics to drive value in diverse fields such as\n\rDriving revenues in e-commerce\rDriving recommendations to consumers in various web applications such as social media, online retail, ads, etc.\rHealthcare applications including diagnosis of diseases\rSocial sciences – predicting election outcomes and econometric models\r\rIn India the market for Big-data analytics is increasing at the rate of 45% and there are opportunities in every sector.\n\r\r",
    "ref": "/blog/big-data-analytics/"
  },{
    "title": "Pandas (Python)",
    "date": "",
    "description": "Pandas is a software module for the Python programming language for the purpose of data manipulation and analysis. It provides data structures and operations to  manipulate numerical tables and time series data. It is build on top of numpy. It is applied for fast analysis and data cleaning and preparation.In this post we will learn various wasy to work with Pandas DataFrames.",
    "body": "\r\r\rPandas\r\rPandas Series\rPANDAS DataFrame\r\r\r\rPandas\rPandas is probably the most powerful library. It provides high-performance tools for data manipulation and analysis. Furthermore, it is very effective at converting data formats and querying data out of databases. The two main data structures of Pandas are the series and the data frame. To work with Pandas, we need to import the module.\nimport pandas as pd\rPandas Series\rA series in Pandas is a one-dimensional array which is labeled. You can imagine it to be the equivalent of an ordinary Python dictionary.\nseries = pd.Series([ 10 , 20 , 30 , 40 ],\r[ \u0026#39;A\u0026#39; , \u0026#39;B\u0026#39; , \u0026#39;C\u0026#39; , \u0026#39;D\u0026#39; ])\rIn order to create a series, we use the constructor of the Series class. The first parameter that we pass is a list full of values (in this case numbers). The second parameter is the list of the indices or keys (in this case strings). When we now print our series, we can see what the structure looks like.\nprint(series)\r## A 10\r## B 20\r## C 30\r## D 40\r## dtype: int64\rThe first column represents the indices, whereas the second column represents the actual values.\nACCESSING VALUES\rThe accessing of values works in the same way that it works with dictionaries. We need to address the respective index or key to get our desired value.\nprint (series[ \u0026#39;C\u0026#39; ])\r## 30\rprint (series[ 1 ])\r## 20\rAs you can see, we can choose how we want to access our elements. We can either address the key or the position that the respective element is at.\nCONVERTING DICTIONARIES\nSince series and dictionaries are quite similar, we can easily convert our Python dictionaries into Pandas series.\nmyDict = { \u0026#39;A\u0026#39; : 10 , \u0026#39;B\u0026#39; : 20 , \u0026#39;C\u0026#39; : 30 }\rseries = pd.Series(myDict)\rNow the keys are our indices and the values remain values. But what we can also do is, to change the order of the indices.\nmyDict = { \u0026#39;A\u0026#39; : 10 , \u0026#39;B\u0026#39; : 20 , \u0026#39;C\u0026#39; : 30 }\rseries = pd.Series(myDict, index =[ \u0026#39;C\u0026#39; , \u0026#39;A\u0026#39; , \u0026#39;B\u0026#39; ])\rOur series now looks like this:\nprint(series)\r## C 30\r## A 10\r## B 20\r## dtype: int64\r\r\rPANDAS DataFrame\rDataFrame is the main thing on which we’ll be mostly working on. Most manipulation or operation on the data will be applied by means of DataFrame.\nCreating DataFrame using dictionary data\rThis is a simple process in which we just need to pass the json data to the DataFrame method.\ncars = {\u0026#39;Brand\u0026#39;:[\u0026#39;Honda\u0026#39;,\u0026#39;Toyota\u0026#39;,\u0026#39;Ford\u0026#39;,\u0026#39;Audi\u0026#39;],\u0026#39;Price\u0026#39;:[22000,21000,27000,35000]}\rdf = pd.DataFrame(cars)\rdf\r## Brand Price\r## 0 Honda 22000\r## 1 Toyota 21000\r## 2 Ford 27000\r## 3 Audi 35000\rdata = { \u0026#39;Name\u0026#39; : [ \u0026#39;Anna\u0026#39; , \u0026#39;Bob\u0026#39; , \u0026#39;Charles\u0026#39; ], \u0026#39;Age\u0026#39; : [ 24 , 32 , 35 ], \u0026#39;Height\u0026#39; : [ 176 , 187 , 175 ]}\rdf = pd.DataFrame(data)\rTo create a Pandas data frame, we use the constructor of the class. In this case, we first create a dictionary with some data about three persons. We feed that data into our data frame. It then looks like this:\ndf\r## Name Age Height\r## 0 Anna 24 176\r## 1 Bob 32 187\r## 2 Charles 35 175\rAs you can see, without any manual work, we already have a structured data frame and table.\nTo now access the values is a bit more complicated than with series. We have multiple columns and multiple rows, so we need to address two values.\nprint (df[ \u0026#39;Name\u0026#39; ][ 1 ])\r## Bob\rSo first we choose the column Name and then we choose the second element (index one) of this column. In this case, this is Bob .\nWhen we omit the last index, we can also select only the one column. This is useful when we want to save specific columns of our data frame into a new one. What we can also do in this case is to select multiple columns.\nprint (df[[ \u0026#39;Name\u0026#39; , \u0026#39;Height\u0026#39; ]])\r## Name Height\r## 0 Anna 176\r## 1 Bob 187\r## 2 Charles 175\r\rDATA FRAME FUNCTIONS\rFor data frames we have a couple of basic functions and attributes that we already know from lists or NumPy arrays.\n\r\rBASIC FUNCTIONS AND ATTRIBUTES\r\r\r\r\rdf.T\rTransposes the rows and columns of the data frame\r\rdf.dtypes\rReturns data types of the data frame\r\rdf.ndim\rReturns the number of dimensions of the data frame\r\rdf.shape\rReturns the shape of the data frame\r\rdf.size\rReturns the number of elements in the data frame\r\rdf.head(n)\rReturns the first n rows of the data frame (default is five)\r\rdf.tail(n)\rReturns the last n rows of the data frame (default is five)\r\r\r\r\rSTATISTICAL FUNCTIONS\rFor the statistical functions, we will now extend our data frame a little bit and add some more persons.\ndata = { \u0026#39;Name\u0026#39; : [ \u0026#39;Anna\u0026#39; , \u0026#39;Bob\u0026#39; , \u0026#39;Charles\u0026#39; ,\r\u0026#39;Daniel\u0026#39; , \u0026#39;Evan\u0026#39; , \u0026#39;Fiona\u0026#39; ,\r\u0026#39;Gerald\u0026#39; , \u0026#39;Henry\u0026#39; , \u0026#39;India\u0026#39; ],\r\u0026#39;Age\u0026#39; : [ 24 , 32 , 35 , 45 , 22 , 54 , 55 , 43 , 25 ],\r\u0026#39;Height\u0026#39; : [ 176 , 187 , 175 , 182 , 176 ,\r189 , 165 , 187 , 167 ]}\rdf = pd.DataFrame(data)\r\r\rSTATISTICAL FUNCTIONS\r\r\r\r\rFUNCTION\rDESCRIPTION\r\rcount()\rCount the number of non-null elements\r\rsum()\rReturns the sum of values of the selected columns\r\rmean()\rReturns the arithmetic mean of values of the selected columns\r\rmedian()\rReturns the median of values of the selected columns\r\rmode()\rReturns the value that occurs most often in the columns selected\r\rstd()\rReturns standard deviation of the values\r\rmin()\rReturns the minimum value\r\rmax()\rReturns the maximum value\r\rabs()\rReturns the absolute values of the elements\r\rprod()\rReturns the product of the selected elements\r\rdescribe()\rReturns data frame with all statistical values summarized\r\r\r\rNow, we are not going to dig deep into every single function here. But let’s take a look at how to apply some of them.\nprint (df[ \u0026#39;Age\u0026#39; ].mean())\r## 37.22222222222222\rprint (df[ \u0026#39;Height\u0026#39; ].median())\r## 176.0\rHere we choose a column and then apply the statistical functions on it. What we get is just a single scalar with the desired value.\nWe can also apply the functions to the whole data frame. In this case, we get returned another data frame with the results for each column.\nprint (df.mean())\r## Age 37.222222\r## Height 178.222222\r## dtype: float64\r\rAPPLYING NUMPY FUNCTIONS\rInstead of using the built-in Pandas functions, we can also use the methods we already know. For this, we just use the apply function of the data frame and then pass our desired method.\nimport numpy as np\rprint (df[ \u0026#39;Age\u0026#39; ].apply(np.sin))\r## 0 -0.905578\r## 1 0.551427\r## 2 -0.428183\r## 3 0.850904\r## 4 -0.008851\r## 5 -0.558789\r## 6 -0.999755\r## 7 -0.831775\r## 8 -0.132352\r## Name: Age, dtype: float64\rIn this example, we apply the sine function onto our ages. It doesn’t make any sense but it demonstrates how this works.\n\rLAMBDA EXPRESSIONS\rA very powerful in Python are lambda expression . They can be thought of as nameless functions that we pass as a parameter.\nprint (df[ \u0026#39;Age\u0026#39; ].apply( lambda x: x * 100 ))\r## 0 2400\r## 1 3200\r## 2 3500\r## 3 4500\r## 4 2200\r## 5 5400\r## 6 5500\r## 7 4300\r## 8 2500\r## Name: Age, dtype: int64\rBy using the keyword lambda we create a temporary variable that represents the individual values that we are applying the operation onto. After the colon, we define what we want to do. In this case, we multiply all values of the column Age by 100.\ndf = df[[ \u0026#39;Age\u0026#39; , \u0026#39;Height\u0026#39; ]]\rprint (df.apply( lambda x: x.max() - x.min()))\r## Age 33\r## Height 24\r## dtype: int64\rHere we removed the Name column, so that we only have numerical values. Since we are applying our expression on the whole data frame now, x refers to the whole columns. What we do here is calculating the difference between the maximum value and the minimum value.\n\rITERATING\rIterating over data frames is quite easy with Pandas. We can either do it in the classic way or use specific functions for it.\nfor x in df[ \u0026#39;Age\u0026#39; ]:\rprint (x)\r## 24\r## 32\r## 35\r## 45\r## 22\r## 54\r## 55\r## 43\r## 25\rAs you can see, iterating over a column’s value is very simple and nothing new. This would print all the ages. When we iterate over the whole data frame, our control variable takes on the column names.\n\r\rSTATISTICAL FUNCTIONS\r\r\r\r\riteritems()\rIterator for key-value pairs\r\riterrows()\rIterator for the rows (index, series)\r\ritertuples()\rIterator for the rows as named tuples\r\r\r\rLet’s take a look at some practical examples.\nfor key, value in df.iteritems():\rprint ( \u0026#39;{}: {}\u0026#39; .format(key, value))\r## Age: 0 24\r## 1 32\r## 2 35\r## 3 45\r## 4 22\r## 5 54\r## 6 55\r## 7 43\r## 8 25\r## Name: Age, dtype: int64\r## Height: 0 176\r## 1 187\r## 2 175\r## 3 182\r## 4 176\r## 5 189\r## 6 165\r## 7 187\r## 8 167\r## Name: Height, dtype: int64\rHere we use the iteritems function to iterate over key-value pairs. What we get is a huge output of all rows for each column.\nOn the other hand, when we use iterrows , we can print out all the column-values for each row or index.\nfor index, value in df.iterrows():\rprint (index,value)\r## 0 Age 24\r## Height 176\r## Name: 0, dtype: int64\r## 1 Age 32\r## Height 187\r## Name: 1, dtype: int64\r## 2 Age 35\r## Height 175\r## Name: 2, dtype: int64\r## 3 Age 45\r## Height 182\r## Name: 3, dtype: int64\r## 4 Age 22\r## Height 176\r## Name: 4, dtype: int64\r## 5 Age 54\r## Height 189\r## Name: 5, dtype: int64\r## 6 Age 55\r## Height 165\r## Name: 6, dtype: int64\r## 7 Age 43\r## Height 187\r## Name: 7, dtype: int64\r## 8 Age 25\r## Height 167\r## Name: 8, dtype: int64\r\rSORTING\rOne very powerful thing about Pandas data frames is that we can easily sort them.\n\rSORT BY INDEX\rdf = pd.DataFrame(np.random.rand( 10 , 2 ),\rindex =[ 1 , 5 , 3 , 6 , 7 , 2 , 8 , 9 , 0 , 4 ],\rcolumns =[ \u0026#39;A\u0026#39; , \u0026#39;B\u0026#39; ])\rHere we create a new data frame, which is filled with random numbers. We specify our own indices and as you can see, they are completely unordered.\nprint (df.sort_index())\r## A B\r## 0 0.542103 0.598631\r## 1 0.399766 0.495972\r## 2 0.980509 0.835775\r## 3 0.676856 0.993693\r## 4 0.397197 0.450716\r## 5 0.507255 0.553412\r## 6 0.775124 0.687753\r## 7 0.039972 0.772406\r## 8 0.060967 0.921264\r## 9 0.945282 0.797821\rBy using the method sort_index , we sort the whole data frame by the index column. The result is now sorted:\n\rINPLACE PARAMETER\rWhen we use functions that manipulate our data frame, we don’t actually change it but we return a manipulated copy. If we wanted to apply the changes on the actual data frame, we would need to do it like this:\ndf = df.sort_index()\rBut Pandas offers us another alternative as well. This alternative is the parameter inplace . When this parameter is set to True , the changes get applied to our actual data frame\ndf.sort_index( inplace = True )\r\rSORT BY COLUMNS\rNow, we can also sort our data frame by specific columns.\ndata = { \u0026#39;Name\u0026#39; : [ \u0026#39;Anna\u0026#39; , \u0026#39;Bob\u0026#39; , \u0026#39;Charles\u0026#39; ,\r\u0026#39;Daniel\u0026#39; , \u0026#39;Evan\u0026#39; , \u0026#39;Fiona\u0026#39; ,\r\u0026#39;Gerald\u0026#39; , \u0026#39;Henry\u0026#39; , \u0026#39;India\u0026#39; ],\r\u0026#39;Age\u0026#39; : [ 24 , 24 , 35 , 45 , 22 , 54 , 54 , 43 , 25 ],\r\u0026#39;Height\u0026#39; : [ 176 , 187 , 175 , 182 , 176 ,\r189 , 165 , 187 , 167 ]}\rdf = pd.DataFrame(data)\rdf.sort_values( by =[ \u0026#39;Age\u0026#39; , \u0026#39;Height\u0026#39; ], inplace = True )\rprint (df)\r## Name Age Height\r## 4 Evan 22 176\r## 0 Anna 24 176\r## 1 Bob 24 187\r## 8 India 25 167\r## 2 Charles 35 175\r## 7 Henry 43 187\r## 3 Daniel 45 182\r## 6 Gerald 54 165\r## 5 Fiona 54 189\rHere we have our old data frame slightly modified. We use the function sort_values to sort our data frames. The parameter by states the columns that we are sorting by. In this case, we are first sorting by age and if two persons have the same age, we sort by height.\n\rJOINING AND MERGING\rAnother powerful concept in Pandas is joining and merging data frames.\nnames = pd.DataFrame({\r\u0026#39;id\u0026#39; : [ 1 , 2 , 3 , 4 , 5 ],\r\u0026#39;name\u0026#39; : [ \u0026#39;Anna\u0026#39; , \u0026#39;Bob\u0026#39; , \u0026#39;Charles\u0026#39; ,\r\u0026#39;Daniel\u0026#39; , \u0026#39;Evan\u0026#39; ],\r})\rages = pd.DataFrame({\r\u0026#39;id\u0026#39; : [ 1 , 2 , 3 , 4 , 5 ],\r\u0026#39;age\u0026#39; : [ 20 , 30 , 40 , 50 , 60 ]\r})\rdf = pd.merge(names,ages, on = \u0026#39;id\u0026#39; )\rdf.set_index( \u0026#39;id\u0026#39; , inplace = True )\rFirst we use the method merge and specify the column to merge on. We then have a new data frame with the combined data but we also want our id column to be the index. For this, we use the set_index method.\rNow when we have two separate data frames which are related to one another, we can combine them into one data frame. It is important that we have a common column that we can merge on. In this case, this is id .\n\rJOINS\rIt is not necessarily always obvious how we want to merge our data frames. This is where joins come into play. We have four types of joins.\n\r\rJOIN MERGE TYPES\r\r\r\r\rleft\rUses all keys from left object and merges with right\r\rright\rUses all keys from right object and merges with left\r\router\rUses all keys from both objects and merges them\r\rinner\rUses only the keys which both objects have and merges them\r\r\r(default)\r\r\r\rNow let’s change our two data frames a little bit.\nnames = pd.DataFrame({\r\u0026#39;id\u0026#39; : [ 1 , 2 , 3 , 4 , 5 , 6 ],\r\u0026#39;name\u0026#39; : [ \u0026#39;Anna\u0026#39; , \u0026#39;Bob\u0026#39; , \u0026#39;Charles\u0026#39; ,\r\u0026#39;Daniel\u0026#39; , \u0026#39;Evan\u0026#39; , \u0026#39;Fiona\u0026#39; ],\r})\rages = pd.DataFrame({\r\u0026#39;id\u0026#39; : [ 1 , 2 , 3 , 4 , 5 , 7 ],\r\u0026#39;age\u0026#39; : [ 20 , 30 , 40 , 50 , 60 , 70 ],\r\u0026#39;Height\u0026#39; : [ 176 , 187 , 175 , 182 , 176 ,\r189 ]\r})\rOur names frame now has an additional index 6 and an additional name. And our ages frame has an additional index 7 with an additional name.\ndf = pd.merge(names,ages, on = \u0026#39;id\u0026#39; , how = \u0026#39;inner\u0026#39; )\rdf.set_index( \u0026#39;id\u0026#39; , inplace = True )\rIf we now perform the default inner join , we will end up with the same data frame as in the beginning. We only take the keys which both objects have. This means one to five.\ndf = pd.merge(names,ages, on = \u0026#39;id\u0026#39; , how = \u0026#39;left\u0026#39; )\rdf.set_index( \u0026#39;id\u0026#39; , inplace = True )\rWhen we use the left join , we get all the keys from the names data frame but not the additional index 7 from ages. This also means that Fiona won’t be assigned any age.\nThe same principle goes for the right join just the other way around\ndf = pd.merge(names,ages, on = \u0026#39;id\u0026#39; , how = \u0026#39;right\u0026#39; )\rdf.set_index( \u0026#39;id\u0026#39; , inplace = True )\rNow, we only have the keys from the ages frame and the 6 is missing. Finally, if we use the outer join , we combine all keys into one data frame.\ndf = pd.merge(names,ages, on = \u0026#39;id\u0026#39; , how = \u0026#39;outer\u0026#39; )\rdf.set_index( \u0026#39;id\u0026#39; , inplace = True )\r\rQUERYING DATA\rLike in databases with SQL, we can also query data from our data frames in Pandas. For this, we use the function loc , in which we put our expression.\nprint (df.loc[df[ \u0026#39;age\u0026#39; ] == 24 ])\r## Empty DataFrame\r## Columns: [name, age, Height]\r## Index: []\rprint (df.loc[(df[ \u0026#39;age\u0026#39; ] == 24 ) \u0026amp;\r(df[ \u0026#39;Height\u0026#39; ] \u0026gt; 180 )])\r## Empty DataFrame\r## Columns: [name, age, Height]\r## Index: []\rprint (df.loc[df[ \u0026#39;age\u0026#39; ] \u0026gt; 30 ][ \u0026#39;name\u0026#39; ])\r## id\r## 3 Charles\r## 4 Daniel\r## 5 Evan\r## 7 NaN\r## Name: name, dtype: object\rHere we have some good examples to explain how this works. The first expression returns all rows where the value for Age is 24.\nThe second query is a bit more complicated. Here we combine two conditions. The first one is that the age needs to be 24 but we then combine this with the condition that the height is greater than 180. This leaves us with one row.\nIn the last expression, we can see that we are only choosing one column to be returned. We want the names of all people that are older than 30.\n\rREAD DATA FROM FILES\rSimilar to NumPy, we can also easily read data from external files into Pandas. Let’s say we have an CSV-File like this (opened in Excel):\nThe only thing that we need to do now is to use the function read_csv to import our data into a data frame.\ndf = pd.read_csv( \u0026#39;data.csv\u0026#39; )\rdf.set_index( \u0026#39;id\u0026#39; , inplace = True )\rprint (df)\rWe also set the index to the id column again. This is what we have imported:\nThis of course, also works the other way around. By using the method to_csv , we can also save our data frame into a CSV-file.\ndata = { \u0026#39;Name\u0026#39; : [ \u0026#39;Anna\u0026#39; , \u0026#39;Bob\u0026#39; , \u0026#39;Charles\u0026#39; ,\r\u0026#39;Daniel\u0026#39; , \u0026#39;Evan\u0026#39; , \u0026#39;Fiona\u0026#39; ,\r\u0026#39;Gerald\u0026#39; , \u0026#39;Henry\u0026#39; , \u0026#39;India\u0026#39; ],\r\u0026#39;Age\u0026#39; : [ 24 , 24 , 35 , 45 , 22 , 54 , 54 , 43 , 25 ],\r\u0026#39;Height\u0026#39; : [ 176 , 187 , 175 , 182 , 176 ,\r189 , 165 , 187 , 167 ]}\rdf = pd.DataFrame(data)\rdf.to_csv( \u0026#39;mydf.csv\u0026#39; )\r\rPLOTTING DATA\rSince Pandas builds on Matplotlib, we can easily visualize the data from our data frame.\ndata = { \u0026#39;Name\u0026#39; : [ \u0026#39;Anna\u0026#39; , \u0026#39;Bob\u0026#39; , \u0026#39;Charles\u0026#39; ,\r\u0026#39;Daniel\u0026#39; , \u0026#39;Evan\u0026#39; , \u0026#39;Fiona\u0026#39; ,\r\u0026#39;Gerald\u0026#39; , \u0026#39;Henry\u0026#39; , \u0026#39;India\u0026#39; ],\r\u0026#39;Age\u0026#39; : [ 24 , 24 , 35 , 45 , 22 , 54 , 54 , 43 , 25 ],\r\u0026#39;Height\u0026#39; : [ 176 , 187 , 175 , 182 , 176 ,\r189 , 165 , 187 , 167 ]}\rdf = pd.DataFrame(data)\rdf.sort_values( by =[ \u0026#39;Age\u0026#39; , \u0026#39;Height\u0026#39; ])\rdf.hist()\rplt.show()\rIn this example, we use the method hist to plot a histogram of our numerical columns. Without specifying anything more, this is what we end up with:\nBut we can also just use the function plot to plot our data frame or individual columns.\ndf.plot()\rplt.show()\rThe result is the following:\n\r\r\r",
    "ref": "/blog/pandas/"
  },{
    "title": "Python - Matplotlib- Plot Types",
    "date": "",
    "description": "Matplotlib- Plot Types",
    "body": "\r\r\rMATPLOTLIB PLOT TYPES\r\r\rMATPLOTLIB PLOT TYPES\rMatplotlib offers a huge arsenal of different plot types. Here we are going to take a look at these.\nHISTOGRAMS\rLet’s start out with some statistics here. So-called histograms represent the distribution of numerical values. For example, we could graph the distribution of heights amongst students in a class.\nmu, sigma = 172 , 4\rx = mu + sigma * np.random.randn( 10000 ) \rWe start by defining a mean value mu (average height) and a standard deviation sigma . To create our x-values, we use our mu and sigma combined with 10000 randomly generated values. Notice that we are using the randn function here. This function generates values for a standard normal distribution , which means that we will get a bell curve of values.\nplt.hist(x, 100 , density = True , facecolor = \u0026#39;blue\u0026#39; ) \rThen we use the hist function, in order to plot our histogram. The second parameter states how many values we want to plot. Also, we want our values to be normed. So we set the parameter density to True . This means that our y-values will sum up to one and we can view them as percentages. Last but not least, we set the color to blue.\nNow, when we show this plot, we will realize that it is a bit confusing. So we are going to add some labeling here.\nplt.xlabel( \u0026#39;Height\u0026#39; )\rplt.ylabel( \u0026#39;Probability\u0026#39; )\rplt.title( \u0026#39;Height of Students\u0026#39; )\rplt.text( 160 , 0.125 , \u0026#39;µ = 172, σ = 4\u0026#39; )\rplt.axis([ 155 , 190 , 0 , 0.15 ])\rplt.grid( True )\rFirst we label the two axes. The x-values represent the height of the students, whereas the y-values represent the probability that a randomly picked student has the respective height. Besides the title, we also add some text to our graph. We place it at the x-value 160 and the y-value of 0.125. The text just states the values for µ (mu) and σ (sigma).\nLast but not least, we set the ranges for the two axes. Our x-values range from 155 to 190 and our y-values from 0 to 0.15. Also, the grid is turned on. This is what our graph looks like at the end:\nWe can see the Gaussian bell curve which is typical for the standard normal distribution.\n\rBAR CHART\rFor visualizing certain statistics, bar charts are oftentimes very useful, especially when it comes to categories. In our case, we are going to plot the skill levels of three different people in the IT realm.\nbob = ( 90 , 67 , 87 , 76 )\rcharles = ( 80 , 80 , 47 , 66 )\rdaniel = ( 40 , 95 , 76 , 89 )\rskills = ( \u0026#39;Python\u0026#39; , \u0026#39;Java\u0026#39; , \u0026#39;Networking\u0026#39; , \u0026#39;Machine Learning\u0026#39; )\rHere we have the three persons Bob, Charles and Daniel . They are represented by tuples with four values that indicate their skill levels in Python programming, Java programming, networking and machine learning.\nwidth = 0.2\rindex = np.arange( 4 )\rplt.bar(index, bob,\rwidth =width, label = \u0026#39;Bob\u0026#39; )\rplt.bar(index + width, charles,\rwidth =width, label = \u0026#39;Charles\u0026#39; )\rplt.bar(index + width * 2 , daniel,width =width, label = \u0026#39;Daniel\u0026#39; )\rWe then use the bar function to plot our bar chart. For this, we define an array with the indices one to four and a bar width of 0.2. For each person we plot the four respective values and label them.\nplt.xticks(index + width, skills)\rplt.ylim( 0 , 120 )\rplt.title( \u0026#39;IT Skill Levels\u0026#39; )\rplt.ylabel( \u0026#39;Skill Level\u0026#39; )\rplt.xlabel( \u0026#39;IT Skill\u0026#39; )\rplt.legend()\rThen we label the x-ticks with the method xticks and set the limit of the y-axis to 120 to free up some space for our legend. After that we set a title and label the axes. The result looks like this:\nWe can now see who is the most skilled in each category. Of course we could also change the graph so that we have the persons on the x-axis with the skill-colors in the legend.\n\rPIE CHART\rPie charts are used to display proportions of numbers. For example, we could graph how many percent of the students have which nationality.\nlabels = ( \u0026#39;American\u0026#39; , \u0026#39;German\u0026#39; , \u0026#39;French\u0026#39; , \u0026#39;Other\u0026#39; )\rvalues = ( 47 , 23 , 20 , 10 ) \rWe have one tuple with our four nationalities. They will be our labels. And we also have one tuple with the percentages.\nplt.pie(values, labels =labels,\rautopct = \u0026#39;%.2f%%\u0026#39; , shadow = True )\rplt.title( \u0026#39;Student Nationalities\u0026#39; )\rplt.show()\rNow we just need to use the pie function, to draw our chart. We pass our values and our labels. Then we set the autopct parameter to our desired percentage format. Also, we turn on the shadow of the chart and set a title. And this is what we end up with:\nAs you can see, this chart is perfect for visualizing percentages.\n\rSCATTER PLOTS\rSo-called scatter plots are used to represent two-dimensional data using dots.\nx = np.random.rand( 50 )\ry = np.random.rand( 50 )\rplt.scatter(x,y)\rplt.show()\rHere we just generate 50 random x-values and 50 random y-values. By using the scatter function, we can then plot them.\n\rBOXPLOT\rBoxplot diagrams are used, in order to split data into quartiles . We do that to get information about the distribution of our values. The question we want to answer is: How widely spread is the data in each of the quartiles.\nmu, sigma = 172 , 4\rvalues = np.random.normal(mu,sigma, 200 )\rplt.boxplot(values)\rplt.title( \u0026#39;Student\u0026#39;s Height\u0026#39; )\rplt.ylabel( \u0026#39;Height\u0026#39; )\rplt.show()\rIn this example, we again create a normal distribution of the heights of our students. Our mean value is 172, our standard deviation 4 and we generate 200 values. Then we plot our boxplot diagram.\nHere we see the result. Notice that a boxplot doesn’t give information about the frequency of the individual values. It only gives information about the spread of the values in the individual quartiles. Every quartile has 25% of the values but some have a very small spread whereas others have quite a large one.\n\r3D PLOTS\rNow last but not least, let’s take a look at 3D-plotting. For this, we will need to import another plotting module. It is called mpl_toolkits and it is part of the Matplotlib stack.\nfrom mpl_toolkits import mplot3d\rSpecifically, we import the module mplot3d from this library. Then, we can use 3d as a parameter when defining our axes.\nax = plt.axes( projection = \u0026#39;3d\u0026#39; )\rplt.show()\rWe can only use this parameter, when mplot3d is imported. Now, our plot looks like this:\nSince we are now plotting in three dimensions, we will also need to define three axes.\nz = np.linspace( 0 , 20 , 100 )\rx = np.sin(z)\ry = np.cos(z)\rax = plt.axes( projection = \u0026#39;3d\u0026#39; )\rax.plot3D(x,y,z)\rplt.show()\rIn this case, we are taking the z-axis as the input. The z-axis is the one which goes upwards. We define the x-axis and the y-axis to be a sine and cosine function. Then, we use the function plot3D to plot our function. We end up with this:\n\rSURFACE PLOTS\rNow in order to plot a function with a surface, we need to calculate every point on it. This is impossible, which is why we are just going to calculate enough to estimate the graph. In this case, x and y will be the input and the z-function will be the 3D-result which is composed of them.\nax = plt.axes( projection = \u0026#39;3d\u0026#39; )\rdef z_function(x, y):\rreturn np.sin(np.sqrt(x ** 2 + y ** 2 ))\rx = np.linspace(- 5 , 5 , 50 )\ry = np.linspace(- 5 , 5 , 50 )\rWe start by defining a z_function which is a combination of sine, square root and squaring the input. Our inputs are just 50 numbers from -5 to 5.\nX, Y = np.meshgrid(x,y)\rZ = z_function(X,Y)\rax.plot_surface(X,Y,Z)\rplt.show()\rThen we define new variables for x and y (we are using capitals this time). What we do is converting the x- and y-vectors into matrices using the meshgrid function. Finally, we use the z_function to calculate our z-values and then we plot our surface by using the method plot_surface\n\r\r",
    "ref": "/blog/numpy/"
  },{
    "title": "Python - Matplotlib",
    "date": "",
    "description": "Introduction to Matplotlib",
    "body": "\r\r\rMatplotlib\r\rPLOTTING MATHEMATICAL FUNCTIONS\rVISUALIZING VALUES\rMULTIPLE GRAPHS\rSUBPLOTS\rMULTIPLE PLOTTING WINDOWS\rPLOTTING STYLES\rLABELING DIAGRAMS\rSETTING TITLES\rLABELING AXES\rLEGENDS\rSAVING DIAGRAMS\r\r\r\rMatplotlib\rVisualizing our data is crucial for data science. It gives us an overview and helps us to analyze data and make conclusions. Matplotlib is the library which we use for plotting and visualizing.\nPLOTTING MATHEMATICAL FUNCTIONS\rNow first we will be drawing some mathematical functions.\nWe need importing the matplotlib.pyplot module and also NumPy.\nimport numpy as np\rimport matplotlib.pyplot as plt\rWe are also using alias for pyplot. In this case, it is plt .\rIn order to plot a function, we need the x-values or the input and the y-values or the output.\rSo first let us generate the x-values.\nx_values = np.linspace( 0 , 18 , 100 )\rWe wll be doing this by using the already known linspace function. Here we create an array with 100 values between 0 and 18. To now get our y-values, we just need to apply the respective function on our\nx-values. For this example, we are going with the sine function.\ny_values = np.sin(x_values)\rRemember that the function gets applied to every single item of the input array. So in this case, we have an array with the sine value of every element of the x-values array. We just need to plot them now.\nplt.plot(x_values, y_values)\rplt.show()\rWe do this by using the function plot and passing our x-values and y-values. At the end we call the show function, to display our plot.\nThat was very simple. Now, we can go ahead and define our own function that we want to plot.\nx = np.linspace( 0 , 10 , 100 )\ry = ( 6 * x - 30 ) ** 2\rplt.plot(x, y)\rplt.show()\rThe result looks like this:\nThis function (6x – 30)² is plotted with Matplotlib.\n\rVISUALIZING VALUES\rWhat we can also do, instead of plotting functions, is just visualizing values in form of single dots for example.\nnumbers = 10 * np.random.random( 100 )\rplt.plot(numbers, \u0026#39;bo\u0026#39; )\rplt.show()\rHere we generate 100 random numbers from 0 to 10. We then plot these numbers as blue dots. This is defined by the second parameter ‘bo’ , where the first letter indicates the color (blue) and the second one the shape (dots).\n\rMULTIPLE GRAPHS\rWe can plot multiple functions in different color and shape.\nx = np.linspace( 0 , 5 , 200 )\ry1 = 2 * x\ry2 = x ** 2\ry3 = np.log(x)\rplt.plot(x, y1)\rplt.plot(x, y2)\rplt.plot(x, y3)\rplt.show()\rIn this example, we first generate 200 x-values from 0 to 5. Then we define three different functions y1, y2 and y3 . We plot all these and view the plotting window. This is what it looks like:\n\rSUBPLOTS\rNow, sometimes we want to draw multiple graphs but we don’t want them in the same plot necessarily. For this reason, we have so-called subplots . These are plots that are shown in the same window but independently from each other.\nx = np.linspace( 0 , 5 , 200 )\ry1 = np.sin(x)\ry2 = np.sqrt(x)\rplt.subplot( 211 )\rplt.plot(x, y1, \u0026#39;r-\u0026#39; )\rplt.subplot( 212 )\rplt.plot(x, y2, \u0026#39;g--\u0026#39; )\rplt.show()\rBy using the function subplot we state that everything we plot now belongs to this specific subplot. The parameter we pass defines the grid of our window. The first digit indicates the number of rows, the second the number of columns and the last one the index of the subplot. So in this case, we have two rows and one column. Index one means that the respective subplot will be at the top.\nAs you can see, we have two subplots in one window and both have a different color and shape. Notice that the ratios between the x-axis and the y-axis differ in the two plots.\n\rMULTIPLE PLOTTING WINDOWS\rInstead of plotting into subplots, we can also go ahead and plot our graphs into multiple windows. In Matplotlib we call these figures .\nplt.figure( 1 )\rplt.plot(x, y1, \u0026#39;r-\u0026#39; )\rplt.figure( 2 )\rplt.plot(x, y2, \u0026#39;g--\u0026#39; )\rBy doing this, we can show two windows with their graphs at the same time. Also, we can use subplots within figures.\n\rPLOTTING STYLES\rIn order to use a style, we need to import the style module of Matplotlib and then call the function use .\nfrom matplotlib import style\rstyle.use( \u0026#39;ggplot\u0026#39; )\rBy using the from … import … notation we don’t need to specify the parent module matplotlib . Here we apply the style of ggplot . This adds a grid and some other design changes to our plots. For more information, check out the link above.\n\rLABELING DIAGRAMS\rIn order to make our graphs understandable, we need to label them properly. We should label the axes, we should give our windows titles and in some cases we should also add a legend.\n\rSETTING TITLES\rLet’s start out by setting the titles of our graphs and windows.\nx = np.linspace( 0 , 50 , 100 )\ry = np.sin(x)\rplt.title( \u0026#39;Sine Function\u0026#39; )\rplt.suptitle( \u0026#39;Data Science\u0026#39; )\rplt.grid( True )\rplt.plot(x,y)\rplt.show()\rIn this example, we used the two functions title and suptitle . The first function adds a simple title to our plot and the second one adds an additional centered title above it. Also, we used the grid function, to turn on the grid of our plot.\nIf you want to change the title of the window, you can use the figure function that we already know.\nplt.figure( \u0026#39;MyFigure\u0026#39; )\r\rLABELING AXES\rAs a next step, we are going to label our axes. For this, we use the two functions xlabel and ylabel .\nplt.xlabel( \u0026#39;x-values\u0026#39; )\rplt.ylabel( \u0026#39;y-values\u0026#39; )\rYou can choose whatever labels you like. When we combine all these pieces of code, we end up with a graph like this:\nIn this case, the labels aren’t really necessary because it is obvious what we see here. But sometimes we want to describe what our values actually mean and what the plot is about.\n\rLEGENDS\rSometimes we will have multiple graphs and objects in a plot. We then use legends to label these individual elements, in order to make everything more readable.\nx = np.linspace( 10 , 50 , 100 )\ry1 = np.sin(x)\ry2 = np.cos(x)\ry3 = np.log(x/ 3 )\rplt.plot(x,y1, \u0026#39;b-\u0026#39; , label = \u0026#39;Sine\u0026#39; )\rplt.plot(x,y2, \u0026#39;r-\u0026#39; , label = \u0026#39;Cosine\u0026#39; )\rplt.plot(x,y3, \u0026#39;g-\u0026#39; , label = \u0026#39;Logarithm\u0026#39; )\rplt.legend( loc = \u0026#39;upper left\u0026#39; )\rplt.show()\rHere we have three functions, sine , cosine and a logarithmic function. We draw all graphs into one plot and add a label to them. In order to make these labels visible, we then use the function legend and specify a location for it. Here we chose the upper left . Our result looks like this:\n\rSAVING DIAGRAMS\rSo now that we know quite a lot about plotting and graphing, let’s take a look at how to save our diagrams.\nplt.savefig( \u0026#39;functions.png\u0026#39; )\rActually, this is quite simple. We just plot whatever we want to plot and then use the function savefig to save our figure into an image file.\n\r\r",
    "ref": "/blog/matplotlib/"
  },{
    "title": "Python - Numpy",
    "date": "",
    "description": "Introduction to Numpy",
    "body": "\r\r\rNUMPY ARRAYS\r\rCREATING ARRAYS\rMULTI-DIMENSIONAL ARRAYS\rFILLING ARRAYS\rFULL FUNCTION\rZEROS AND ONES\rEMPTY AND RANDOM\rRANGES\rNOT A NUMBER (NAN)\rATTRIBUTES OF ARRAYS\rMATHEMATICAL OPERATIONS\rARITHMETIC OPERATIONS\rMATHEMATICAL FUNCTIONS\rAGGREGATE FUNCTIONS\rMANIPULATING ARRAYS\r\rSHAPE MANIPULATION FUNCTIONS\r\rJOINING FUNCTIONS\r\rSPLITTING FUNCTIONS\r\rADDING AND REMOVING\rLOADING AND SAVING ARRAYS\rNUMPY FORMAT\rCSV FORMAT\r\r\r\rNUMPY ARRAYS\rWe can’t do a lot of data science with NumPy alone. But it provides the basis for all the high-level libraries or modules for data science. It is essential for the efficient management of arrays and linear algebra.\nIn order to use NumPy, we of course have to import the respective module first.\nimport numpy as np\rAs you can see, we are also defining an alias here, so that we can address NumPy by just writing np .\nCREATING ARRAYS\rTo create a NumPy array, we just use the respective function array and pass a list to it.\na = np.array([ 10 , 20 , 30 ])\rb = np.array([ 1 , 77 , 2 , 3 ])\rNow we can access the values in the same way as we would do it with a list.\nprint (a[ 0 ])\r## 10\rprint (b[ 2 ])\r## 2\r\rMULTI-DIMENSIONAL ARRAYS\rThe arrays we created are one-dimensional arrays. With NumPy, we can create large multi-dimensional arrays that have the same structure as a matrix.\na = np.array([\r[ 10 , 20 , 30 ],\r[ 40 , 50 , 60 ]\r])\rprint (a)\r## [[10 20 30]\r## [40 50 60]]\rHere, we pass two lists within a list as a parameter. This creates a 2x3 matrix. When we print the array, we get the following result:\n[[10 20 30]\r[40 50 60]]\nSince we now have two dimensions, we also need to address two indices, in order to access a specific element.\nprint (a[ 1 ][ 2 ])\r## 60\rIn this case, we are addressing the second row (index one) and the third element or column (index two). Therefore, our result is 60 .\rWe can extend this principle as much as we want. For example, let’s create a much bigger array.\na = np.array([\r[\r[ 10 , 20 , 30 , 40 ], [ 8 , 8 , 2 , 1 ], [ 1 , 1 , 1 , 2 ]\r],\r[\r[ 9 , 9 , 2 , 39 ], [ 1 , 2 , 3 , 3 ], [ 0 , 0 , 3 , 2 ]\r],\r[\r[ 12 , 33 , 22 , 1 ], [ 22 , 1 , 22 , 2 ], [ 0 , 2 , 3 , 1 ]\r]\r], dtype = float )\rHere we have a 3x3x4 matrix and slowly but surely it becomes a bit irritating and we can’t really grasp the structure of the array. This is especially the case when we get into four or more dimensions, since we only perceive three dimensions in everyday life.\nYou can imagine this three-dimensional array as a cube. We have three rows, four columns and three pages or layers. Such visualizations fail in higher dimensions.\nAnother thing that is worth mentioning is the parameter dtype . It stands for data type and allows us to specify which data type our values have. In this case we specified float and therefore our values will be stored as floating point numbers with the respective notation.\n\rFILLING ARRAYS\rInstead of manually filling our arrays with values, we can also use pre-defined functions in certain cases. The only thing we need to specify is the desired function and the shape of the array.\n\rFULL FUNCTION\rBy using the full function for example, we fill an array of a certain shape with the same number. In this case we create a 3x5x4 matrix, which is filled with sevens.\na = np.full(( 3 , 5 , 4 ), 7 )\rprint (a)\r## [[[7 7 7 7]\r## [7 7 7 7]\r## [7 7 7 7]\r## [7 7 7 7]\r## [7 7 7 7]]\r## ## [[7 7 7 7]\r## [7 7 7 7]\r## [7 7 7 7]\r## [7 7 7 7]\r## [7 7 7 7]]\r## ## [[7 7 7 7]\r## [7 7 7 7]\r## [7 7 7 7]\r## [7 7 7 7]\r## [7 7 7 7]]]\r\rZEROS AND ONES\rFor the cases that we want arrays full of zeros or ones, we even have specific functions.\na = np.zeros(( 3 , 3 ))\rb = np.ones(( 2 , 3 , 4 , 2 ))\rHere we create a 3x3 array full of zeros and a four-dimensional array full of ones.\n\rEMPTY AND RANDOM\rOther options would be to create an empty array or one that is filled with random numbers. For this, we use the respective functions once again.\na = np.empty(( 4 , 4 ))\rb = np.random.random(( 2 , 3 ))\rThe function empty creates an array without initializing the values at all. This makes it a little bit faster but also more dangerous to use, since the user needs to manually initialize all the values.\nWhen using the random function, make sure that you are referring to the module np.random . You need to write it two times because otherwise you are calling the library.\n\rRANGES\rInstead of just filling arrays with the same values, we can fill create sequences of values by specifying the boundaries. For this, we can use two different functions, namely arange and linspace .\na = np.arange( 10 , 50 , 5 )\rThe function arange creates a list with values that range from the minimum to the maximum. The step-size has to be specified in the parameters.\n[10 15 20 25 30 35 40 45]\nIn this example, we create have count from 10 to 45 by always adding 5. The result can be seen above.\nBy using linspace we also create a list from a minimum value to a maximum value. But instead of specifying the step-size, we specify the amount of values that we want to have in our list. They will all be spread evenly and have the same distance to their neighbors.\nb = np.linspace( 0 , 100 , 11 )\rHere, we want to create a list that ranges from 0 to 100 and contains 11 elements. This fits smoothly with a difference of 10 between all numbers. So the result looks like this:\n[ 0. 10. 20. 30. 40. 50. 60. 70. 80. 90. 100.]\nOf course, if we choose different parameters, the numbers don’t be that “beautiful”.\n\rNOT A NUMBER (NAN)\rThere is a special value in NumPy that represents values that are not numbers. It is called NaN and stands for Not a Number . We basically just use it as a placeholder for empty spaces. It can be seen as a value that indicates that something is missing at that place.\nWhen importing big data packets into our application, there will sometimes be missing data. Instead of just setting these values to zero or something else, we can set them to NaN and then filter these data sets out.\n\rATTRIBUTES OF ARRAYS\rNumPy arrays have certain attributes that we can access and that provide information about the structure of it.\n\r\rUMPY ARRAY ATTRIBUTES\r\r\r\r\ra.shape\rReturns the shape of the array\r\r\re.g. (3,3) or (3,4,7)\r\ra.ndim\rReturns how many dimensions our array has\r\ra.size\rReturns the amount of elements an array has\r\ra.dtype\rReturns the data type of the values in the array\r\r\r\r\rMATHEMATICAL OPERATIONS\rNow that we know how to create an array and what attributes it has, let’s take a look at how to work with arrays. For this, we will start out with basic mathematical operations.\n\rARITHMETIC OPERATIONS\ra = np.array([\r[ 1 , 4 , 2 ],\r[ 8 , 8 , 2 ]\r])\rprint (a + 2 )\r## [[ 3 6 4]\r## [10 10 4]]\rprint (a - 2 )\r## [[-1 2 0]\r## [ 6 6 0]]\rprint (a * 2 )\r## [[ 2 8 4]\r## [16 16 4]]\rprint (a / 2 )\r## [[0.5 2. 1. ]\r## [4. 4. 1. ]]\rWhen we perform basic arithmetic operations like addition, subtraction, multiplication and division to an array and a scalar, we apply the operation on every single element in the array. Let’s take a look at the results:\n[[ 3 6 4]\r[10 10 4]]\r[[-1 2 0]\r[ 6 6 0]]\r[[ 2 8 4]\r[16 16 4]]\r[[0.5 2. 1. ]\r[4. 4. 1. ]]\nAs you can see, when we multiply the array by two, we multiply every single value in it by two. This is also the case for addition, subtraction and division. But what happens when we apply these operations on two arrays?\na = np.array([\r[ 1 , 4 , 2 ],\r[ 8 , 8 , 2 ]\r])\rb = np.array([\r[ 1 , 2 , 3 ]\r])\rc = np.array([\r[ 1 ],\r[ 2 ]\r])\rd = np.array([\r[ 1 , 2 , 3 ],\r[ 3 , 2 , 1 ]\r]) \rIn order to apply these operations on two arrays, we need to take care of the shapes. They don’t have to be the same, but there has to be a reasonable way of performing the operations. We then again apply the operations on each element of the array.\nFor example, look at a and b . They have different shapes but when we add these two, they share at least the amount of columns.\nprint (a+b)\r## [[ 2 6 5]\r## [ 9 10 5]]\r[[ 2 6 5]\r[ 9 10 5]]\nSince they match the columns, we can just say that we add the individual columns, even if the amount of rows differs.\rThe same can also be done with a and c where the rows match and the columns differ.\nprint (a+c)\r## [[ 2 5 3]\r## [10 10 4]]\r[[ 2 5 3]\r[10 10 4]]\nAnd of course it also works, when the shapes match exactly. The only problem is when the shapes differ too much and there is no reasonable way of performing the operations. In these cases, we get ValueErrors .\n\rMATHEMATICAL FUNCTIONS\rAnother thing that the NumPy module offers us is mathematical functions that we can apply to each value in an array.\n\r\rNUMPY MATHEMATICAL FUNCTIONS\r\r\r\r\rnp.exp(a)\rTakes e to the power of each value\r\rnp.sin(a)\rReturns the sine of each value\r\rnp.cos(a)\rReturns the cosine of each value\r\rnp.tan(a)\rReturns the tangent of each value\r\rnp.log(a)\rReturns the logarithm of each value\r\rnp.sqrt(a)\rReturns the square root of each value\r\r\r\r\rAGGREGATE FUNCTIONS\rNow we are getting into the statistics. NumPy offers us some so-called aggregate functions that we can use in order to get a key statistic from all of our values.\n\r\rNUMPY AGGREGATE FUNCTIONS\r\r\r\r\ra.sum()\rReturns the sum of all values in the array\r\ra.min()\rReturns the lowest value of the array\r\ra.max()\rReturns the highest value of the array\r\ra.mean()\rReturns the arithmetic mean of all values in the array\r\rnp.median(a)\rReturns the median value of the array\r\rnp.std(a)\rReturns the standard deviation of the values in the array\r\r\r\r\rMANIPULATING ARRAYS\rNumPy offers us numerous ways in which we can manipulate the data of our arrays. Here, we are going to take a quick look at the most important functions and categories of functions.\rIf you just want to change a single value however, you can just use the basic indexing of lists.\na = np.array([\r[ 4 , 2 , 9 ],\r[ 8 , 3 , 2 ]\r])\ra[ 1 ][ 2 ] = 7 \rSHAPE MANIPULATION FUNCTIONS\rOne of the most important and helpful types of functions are the shape manipulating functions . These allow us to restructure our arrays without changing their values.\n\r\r\r\rSHAPE MANIPULATION FUNCTIONS\r\r\r\r\ra.reshape(x,y)\rReturns an array with the same values structured in a different shape\r\ra.flatten()\rReturns a flattened one-dimensional copy of the array\r\ra.ravel()\rDoes the same as flatten but works with the actual array instead of a copy\r\ra.transpose()\rReturns an array with the same values but swapped dimensions\r\ra.swapaxes()\rReturns an array with the same values but two swapped axes\r\ra.flat\rNot a function but an iterator for the flattened version of the array\r\r\r\rThere is one more element that is related to shape but it’s not a function. It is called flat and it is an iterator for the flattened one-dimensional version of the array. Flat is not callable but we can iterate over it with for loops or index it.\nfor x in a.flat:\rprint (x)\r## 4\r## 2\r## 9\r## 8\r## 3\r## 7\rprint (a.flat[ 5 ])\r## 7\r\r\rJOINING FUNCTIONS\rWe use joining functions when we combine multiple arrays into one new array.\n\r\rJOINING FUNCTIONS\r\r\r\r\rFUNCTION\rDESCRIPTION\r\rnp.concatenate(a,b)\rJoins multiple arrays along an existing axis\r\rnp.stack(a,b)\rJoins multiple arrays along a new axis\r\rnp.hstack(a,b)\rStacks the arrays horizontally (column-wise)\r\rnp.vstack(a,b)\rStacks the arrays vertically\r\r\r(row-wise)\r\r\r\rIn the following, you can see the difference between concatenate and stack :\na = np.array([ 10 , 20 , 30 ])\rb = np.array([ 20 , 20 , 10 ])\rprint (np.concatenate((a,b)))\r## [10 20 30 20 20 10]\rprint (np.stack((a,b)))\r## [[10 20 30]\r## [20 20 10]]\r[10 20 30 20 20 10]\r[[10 20 30]\r[20 20 10]]\nWhat concatenate does is, it joins the arrays together by just appending one onto the other. Stack on the other hand, creates an additional axis that separates the two initial arrays.\nSPLITTING FUNCTIONS\rWe can not only join and combine arrays but also split them again. This is done by using splitting functions that split arrays into multiple sub-arrays.\n\r\r\r\rSPLITTING FUNCTIONS\r\r\r\r\rnp.split(a, x)\rSplits one array into multiple arrays\r\rnp.hsplit(a, x)\rSplits one array into multiple arrays horizontally (column-wise)\r\rnp.vsplit(a, x)\rSplits one array into multiple arrays vertically (row-wise)\r\r\r\rWhen splitting a list with the split function, we need to specify into how many sections we want to split our array.\na = np.array([\r[ 10 , 20 , 30 ],\r[ 40 , 50 , 60 ],\r[ 70 , 80 , 90 ],\r[ 100 , 110 , 120 ]\r])\rprint (np.split(a, 2 ))\r## [array([[10, 20, 30],\r## [40, 50, 60]]), array([[ 70, 80, 90],\r## [100, 110, 120]])]\rprint (np.split(a, 4 ))\r## [array([[10, 20, 30]]), array([[40, 50, 60]]), array([[70, 80, 90]]), array([[100, 110, 120]])]\rThis array can be split into either two or four equally sized arrays on the default axis. The two possibilities are the following:\n1: [[10, 20, 30],[40, 50, 60]]\r2: [[70, 80, 90],[100, 110, 120]]\nOR\n1: [[10, 20, 30]]\r2: [[40, 50, 60]]\r3: [[70, 80, 90]]\r4: [[100, 110, 120]]\n\r\rADDING AND REMOVING\rThe last manipulating functions that we are going to look at are the ones which allow us to add and to remove items.\n\r\r\r\rADDING AND REMOVING FUNCTIONS\r\r\r\r\rnp.resize(a, (x,y))\rReturns a resized version of the array and fills empty spaces by repeating copies of a\r\rnp.append(a, […])\rAppends values at the end of the array\r\rnp.insert(a, x, …)\rInsert a value at the index x of the array\r\rnp.delete(a, x, y)\rDelete axes of the array\r\r\r\r\rLOADING AND SAVING ARRAYS\rNow last but not least, we are going to talk about loading and saving NumPy arrays. For this, we can use the integrated NumPy format or CSV-files.\n\rNUMPY FORMAT\rBasically, we are just serializing the object so that we can use it later. This is done by using the save function.\na = np.array([\r[ 10 , 20 , 30 ],\r[ 40 , 50 , 60 ],\r[ 70 , 80 , 90 ],\r[ 100 , 110 , 120 ]\r])\rnp.save( \u0026#39;myarray.npy\u0026#39; , a)\rNotice that you don’t have to use the file ending npy . In this example, we just use it for clarity. You can pick whatever you want.\rNow, in order to load the array into our script again, we will need the load function.\na = np.load( \u0026#39;myarray.npy\u0026#39; )\rprint (a)\r\rCSV FORMAT\rAs I already mentioned, we can also save our NumPy arrays into CSV files, which are just comma-separated text files. For this, we use the function savetxt .\nnp.savetxt( \u0026#39;myarray.csv\u0026#39; , a)\rOur array is now stored in a CSV-file which is very useful, because it can then also be read by other applications and scripts.\nIn order to read this CSV-file back into our script, we use the function loadtxt .\na = np.loadtxt( \u0026#39;myarray.csv\u0026#39; )\rprint (a)\rIf we want to read in a CSV-file that uses another separator than the default one, we can specify a certain delimiter.\na = np.loadtxt( \u0026#39;myarray.csv\u0026#39; , delimiter = \u0026#39;;\u0026#39; )\rprint (a)\rNow it uses semi-colons as separator when reading the file. The same can also be done with the saving or writing function\n\r\r",
    "ref": "/blog/numpy/"
  },{
    "title": "Python - Regular Expressions",
    "date": "",
    "description": "Introduction to Regular Expressions",
    "body": "\r\r\rREGULAR EXPRESSIONS\r\rIDENTIFIER\rMODIFIER\rESCAPE CHARACTERS\rAPPLYING REGULAR EXPRESSIONS\rMATCHING STRINGS\r\rMANIPULATING STRINGS\r\r\r\r\rREGULAR EXPRESSIONS\rIn programming, you will oftentimes have to deal with long texts from which we want to extract specific information. Also, when we want to process certain inputs, we need to check for a specific pattern. For example, think about emails. They need to have some text, followed by an @ character, then again some text and finally a dot and again some little text.\nIn order to make the validations easier, more efficient and more compact, we use so-called regular expressions .\nThe topic of regular expressions is very huge and you could write a whole book only about it. This is why we are not going to focus too much on the various placeholders and patterns of the expressions themselves but on the implementation of RegEx in Python.\rSo in order to confuse you right in the beginning, let’s look at a regular expression that checks if the format of an email-address is valid.\n^[a-zA-Z0-9.!#$%\u0026amp;\u0026#39;*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*$\rNow you can see why this is a huge field to learn. We are going to focus on quite simple examples and how to properly implement them in Python.\nIDENTIFIER\rLet’s get started with some basic knowledge first. So-called identifiers define what kind of\rcharacter should be at a certain place. Here you have some examples:\n| |REGEX IDENTIFIERS | |\r|--------------------|--------------------------------------|\r| IDENTIFIER | DESCRIPTION |\r| \\d | Some digit |\r| \\D | Everything BUT a digit |\r| \\s | White space |\r| \\S | Everything BUT a white space |\r| \\w | Some letter |\r| \\W | Everything BUT a letter |\r| . | Every character except for new lines |\r| \\b | White spaces around a word |\r| \\. | A dot |\r\rMODIFIER\rThe modifiers extend the regular expressions and the identifiers. They might be seen as some kind of operator for regular expressions.\n| REGEX MODIFIERS | |\r|-----------------|--------------------------------------------|\r| MODIFIER | DESCRIPTION |\r| {x,y} | A number that has a length between x and y |\r| + | At least one |\r| ? | None or one |\r| * | Everything |\r| $ | At the end of a string |\r| ^ | At the beginning of a string |\r| | | Either Or |\r| | Example: x | y = either x or y |\r| [] | Value range |\r| {x} | x times |\r| {x,y} | x to y times |\r\rESCAPE CHARACTERS\r| REGEX ESCAPE CHARATCERS | |\r|-------------------------|-------------|\r| CHARACTER | DESCRIPTION |\r| \\n | New Line |\r| \\t | Tab |\r| \\s | White Space |\r\rAPPLYING REGULAR EXPRESSIONS\rFINDING STRINGS\nIn order to apply these regular expressions in Python, we need to import the module re .\nimport re\rNow we can start by trying to find some patterns in our strings.\ntext = \u0026#39;\u0026#39;\u0026#39;\rMike is 20 years old and George is 29!\rMy grandma is even 104 years old!\r\u0026#39;\u0026#39;\u0026#39;\rages = re.findall( r\u0026#39;\\d{1,3}\u0026#39; , text)\rprint (ages)\r## [\u0026#39;20\u0026#39;, \u0026#39;29\u0026#39;, \u0026#39;104\u0026#39;]\rIn this example, we have a text with three ages in it. What we want to do is to filter these out and print them separately.\nAs you can see, we use the function findall in order to apply the regular expression onto our string. In this case, we are looking for numbers that are one to three digits long. Notice that we are using an r character before we write our expression. This indicates that the given string is a regular expression.\nAt the end, we print our result and get the following output:\n[‘20’, ‘29’, ‘104’]\n\rMATCHING STRINGS\rWhat we can also do is to check if a string matches a certain regular expression. For example, we can apply our regular expression for mails here.\nimport re\rtext = \u0026#39;test@mail.com\u0026#39;\rresult = re.fullmatch( r\u0026#39;^[a-zA-Z0-9.!#$%\u0026amp;\u0026#39;*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*$\u0026#39; , text)\rif result != None :\rprint ( \u0026#39;VALID!\u0026#39; )\relse :\rprint ( \u0026#39;INVALID!\u0026#39; )\rWe are not going to talk about the regular expression itself here. It is very long and complicated. But what we see here is a new function called fullmatch . This function returns the checked string if it matches the regular expression. In this case, this happens when the string has a valid mail format.\nIf the expression doesn’t match the string, the function returns None . In our example above, we get the message “VALID!” since the expression is met. If we enter something like “Hello World!”, we will get the other message.\nMANIPULATING STRINGS\rFinally, we are going to take a look at manipulating strings with regular expressions. By using the function sub we can replace all the parts of a string that match the expression by something else.\nimport re\rtext = \u0026#39;\u0026#39;\u0026#39;\rMike is 20 years old and George is 29!\rMy grandma is even 104 years old!\r\u0026#39;\u0026#39;\u0026#39;\rtext = re.sub( r\u0026#39;\\d{1,3}\u0026#39; , \u0026#39;100\u0026#39; , text)\rprint (text)\r## ## Mike is 100 years old and George is 100!\r## My grandma is even 100 years old!\rIn this example, we replace all ages by 100 . This is what gets printed:\rMike is 100 years old and George is 100!\rMy grandma is even 100 years old!\nThese are the basic functions that we can operate with in Python when dealing with regular expressions. If you want to learn more about regular expressions just google and you will find a lot of guides. Play around with the identifiers and modifiers a little bit until you feel like you understand how they work.\n\r\r\r",
    "ref": "/blog/regular-expressions/"
  },{
    "title": "Python - Logging",
    "date": "",
    "description": "Introduction to Python Logging",
    "body": "\r\r\rLOGGING\r\rSECURITY LEVELS\rCREATING LOGGERS\rLOGGING INTO FILES\rFORMATTING LOGS\r\r\r\rLOGGING\rNo matter what we do in computer science, sooner or later we will need logs. Every system that has a certain size produces errors or conditions in which specific people should be warned or informed. Nowadays, everything gets logged or recorded. Bank transactions, flights, networking activities, operating systems and much more. Log files help us to find problems and to get information about the state of our systems. They are an essential tool for avoiding and understanding errors.\rUp until now, we have always printed some message onto the console screen when we encountered an error. But when our applications grow, this becomes confusing and we need to categorize and outsource our logs. In addition, not every message is equally relevant. Some messages are urgent because a critical component fails and some just provide nice information.\nSECURITY LEVELS\rIn Python, we have got five security levels. A higher level means higher importance or urgency.\n\rDEBUG\rINFO\rWARNING\rERROR\rCRITICAL\n\rNotice that when we choose a certain security level, we also get all the messages of the levels above. So for example, INFO also prints the messages of WARNING, ERROR and CRITICAL but not of DEBUG .\rDEBUG is mainly used for tests, experiments or in order to check something. We typically use this mode, when we are looking for errors (troubleshooting).\rWe use INFO when we want to log all the important events that inform us about what is happening. This might be something like “User A logged in successfully!” or “Now we have 17 users online!”\rWARNING messages are messages that inform us about irregularities and things that might go wrong and become a problem. For example messages like “Only 247 MB of RAM left!”\rAn ERROR message gets logged or printed when something didn’t go according to the plan. When we get an exception this is a classical error.\rCRITICAL messages tell us that critical for the whole system or application happened. This might be the case when a crucial component fails and we have to immediately stop all operations.\n\rCREATING LOGGERS\rIn order to create a logger in Python, we need to import the logging module.\nimport logging\rNow we can just log messages by directly using the respective functions of the logging module.\nlogging.info( \u0026#39;First informational message!\u0026#39; )\rlogging.critical( \u0026#39;This is serious!\u0026#39; )\rThis works because we are using the root logger. We haven’t created our own loggers yet. The output looks like this:\nCRITICAL:root:This is serious!\nINFO:root:Logger successfully created!\nSo let’s create our own logger now. This is done by either using the constructor of the Logger class or by using the method getLogger .\nlogger = logging.getLogger()\rlogger = logging.Logger( \u0026#39;MYLOGGER\u0026#39; )\rNotice that we need to specify a name for our logger, if we use the constructor. Now we can log our messages.\nlogger.info( \u0026#39;Logger successfully created!\u0026#39; )\rlogger.log(logging.INFO, \u0026#39;Successful!\u0026#39; )\rlogger.critical( \u0026#39;Critical Message!\u0026#39; )\rlogger.log(logging.CRITICAL, \u0026#39;Critical!\u0026#39; )\rHere we also have two different options for logging messages. We can either directly call the function of the respective security level or we can use the method log and specify the security level in the parameters.\nBut when you now execute the script, you will notice that it will only print the critical messages. This has two reasons. First of all, we need to adjust the level of the logger and second of all, we need to remove all of the handlers from the default root logger.\nfor handler in logging.root.handlers:\nlogging.root.removeHandler(handler)\rlogging.basicConfig( level =logging.INFO)\rHere we just use a for loop in order to remove all the handlers from the root logger. Then we use the basicConfig method, in order to set our logging level to INFO . When we now run our code again, the output is the following:\nINFO:MYLOGGER:Logger successfully created!\rINFO:MYLOGGER:Successful!\rCRITICAL:MYLOGGER:Critical Message!\rCRITICAL:MYLOGGER:Critical!\n\rLOGGING INTO FILES\rWhat we are mainly interested in is logging into files. For this, we need a so-called FileHandler . It is an object that we add to our logger, in order to make it log everything into a specific file.\nimport logging\rlogger = logging.getLogger( \u0026#39;MYLOGGER\u0026#39; )\rlogger.setLevel(logging.INFO)\rhandler = logging.FileHandler( \u0026#39;logfile.log\u0026#39; )\rhandler.setLevel(logging.INFO)\rlogger.addHandler(handler)\rlogger.info( \u0026#39;Log this into the file!\u0026#39; )\rlogger.critical( \u0026#39;This is critical!\u0026#39; )\rWe start again by defining a logger. Then we set the security level to INFO by using the function setLevel . After that, we create a FileHandler that logs into the file logfile.log . Here we also need to set the security level. Finally, we add the handler to our logger using the addHandler function and start logging messages.\n\rFORMATTING LOGS\rOne thing that you will notice is that we don’t have any format in our logs. We don’t know which logger was used or which security level our message has. For this, we can use a so-called formatter .\nimport logging\rlogger = logging.getLogger()\rlogger.setLevel(logging.INFO)\rhandler = logging.FileHandler( \u0026#39;logfile.log\u0026#39; )\rhandler.setLevel(logging.INFO)\rformatter = logging.Formatter( \u0026#39;%(asctime)s: %(levelname)s - %(message)s\u0026#39; )\rhandler.setFormatter(formatter)\rlogger.addHandler(handler)\rlogger.info( \u0026#39;This will get into the file!\u0026#39; )\rWe create a formatter by using the constructor of the respective class. Then we use the keywords for the timestamp, the security level name and the message. Last but not least, we assign the formatter to our handler and start logging again. When we now look into our file, we will find a more detailed message.\r2019-06-25 15:41:43,523: INFO - This will get into the file!\rThese log messages can be very helpful, if they are used wisely. Place them wherever something important or alarming happens in your code\n\r\r",
    "ref": "/blog/python-logging/"
  },{
    "title": "Python - XML Processing",
    "date": "",
    "description": "Introduction to Python XML Processing",
    "body": "\r\r\rXML PROCESSING\r\rXML PARSER\rSIMPLE API FOR XML (SAX)\rDOCUMENT OBJECT MODEL (DOM)\rXML STRUCTURE\rXML WITH SAX\rCONTENT HANDLER CLASS\rPROCESSING XML DATA\rXML WITH DOM\rMANIPULATING XML FILES\rCREATING NEW ELEMENTS\r\r\r\rXML PROCESSING\rUp until now, we either saved our data into regular text files or into professional databases. Sometimes however, our script is quite small and doesn’t need a big database but we still want to structure our data in files. For this, we can use XML .\nXML stands for Extensible Markup Language and is a language that allows us to hierarchically structure our data in files. It is platform-independent and also application-independent. XML files that you create with a Python script, can be read and processed by a C++ or Java application.\nXML PARSER\rIn Python, we can choose between two modules for parsing XML files – SAX and DOM .\n\rSIMPLE API FOR XML (SAX)\rSAX stands for Simple API for XML and is better suited for large XML files or in situations where we have very limited RAM memory space. This is because in this mode we never load the full file into our RAM. We read the file from our hard drive and only load the little parts that we need right at the moment into the RAM. An additional effect of this is that we can only read from the file and not manipulate it and change values.\n\rDOCUMENT OBJECT MODEL (DOM)\rDOM stands for Document Object Model and is the generally recommended option. It is a language-independent API for working with XML. Here we always load the full XML file into our RAM and then save it there in a hierarchical structure. Because of that, we can use all of the features and also manipulate the file.\nObviously, DOM is a lot faster than SAX because it is using the RAM instead of the hard disk. The main memory is way more efficient than the hard drive. We only use SAX when our RAM is so limited that we can’t even load the full XML file into it without problems.\rThere is no reason to not use both options in the same projects. We can choose depending on the use case.\n\rXML STRUCTURE\rFor this,we are going to use the following XML file:\n\u0026lt;? xml version= \u0026#39;1.0\u0026#39; ?\u0026gt;\r\u0026lt; group \u0026gt;\r\u0026lt; person id= \u0026#39;1\u0026#39; \u0026gt;\r\u0026lt; name \u0026gt;John Smith\u0026lt;/ name \u0026gt;\r\u0026lt; age \u0026gt;20\u0026lt;/ age \u0026gt;\r\u0026lt; weight \u0026gt;80\u0026lt;/ weight \u0026gt;\r\u0026lt; height \u0026gt;188\u0026lt;/ height \u0026gt;\r\u0026lt;/ person \u0026gt;\r\u0026lt; person id= \u0026#39;2\u0026#39; \u0026gt;\r\u0026lt; name \u0026gt;Mike Davis\u0026lt;/ name \u0026gt;\r\u0026lt; age \u0026gt;45\u0026lt;/ age \u0026gt;\r\u0026lt; weight \u0026gt;82\u0026lt;/ weight \u0026gt;\r\u0026lt; height \u0026gt;185\u0026lt;/ height \u0026gt;\r\u0026lt;/ person \u0026gt;\r\u0026lt; person id= \u0026#39;3\u0026#39; \u0026gt;\r\u0026lt; name \u0026gt;Anna Johnson\u0026lt;/ name \u0026gt;\r\u0026lt; age \u0026gt;33\u0026lt;/ age \u0026gt;\r\u0026lt; weight \u0026gt;67\u0026lt;/ weight \u0026gt;\r\u0026lt; height \u0026gt;167\u0026lt;/ height \u0026gt;\r\u0026lt;/ person \u0026gt;\r\u0026lt; person id= \u0026#39;4\u0026#39; \u0026gt;\r\u0026lt; name \u0026gt;Bob Smith\u0026lt;/ name \u0026gt;\r\u0026lt; age \u0026gt;60\u0026lt;/ age \u0026gt;\r\u0026lt; weight \u0026gt;70\u0026lt;/ weight \u0026gt;\r\u0026lt; height \u0026gt;174\u0026lt;/ height \u0026gt;\r\u0026lt;/ person \u0026gt;\r\u0026lt; person id= \u0026#39;5\u0026#39; \u0026gt;\r\u0026lt; name \u0026gt;Sarah Pitt\u0026lt;/ name \u0026gt;\r\u0026lt; age \u0026gt;12\u0026lt;/ age \u0026gt;\r\u0026lt; weight \u0026gt;50\u0026lt;/ weight \u0026gt;\r\u0026lt; height \u0026gt;152\u0026lt;/ height \u0026gt;\r\u0026lt;/ person \u0026gt;\r\u0026lt;/ group \u0026gt; \rAs you can see, the structure is quite simple. The first row is just a notation and indicates that we are using XML version one. After that we have various tags. Every tag that gets opened also gets closed at the end.\rBasically, we have one group tag. Within that, we have multiple person tags that all have the attribute id . And then again, every person has four tags with their values. These tags are the attributes of the respective person. We save this file as group.xml .\n\rXML WITH SAX\rIn order to work with SAX, we first need to import the module:\nimport xml.sax\rNow, what we need in order to process the XML data is a content handler . It handles and processes the attributes and tags of the file.\nimport xml.sax\rhandler = xml.sax.ContentHandler()\rparser = xml.sax.make_parser()\rparser.setContentHandler(handler)\rparser.parse( \u0026#39;group.xml\u0026#39; )\rFirst we create an instance of the ContentHandler class. Then we use the method make_parser, in order to create a parser object. After that, we set our handler to the content handler of our parser. We can then parse the file by using the method parse .\nNow, when we execute our script, we don’t see anything. This is because we need to define what happens when an element gets parsed.\n\rCONTENT HANDLER CLASS\rFor this, we will define our own content handler class. Let’s start with a very simple example.\nimport xml.sax\rclass GroupHandler(xml.sax.ContentHandler):\rdef startElement( self , name, attrs):\rprint (name)\rhandler = GroupHandler()\rparser = xml.sax.make_parser()\rparser.setContentHandler(handler)\rparser.parse( \u0026#39;group.xml\u0026#39; )\rWe created a class GroupHandler that inherits from ContentHandler . Then we overwrite the function startElement . Every time an element gets processed, this function gets called. So by manipulating it, we can define what shall happen during the parsing process.\nNotice that the function has two parameters – name and attr . These represent the tag name and the attributes. In our simple example, we just print the tag names. So, let’s get to a more interesting example.\n\rPROCESSING XML DATA\rThe following example is a bit more complex and includes two more functions.\nimport xml.sax\rclass GroupHandler(xml.sax.ContentHandler):\rdef startElement( self , name, attrs):\rself .current = name\rif self .current == \u0026#39;person\u0026#39; :\rprint ( \u0026#39;--- Person ---\u0026#39; )\rid = attrs[ \u0026#39;id\u0026#39; ]\rprint ( \u0026#39;ID: %s\u0026#39; % id)\rdef endElement( self , name):\rif self .current == \u0026#39;name\u0026#39; :\rprint ( \u0026#39;Name: %s\u0026#39; % self .name)\relif self .current == \u0026#39;age\u0026#39; :\rprint ( \u0026#39;Age: %s\u0026#39; % self .age)\relif self .current == \u0026#39;weight\u0026#39; :\rprint ( \u0026#39;Weight: %s\u0026#39; % self .weight)\relif self .current == \u0026#39;height\u0026#39; :\rprint ( \u0026#39;Height: %s\u0026#39; % self .height)\rself .current = \u0026#39;\u0026#39; def characters( self , content):\rif self .current == \u0026#39;name\u0026#39; :\rself .name = content\relif self .current == \u0026#39;age\u0026#39; :\rself .age = content\relif self .current == \u0026#39;weight\u0026#39; :\rself .weight = content\relif self .current == \u0026#39;height\u0026#39; :\rself .height = content\rhandler = GroupHandler()\rparser = xml.sax.make_parser()\rparser.setContentHandler(handler)\rparser.parse( \u0026#39;group.xml\u0026#39; ) \rThe first thing you will notice here is that we have three functions instead of one. When we start processing an element, the function startElement gets called. Then we go on to process the individual characters which are name, age, weight and height . At the end of the element parsing, we call the endElement function.\nIn this example, we first check if the element is a person or not. If this is the case we print the id just for information. We then go on with the characters method. It checks which tag belongs to which attribute and saves the values accordingly. At the end, we print out all the values. This is what the results look like:\n— Person —\rID: 1\rName: John Smith\rAge: 20\rWeight: 80\rHeight: 188\r— Person —\rID: 2\rName: Mike Davis\rAge: 45\rWeight: 82\rHeight: 185\r— Person —\r…\n\rXML WITH DOM\rNow, let’s look at the DOM option. Here we can not only read from XML files but also change values and attributes. In order to work with DOM, we again need to import the respective module.\nimport xml.dom.minidom\rWhen working with DOM, we need to create a so-called DOM-Tree and view all elements as collections or sequences.\ndomtree = xml.dom.minidom.parse( \u0026#39;group.xml\u0026#39; )\rgroup = domtree.documentElement \rWe parse the XML file by using the method parse . This returns a DOM-tree, which we save into a variable. Then we get the documentElement of our tree and in our case this is group . We also save this one into an object.\npersons = group.getElementsByTagName( \u0026#39;person\u0026#39; )\rfor person in persons:\rprint ( \u0026#39;--- Person ---\u0026#39; )\rif person.hasAttribute( \u0026#39;id\u0026#39; ):\rprint ( \u0026#39;ID: %s\u0026#39; % person.getAttribute( \u0026#39;id\u0026#39; ))\rname = person.getElementsByTagName( \u0026#39;name\u0026#39; )[ 0 ]\rage = person.getElementsByTagName( \u0026#39;age\u0026#39; )[ 0 ]\rweight = person.getElementsByTagName( \u0026#39;weight\u0026#39; )[ 0 ]\rheight = person.getElementsByTagName( \u0026#39;height\u0026#39; )[ 0 ] \rNow, we can get all the individual elements by using the getElementsByTagName function. For example, we save all our person tags into a variable by using this method and specifying the name of our desired tags. Our persons variable is now a sequence that we can iterate over.\nBy using the functions hasAttribute and getAttribute, we can also access the attributes of our tags. In this case, this is only the id . In order to get the tag values of the individual person, we again use the method getElementsByTagName .\nWhen we do all that and execute our script, we get the exact same result as with SAX .\r— Person —\rID: 1\rName: John Smith\rAge: 20\rWeight: 80\rHeight: 188\r— Person —\rID: 2\rName: Mike Davis\rAge: 45\rWeight: 82\rHeight: 185\r— Person —\r…\n\rMANIPULATING XML FILES\rSince we are now working with DOM , let’s manipulate our XML file and change some values.\npersons = group.getElementsByTagName( \u0026#39;person\u0026#39; )\rpersons[ 0 ].getElementsByTagName( \u0026#39;name\u0026#39; )[ 0 ].childNodes[ 0 ].nodeValue = \u0026#39;New Name\u0026#39;\rAs you can see, we are using the same function, to access our elements. Here we adress the name tag of the first person object. Then we need to access the childNodes and change their nodeValue . Notice that we only have one element name and also only one child node but we still need to address the index zero, for the first element.\nIn this example, we change the name of the first person to New Name . Now in order to apply these changes to the real file, we need to write into it.\ndomtree.writexml( open ( \u0026#39;group.xml\u0026#39; , \u0026#39;w\u0026#39; ))\rWe use the writexml method of our initial domtree object. As a parameter, we pass a file stream that writes into our XML file. After doing that, we can look at the changes.\n\u0026lt; person id= \u0026#39;1\u0026#39; \u0026gt;\r\u0026lt; name \u0026gt;New Name\u0026lt;/ name \u0026gt;\r\u0026lt; age \u0026gt;20\u0026lt;/ age \u0026gt;\r\u0026lt; weight \u0026gt;80\u0026lt;/ weight \u0026gt;\r\u0026lt; height \u0026gt;188\u0026lt;/ height \u0026gt;\r\u0026lt;/ person \u0026gt;\rWe can also change the attributes by using the function setAttribute .\npersons[ 0 ].setAttribute( ‘id’ , ‘10’ )\rHere we change the attribute id of the first person to 10 .\n\u0026lt; person id= \u0026#39;10\u0026#39; \u0026gt;\r\u0026lt; name \u0026gt;New Name\u0026lt;/ name \u0026gt;\r\u0026lt; age \u0026gt;20\u0026lt;/ age \u0026gt;\r\u0026lt; weight \u0026gt;80\u0026lt;/ weight \u0026gt;\r\u0026lt; height \u0026gt;188\u0026lt;/ height \u0026gt;\r\u0026lt;/ person \u0026gt;\r\rCREATING NEW ELEMENTS\rWe first need to define a new person element.\nnewperson = domtree.createElement( \u0026#39;person\u0026#39; )\rnewperson.setAttribute( \u0026#39;id\u0026#39; , \u0026#39;6\u0026#39; )\rSo we use the domtree object and the respective method, to create a new XML element. Then we set the id attribute to the next number.\nAfter that, we create all the elements that we need for the person and assign values to them.\nname = domtree.createElement( \u0026#39;name\u0026#39; )\rname.appendChild(domtree.createTextNode( \u0026#39;Paul Smith\u0026#39; ))\rage = domtree.createElement( \u0026#39;age\u0026#39; )\rage.appendChild(domtree.createTextNode( \u0026#39;45\u0026#39; ))\rweight = domtree.createElement( \u0026#39;weight\u0026#39; )\rweight.appendChild(domtree.createTextNode( \u0026#39;78\u0026#39; ))\rheight = domtree.createElement( \u0026#39;height\u0026#39; )\rheight.appendChild(domtree.createTextNode( \u0026#39;178\u0026#39; )) \rFirst, we create a new element for each attribute of the person. Then we use the method appendChild to put something in between the tags of our element. In this case we create a new TextNode , which is basically just text.\nLast but not least, we again need to use the method appendChild in order to define the hierarchical structure. The attribute elements are the childs of the person element and this itself is the child of the group element.\nnewperson.appendChild(name)\rnewperson.appendChild(age)\rnewperson.appendChild(weight)\rnewperson.appendChild(height)\rgroup.appendChild(newperson)\rdomtree.writexml( open ( \u0026#39;group.xml\u0026#39; , \u0026#39;w\u0026#39; ))\rWhen we write these changes into our file, we can see the following results:\n\u0026lt; person id= \u0026#39;6\u0026#39; \u0026gt;\r\u0026lt; name \u0026gt;Paul Smith\u0026lt;/ name \u0026gt;\r\u0026lt; age \u0026gt;45\u0026lt;/ age \u0026gt;\r\u0026lt; weight \u0026gt;78\u0026lt;/ weight \u0026gt;\r\u0026lt; height \u0026gt;178\u0026lt;/ heigh\r\r\r",
    "ref": "/blog/python-xml-processing/"
  },{
    "title": "Python - Database Programming",
    "date": "",
    "description": "Introduction to Python Database Programming",
    "body": "\r\r\rDatabase Programming\r\rCONNECTING TO SQLITE\rEXECUTING STATEMENTS\rCREATING TABLES\rINSERTING VALUES\rSELECTING VALUES\rCLASSES AND TABLES\rFROM TABLE TO OBJECT\rFROM OBJECT TO TABLE\r\r\r\rDatabase Programming\rDatabases are one of the most popular ways to store and manage data in computer science. Because of that, in this post we are going to take a look at database programming with Python.\rNotice that for most databases we use the query language SQL , which stands for Structured Query Language . We use this language in order to manage the database, the tables and the rows and columns.\nCONNECTING TO SQLITE\rThe database that comes pre-installed with Python is called SQLite . It is also the one which we are going to use. Of course, there are also other libraries for MySQL, MongoDB etc.\rIn order to use SQLite in Python, we need to import the respective module – sqlite3 .\nimport sqlite3\rNow, to create a new database file on our disk, we need to use the connect method.\nconn = sqlite3.connect( \u0026#39;mydata.db\u0026#39; )\rThis right here creates the new file mydata.db and connects to this database. It returns a connection object which we save in the variable conn .\n\rEXECUTING STATEMENTS\rSo, we have established a connection to the database. But in order to execute SQL statements, we will need to create a so-called cursor .\nc = conn.cursor()\rWe get this cursor by using the method cursor of our connection object that returns it. Now we can go ahead and execute all kinds of statements.\n\rCREATING TABLES\rFor example, we can create our first table like this:\nc.execute( \u0026#39;\u0026#39;\u0026#39;CREATE TABLE persons (\rfirst_name TEXT,\rlast_name TEXT,\rage INTEGER\r)\u0026#39;\u0026#39;\u0026#39; )\rHere we use the execute function and write our query. What we are passing here is SQL code. As I already said, understanding SQL is not the main objective here. We are focusing on the Python part. Nevertheless, it’s quite obvious what’s happening here. We are creating a new table with the name persons and each person will have the three attributes first_name, last_name and age .\nNow our statement is written but in order to really execute it, we ne need to commit to our connection.\nconn.commit()\rWhen we do this, our statement gets executed and our table created. Notice that this works only once, since after that the table already exists and can’t be created again.\rAt the end, don’t forget to close the connection, when you are done with everything.\nconn.close()\r\rINSERTING VALUES\rNow let’s fill up our table with some values. For this, we just use an ordinary INSERT statement.\nc.execute( \u0026#39;\u0026#39;\u0026#39;INSERT INTO persons VALUES\r(\u0026#39;John\u0026#39;, \u0026#39;Smith\u0026#39;, 25),\r(\u0026#39;Anna\u0026#39;, \u0026#39;Smith\u0026#39;, 30),\r(\u0026#39;Mike\u0026#39;, \u0026#39;Johnson\u0026#39;, 40)\u0026#39;\u0026#39;\u0026#39; )\rconn.commit()\rconn.close()\rSo basically, we are just adding three entries to our table. When you run this code, you will see that everything went fine. But to be on the safe side, we will try to now extract the values from the database into our program.\n\rSELECTING VALUES\rIn order to get values from the database, we need to first execute a SELECT statement. After that, we also need to fetch the results.\nc.execute( \u0026#39;\u0026#39;\u0026#39;SELECT * FROM persons\rWHERE last_name = \u0026#39;Smith\u0026#39;\u0026#39;\u0026#39;\u0026#39; )\rprint (c.fetchall())\rconn.commit()\rconn.close()\rAs you can see, our SELECT statement that gets all the entries where the last_name has the value Smith . We then need to use the method fetchall of the cursor, in order to get our results. It returns a list of tuples, where every tuple is one entry. Alternatively, we could use the method fetchone to only get the first entry or fetchmany to get a specific amount of entries. In our case however, the result looks like this:\r[(‘John’, ‘Smith’, 25), (‘Anna’, ‘Smith’, 30)]\n\rCLASSES AND TABLES\rNow in order to make the communication more efficient and easier, we are going to create a Person class that has the columns as attributes.\nclass Person():\rdef __init__ ( self , first= None ,last= None , age= None ):\rself .first = first\rself .last = last\rself .age = age\rdef clone_person( self , result):\rself .first = result[ 0 ]\rself .last = result[ 1 ]\rself .age = result[ 2 ]\rHere we have a constructor with default parameters. In case we don’t specify any values, they get assigned the value None . Also, we have a function clone_person that gets passed a sequence and assigns the values of it to the object. In our case, this sequence will be the tuple from the fetching results.\n\rFROM TABLE TO OBJECT\rSo let’s create a new Person object by getting its data from our database.\nc.execute( \u0026#39;\u0026#39;\u0026#39;SELECT * FROM persons\rWHERE last_name = \u0026#39;Smith\u0026#39;\u0026#39;\u0026#39;\u0026#39; )\rperson1 = Person()\rperson1.clone_person(c.fetchone())\rprint (person1.first)\rprint (person1.last)\rprint (person1.age)\rHere we fetch the first entry of our query results, by using the fetchone function. The result is the following:\rJohn\rSmith\r25\n\rFROM OBJECT TO TABLE\rWe can also do that the other way around. Let’s create a person objects, assign values to the attributes and then insert this object into our database.\nperson2 = Person( \u0026#39;Bob\u0026#39; , \u0026#39;Davis\u0026#39; , 23 )\rc.execute( \u0026#39;\u0026#39;\u0026#39;INSERT INTO persons VALUES\r(\u0026#39;{}\u0026#39;, \u0026#39;{}\u0026#39;, \u0026#39;{}\u0026#39;)\u0026#39;\u0026#39;\u0026#39;\r.format(person2.first,\rperson2.last,\rperson2.age))\rconn.commit()\rconn.close()\rHere we used the basic format function in order to put our values into the statement. When we execute it, our object gets inserted into the database. We can check this by printing all objects of the table persons .\nc.execute( \u0026#39;SELECT * FROM persons\u0026#39; )\rprint (c.fetchall())\rIn the results, we find our new object:\r[(\u0026#39;John\u0026#39;, \u0026#39;Smith\u0026#39;, 25), (\u0026#39;Anna\u0026#39;, \u0026#39;Smith\u0026#39;, 30), (\u0026#39;Mike\u0026#39;, \u0026#39;Johnson\u0026#39;, 40), (\u0026#39;Bob\u0026#39;, \u0026#39;Davis\u0026#39;, 23) ]\rPREPARED STATEMENTS\rThere is a much more secure and elegant way to put the values of our attributes into the SQL statements. We can use prepared statements .\nperson = Person( \u0026#39;Julia\u0026#39; , \u0026#39;Johnson\u0026#39; , 28 )\rc.execute( \u0026#39;INSERT INTO persons VALUES (?, ?, ?)\u0026#39; ,\r(person.first, person.last, person.age))\rconn.commit()\rconn.close()\rWe replace the values with question marks and pass the values as a tuple in the function. This makes our statements cleaner and also less prone to SQL injections.\n\r\r\r",
    "ref": "/blog/python-database-programming/"
  },{
    "title": "Python - Networking",
    "date": "",
    "description": "Introduction to Python Networking",
    "body": "\r\r\rNETWORK PROGRAMMING\r\rSOCKETS\rCREATING SOCKETS\rSERVER SOCKET METHODS\rCLIENT-SERVER ARCHITECTURE\rSERVER SOCKET METHODS\rCLIENT SOCKET METHODS\rOTHER SOCKET METHODS\rCREATING A SERVER\rCREATING A CLIENT\rCONNECTING SERVER AND CLIENT\rPORT SCANNER\rTHREADED PORT SCANNER\r\r\r\rNETWORK PROGRAMMING\rNow we get into one of the most interesting intermediate topics – network programming . It is about communicating with other applications and devices via some network. That can be the internet or just the local area network.\nSOCKETS\rWHAT ARE SOCKETS?\nWhenever we talk about networking in programming, we also have to talk about sockets . They are the endpoints of the communication channels or basically, the endpoints that talk to each other. The communication may happen in the same process or even across different continents over the internet.\rWhat’s important is that in Python we have different access levels for the network services. At the lower layers, we can access the simple sockets that allow us to use the connection-oriented and connectionless protocols like TCP or UDP, whereas other Python modules like FTP or HTTP are working on a higher layer – the application layer .\n\rCREATING SOCKETS\rIn order to work with sockets in Python, we need to import the module socket .\nimport socket\rNow, before we start defining and initializing our socket, we need to know a couple of things in advance:\r· Are we using an internet socket or a UNIX socket?\r· Which protocol are we going to use?\r· Which IP-address are we using?\r· Which port number are we using?\nThe first question can be answered quite simply. Since we want to communicate over a network instead of the operating system, we will stick with the internet socket .\nThe next question is a bit trickier. We choose between the protocols TCP ( Transmission Control Protocol) and UDP ( User Datagram Protocol). TCP is connection-oriented and more trustworthy than UDP. The chances of losing data are minimal in comparison to UDP. On the other hand, UDP is much faster than TCP. So the choice depends on the task we want to fulfil. For our examples, we will stick with TCP since we don’t care too much about speed for now.\nThe IP-address should be the address of the host our application will run on. For now, we will use 127.0.0.1 which is the localhost address. This applies to every machine. But notice that this only works when you are running your scripts locally.\nFor our port we can basically choose any number we want. But be careful with low numbers, since all numbers up to 1024 are standardized and all numbers from 1024 to 49151 are reserved . If you choose one of these numbers, you might have some conflicts with other applications or your operating system.\nimport socket\rs = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\rHere we created our first socket, by initializing an instance of the class socket . Notice that we passed two parameters here. The first one AF_INET states that we want an internet socket rather than a UNIX socket . The second one SOCK_STREAM is for the protocol that we choose. In this case it stands\rfor TCP . If we wanted UDP , we would have to choose SOCK_DGRAM.\nA server opens up a session with every client that connects to it. This way, servers are able to serve multiple clients at once and individually.\n\rSERVER SOCKET METHODS\rThere are three methods of the socket class that are of high importance for the servers.\rSo we have a socket that uses the IP protocol (internet) and the TCP protocol. Now, before we get into the actual setup of the socket, we need to talk a little bit about clients and servers.\n\rCLIENT-SERVER ARCHITECTURE\rIn a nutshell, the server is basically the one who provides information and serves data, whereas the clients are the ones who request and receive the data from the server.A server opens up a session with every client that connects to it. This way, servers are able to serve multiple clients at once and individually.\n\rSERVER SOCKET METHODS\r\r\rSERVER SOCKET METHODS\r\r\r\r\rMETHOD\rDESCRIPTION\r\rbind()\rBinds the address that consists of hostname and port to the socket\r\rlisten()\rWaits for a message or a signal\r\raccept()\rAccepts the connection with a client\r\r\r\r\rCLIENT SOCKET METHODS\rFor the client, there is only one specific and very important method, namely connect . With this method the client attempts to connect to a server which then has to accept this with the respective method.\rThere are three methods of the socket class that are of high importance for the servers.\n\rOTHER SOCKET METHODS\r\r\rOTHER SOCKET METHODS\r\r\r\r\rMETHOD\rDESCRIPTION\r\rrecv()\rReceives a TCP message\r\rsend()\rSends a TCP message\r\rrecvfrom()\rReceives a UDP message\r\rsendto()\rSends a UDP message\r\rclose()\rCloses a socket\r\rgethostname()\rReturns hostname of a socket\r\r\r\r\rCREATING A SERVER\rNow that we understand the client-server architecture, we are going to implement our server. We decided that we want to use TCP and an internet socket. For the address we will use the localhost address 127.0.0.1 and as a port, we will choose 9999 .\ns = socket.socket(socket.AF_INET,\rsocket.SOCK_STREAM)\rs.bind(( \u0026#39;127.0.0.1\u0026#39; , 9999 ))\rs.listen()\rprint ( \u0026#39;Listening...\u0026#39; )\rHere we initialize our socket. We then use the method bind , in order to assign the IP-address and the port we chose. Notice that we are passing a tuple as a parameter here. Last but not least, we put our socket to listening mode by using the method listen .After that, we just have to create a loop that accepts the client requests that will eventually come in.server.py\nimport socket\rs = socket.socket(socket.AF_INET,\rsocket.SOCK_STREAM)\rs.bind(( \u0026#39;127.0.0.1\u0026#39; , 9999 ))\rs.listen()\rprint ( \u0026#39;Listening...\u0026#39; )\rwhile True :\rclient, address = s.accept()\rprint ( \u0026#39;Connected to {}\u0026#39; .format(address))\rmessage = \u0026#39;Hello Client!\u0026#39;\rclient.send(message.encode( \u0026#39;ascii\u0026#39; ))\rclient.close()\rThe method accept waits for a connection attempt to come and accepts it. It then returns a client for responses and the address of the client that is connected. We can then use this client object in order to send the message. But it’s important that we encode the message first, because otherwise we can’t send it properly. At the end, we close the client because we don’t need it anymore.\rAlso, there are some other socket methods that are quite important in general.\n\rCREATING A CLIENT\rNow our server is done and we just need some clients that connect to it. Our clients shall request a resource from the server. In this case, this is the message “Hello Client!” .\rFor our client we also need a socket but this time it will not use the function bind but the function connect . So let’s start writing our code into a new file.\nimport socket\rs = socket.socket(socket.AF_INET,\rsocket.SOCK_STREAM)\rs.connect(( \u0026#39;127.0.0.1\u0026#39; , 9999 ))\rWe just create an ordinary internet socket that uses TCP and then connect it to the localhost IP-address at the port 9999.\rTo now get the message from the server and decode it, we will use the recv function.\rclient.py\nimport socket\rs = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\rs.connect(( \u0026#39;127.0.0.1\u0026#39; , 9999 ))\rmessage = s.recv( 1024 )\rs.close()\rprint (message.decode( \u0026#39;ascii\u0026#39; ))\rAfter we connect to the server, we try to receive up to 1024 bytes from it. We then save the message into our variable and then we decode and print it.\n\rCONNECTING SERVER AND CLIENT\rNow in order to connect these two entities, we first need to run our server. If there is no server listening on the respective port, our client can’t connect to anything. So we run our server.py script and start listening.\rAfter that, we can run our client.py script many times and they will all connect to the server. The results will look like this:\nServer\rListening…\rConnected to (‘127.0.0.1’, 4935)\rConnected to (‘127.0.0.1’, 4942)\rConnected to (‘127.0.0.1’, 4943)\rConnected to (‘127.0.0.1’, 4944)\rConnected to (‘127.0.0.1’, 4945)\nClient\rHello Client!\rOne thing you might optimize on that script if you want is the exception handling. If there is no server listening and our client tries to connect, we get a ConnectionRefusedError and our script crashes. Now you can fix this with the knowledge from the first book.\n\rPORT SCANNER\rNow we have learned a lot about multithreading, locking, queues and sockets. With all that knowledge, we can create a highly efficient and well working port scanner .\rWhat a port scanner basically does is: It tries to connect to certain ports at a host or a whole network, in order to find loopholes for future attacks. Open ports mean a security breach. And with our skills, we can already code our own penetration testing tool.\nimport socket\rtarget = \u0026#39;10.0.0.5\u0026#39;\rdef portscan(port):\rtry :\rs = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\rconn = s.connect((target, port))\rreturn True\rexcept :\rreturn False\rfor x in range ( 1 , 501 ):\rif (portscan(x)):\rprint ( \u0026#39;Port {} is open!\u0026#39; .format(x))\relse :\rprint ( \u0026#39;Port {} is closed!\u0026#39; .format(x))\rSo this scanner is quite simple. We define a target address. In this case, this is 10.0.0.5 . Our function portscan simply tries to connect to a certain port at that host. If it succeeds, the function returns True . If we get an error or an exception, it returns False .\rThis is as simple as a port scan can get. We then use a for loop to scan the first 500 ports and we always print if the port is open or closed.\nJust choose a target address and run this script. You will see that it works.\rPort 21 is closed!\rPort 22 is open!\rPort 23 is closed!\rPort 24 is closed!\rPort 25 is open!\nBut you will also notice that it is extremely slow. That’s because we serially scan one port after the other. And I think we have already learned how to handle that.\n\rTHREADED PORT SCANNER\rIn order to speed up the scanning process, we are going to use multithreading . And to make sure that every port gets scanned and also that no port is scanned twice, we will use queues.\nimport socket\rfrom queue import Queue\rimport threading\rtarget = \u0026#39;10.0.0.5\u0026#39;\rq = Queue()\rfor x in range ( 1 , 501 ):\rq.put(x)\rdef portscan(port):\rtry :\rs = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\rconn = s.connect((target, port))\rreturn True\rexcept :\rreturn False\rdef worker():\rwhile True :\rport = q.get()\rif portscan(port):\rprint ( \u0026#39;Port {} is open!\u0026#39;.format(port))\rSo we start by creating a queue and filling it up with all numbers from 1 to 500. We then have two functions. The portscan function does the scanning itself and the worker function gets all the ports from the queue in order to pass them to the portscan function and prints the result. In order to not get confused with the output, we only print when a port is open because we don’t care when a port is closed.\nNow we just have to decide how many threads we want to start and then we can go for it.\nfor x in range ( 30 ):\rt = threading.Thread( target =worker)\rt.start()\rIn this example, we start 30 threads at the same time. If you run this, you will see that it increases the scanning speed a lot. Within a few seconds, all the 500 ports are scanned. So if you want, you can increase the number to 5000.\nThe results for my virtual server are the following:\rPort 25 is open!\rPort 22 is open!\rPort 80 is open!\rPort 110 is open!\rPort 119 is open!\rPort 143 is open!\rPort 443 is open!\rPort 465 is open!\nAs you can see, there are a lot of vulnerabilities here. You now just have to google which ports are interesting and depending on your side you may either prepare for an attack or fix the security breaches. For example port 22 is SSH and quite dangerous.\n\r\r",
    "ref": "/blog/python-networking/"
  },{
    "title": "Python - Queues",
    "date": "",
    "description": "Introduction to Python Queues",
    "body": "\r\r\rQueues\r\rQUEUING RESOURCES\rLIFO QUEUES\rPRIORITIZING QUEUES\r\r\r\rQueues\rIn Python, queues are structures that take in data in a certain order to then output it in a certain order. The default queue type is the so-called FIFO queue . This stands for first in first out and the name describes exactly what it does. The elements that enter the queue first are also the elements that will leave the queue first.\nimport queue\rq = queue.Queue()\rfor x in range ( 5 ):\rq.put(x)\rfor x in range ( 5 ):\rprint (q.get(x))\rIn order to work with queues in Python, we need to import the module queue . We can then create an instance of the class Queue by using the constructor.\nAs you can see, we apply two functions here – put and get . The put function adds an element to the queue that can then be extracted by the get function.\nHere, we keep numbers one to five into our queue. Then, we just get the elements and print them. The order stays the same, since the default queue is FIFO .\nQUEUING RESOURCES\rLet’s say we have a list of numbers that need to be processed. We decide to use multiple threads, in order to speed up the process. But there might be a problem. The threads don’t know which number has already been processed and they might do the same work twice, which would be unnecessary. Also, solving the problem with a counter variable won’t always work, because too many threads access the same variable and numbers might get skipped.\rIn this case we can just use queues to solve our problems. We fill up our queue with the numbers and every thread just uses the get function, to get the next number and process it.\rLet’s assume we have the following worker function:\nimport threading\rimport queue\rimport math\rq = queue.Queue()\rthreads = []\rdef worker():\rwhile True :\ritem = q.get()\rif item is None :\rbreak\rprint (math.factorial(item))\rq.task_done()\rWe start out with an empty queue and an empty list for threads. Our function has an endless loop that gets numbers from the list and calculates the factorial of them. For this factorial function, we need to import the module math . But you can ignore this part, since it is only used because the computation requires a lot of resources and takes time. At the end, we use the function task_done of the queue, in order to signal that the element was processed.\nfor x in range ( 5 ):\rt = threading.Thread( target =worker)\rt.start()\rthreads.append(t)\rzahle = [ 1340000 , 13 , 3, 300 , 98 , 88 , 11 , 23 ]\rfor item in zahle:\rq.put(item)\rq.join()\rfor i in range ( 5 ):\rq.put( None )\rWe then use a for loop to create and start five threads that we also add to our list. After that, we create a list of numbers, which we then all put into the queue.\nThe method join of the queue waits for all elements to be extracted and processed. Basically, it is going to wait for all the task_done functions. After that, we put None elements into the queue, so that our loops break.\nNotice that our threads can’t process the same element twice or even skip one because they can only get them by using the get function.\rIf we would use a counter for this task, two threads might increase it at the same time and then skip an element. Or they could just access the same element simultaneously. Queues are irreplaceable for tasks like this.\n\rLIFO QUEUES\rAlternative to the FIFO queues is LIFO queues . That stands for last in first out . You can imagine this queue like some sort of stack. The element you put last on top of the stack is the first that you can get from it.\nimport queue\rq = queue.LifoQueue()\rnumbers = [ 1 , 2 , 3 , 4 , 5 ]\rfor x in numbers:\rq.put(x)\rwhile not q.empty():\rprint (q.get())\rBy using the LifoQueue class from the queue module, we can create an instance of this type. When we now put in the numbers one to five in ascending order, we will get them back in descending order.\rThe result would be:\r5 4 3 2 1\n\rPRIORITIZING QUEUES\rWhat you can also do in Python, is creating prioritized queues . In these, every element gets assigned a level of priority that determines when they will leave the queue.\nimport queue\rq = queue.PriorityQueue()\rq.put(( 8 , \u0026#39;Some string\u0026#39; ))\rq.put(( 1 , 2023 ))\rq.put(( 90 , True ))\rq.put(( 2 , 10.23 ))\rwhile not q.empty():\rprint (q.get())\rHere, we create a new instance of the class PriorityQueue . When we put a new element into this queue, we need to pass a tuple as a parameter. The first element of the tuple is the level of importance (the lower the number, the higher the priority) and the second element is the actual object or value that we want to put into the queue.\nWhen we execute the print statement of the loop, we get the following results:\n(1, 2023)\r(2, 10.23)\r(8, ‘Some string’)\r(90, True)\nAs you can see, the elements got sorted by their priority number. If you only want to access the actual value, you need to address the index one because it is the second value of the tuple.\rwhile not q.empty():\rprint (q.get()[ 1 ])\n\r\r",
    "ref": "/blog/python-queues/"
  },{
    "title": "Python - Multithreading",
    "date": "",
    "description": "Introduction to Python Multithreading",
    "body": "\r\r\rMultithreading\r\rHow a thread works\r\rhow to start thread\rStart Vs Run\rWaiting for threads\rThread classes\rSynchronizing Threads\r\r\rSemaphores\r\rDaemon Threads\r\r\r\r\rMultithreading\rThreads are lightweight processes that perform certain actions in a program and they are part of a process themselves. These threads can work in parallel with each other in the same way as two individual applications can.\nSince threads in the same process share the memory space for the variables and the data, they can exchange information and communicate efficiently. Also, threads need fewer resources than processes. That’s why they’re often called lightweight processes.\nHow a thread works\rA thread has a beginning or a start, a working sequence and an end. But it can also be stopped or put on hold at any time. The latter is also called sleep .\rThere are two types of threads: Kernel Threads and User Threads . Kernel threads are part of the operating system, whereas user threads are managed by the programmer. That’s why we will focus on user threads in this book.\nIn Python, a thread is a class that we can create instances of. Each of these instances then represents an individual thread which we can start, pause or stop. They are all independent from each other and they can perform different operations at the same time.\nFor example, in a video game, one thread could be rendering all the graphics, while another thread processes the keyboard and mouse inputs. It would be unthinkable to serially perform these tasks one after the other.\nhow to start thread\rIn order to work with threads in Python, we will need to import the respective library threading .\nimport threading\rThen, we need to define our target function. This will be the function that contains the code that our thread shall be executing. Let’s just keep it simple for the beginning and write a hello world function.\nimport threading\rdef hello():\rprint ( \u0026#39;Hello World!\u0026#39; )\rt1 = threading.Thread( target =hello)\rt1.start()\rAfter we have defined the function, we create our first thread. For this, we use the class Thread of the imported threading module. As a parameter, we specify the target to be the hello function. Notice that we don’t put parentheses after our function name here, since we are not calling it but just referring to it. By using the start method we put our thread to work and it executes our function.\n\rStart Vs Run\rIn this example, we used the function start to put our thread to work. Another alternative would be the function run . The difference between these two functions gets important, when we are dealing with more than just one thread.\rWhen we use the run function to execute our threads, they run serially one after the other. They wait for each other to finish. The start function puts all of them to work simultaneously.\rThe following example demonstrates this difference quite well.\nimport threading\rdef function1():\rfor x in range ( 1000 ):\rprint ( \u0026#39;ONE\u0026#39; )\rdef function2():\rfor x in range ( 1000 ):\rprint ( \u0026#39;TWO\u0026#39; )\rt1 = threading.Thread( target =function1)\rt2 = threading.Thread( target =function2)\rt1.start()\rt2.start()\rWhen you run this script, you will notice that the output alternates between ONEs and TWOs . Now if you use the run function instead of the start function, you will see 1000 times ONE followed by 1000 times TWO . This shows you that the threads are run serially and not in parallel.\nOne more thing that you should know is that the application itself is also the main thread, which continues to run in the background. So while your threads are running, the code of the script will be executed unless you wait for the threads to finish.\n\rWaiting for threads\rimport threading\rdef function():\rfor x in range ( 500000 ):\rprint ( \u0026#39;HELLO WORLD!\u0026#39; )\rt1 = threading.Thread( target =function)\rt1.start()\rprint ( \u0026#39;THIS IS THE END!\u0026#39; )\rIf you execute this code, you will start printing the text “HELLO WORLD!” 500,000 times. But what you will notice is that the last print statement gets executed immediately after our thread starts and not after it ends.\nt1 = threading.Thread( target =function)\rt1.start()\rt1.join()\rprint ( \u0026#39;THIS IS THE END!\u0026#39; )\rBy using the join function here, we wait for the thread to finish before we move on with the last print statement. If we want to set a maximum time that we want to wait, we just pass the number of seconds as a parameter.\nt1 = threading.Thread( target =function)\rt1.start()\rt1.join( 5 )\rprint ( \u0026#39;THIS IS THE END!\u0026#39; )\rIn this case, we will wait for the thread to finish but only a maximum of five seconds. After this time has passed we will proceed with the code.\rNotice that we are only waiting for this particular thread. If we would have other threads running at the same time, we would have to call the join function on each of them in order to wait for all of them.\n\rThread classes\rAnother way to build our threads is to create a class that inherits the Thread class. We can then modify the run function and implement our functionality. The start function is also using the code from the run function so we don’t have to worry about that.\nimport threading\rclass MyThread(threading.Thread):\rdef __init__ ( self , message):\rthreading.Thread. __init__ ( self )\rself .message = message\rdef run( self ):\rfor x in range ( 100 ):\rprint ( self .message)\rmt1 = MyThread( \u0026#39;This is my thread message!\u0026#39; )\rmt1.start()\rIt is basically the same but it offers more modularity and structure, if you want to use attributes and additional functions.\n\rSynchronizing Threads\rSometimes you are going to have multiple threads running that all try to access the same resource. This may lead to inconsistencies and problems. In order to prevent such things there is a concept called locking . Basically, one thread is locking all of the other threads and they can only continue to work when the lock is removed.\nI came up with the following quite trivial example. It seems a bit abstract but you can still get the concept here.\nimport threading\rimport time\rx = 8192\rdef halve():\rglobal x\rwhile (x \u0026gt; 1 ):\rx /= 2\rprint (x)\rtime.sleep( 1 )\rprint ( \u0026#39;END!\u0026#39; )\rdef double():\rglobal x\rwhile (x \u0026lt; 16384 ):\rx *= 2\rprint (x)\rtime.sleep( 1 )\rprint ( \u0026#39;END!\u0026#39; )\rt1 = threading.Thread( target =halve)\rt2 = threading.Thread( target =double)\rt1.start()\rt2.start()\rHere we have two functions and the variable x that starts at the value 8192 . The first function halves the number as long as it is greater than one, whereas the second function doubles the number as long as it is less than 16384 .\nAlso, I’ve imported the module time in order to use the function sleep . This function puts the thread to sleep for a couple of seconds (in this case one second). So it pauses. We just do that, so that we can better track what’s happening.\rWhen we now start two threads with these target functions, we will see that the script won’t come to an end. The halve function will constantly decrease the number and the double function will constantly increase it.\nimport threading\rimport time\rx = 8192\rlock = threading.Lock()\rdef halve():\rglobal x, lock\rlock.acquire()\rwhile (x \u0026gt; 1 ):\rx /= 2\rprint (x)\rtime.sleep( 1 )\rprint ( \u0026#39;END!\u0026#39; )\rlock.release()\rdef double():\rglobal x, lock\rlock.acquire()\rwhile (x \u0026lt; 16384 ):\rx *= 2\rprint (x)\rtime.sleep( 1 )\rprint ( \u0026#39;END!\u0026#39; )\rlock.release()\rt1 = threading.Thread( target =halve)\rt2 = threading.Thread( target =double)\rt1.start()\rt2.start()\rSo here we added a couple of elements. First of all we defined a Lock object. It is part of the threading module and we need this object in order to manage the locking.\rNow, when we want to try to lock the resource, we use the function acquire . If the lock was already locked by someone else, we wait until it is released again before we continue with the code. However, if the lock is free, we lock it ourselves and release it at the end using the release function.\rHere, we start both functions with a locking attempt. The first function that gets executed will lock the other function and finish its loop. After that it will release the lock and the other function can do the same.\rSo the number will be halved until it reaches the number one and then it will be doubled until it reaches the number 16384 .\n\r\r\rSemaphores\rSometimes we don’t want to completely lock a resource but just limit it to a certain amount of threads or accesses. In this case, we can use so-called semaphores .\rTo demonstrate this concept, we will look at another very abstract example.\nimport threading\rimport time\rsemaphore = threading.BoundedSemaphore( value = 5 )\rdef access(thread_number):\rprint ( \u0026#39;{}: Trying access...\u0026#39;.format(thread_number))\rsemaphore.acquire()\rprint ( \u0026#39;{}: Access granted!\u0026#39;.format(thread_number))\rprint ( \u0026#39;{}: Waiting 5 seconds...\u0026#39;.format(thread_number))\rtime.sleep( 5 )\rsemaphore.release()\rprint ( \u0026#39;{}: Releasing!\u0026#39;.format(thread_number))\rfor thread_number in range ( 10 ):\rt = threading.Thread( target =access,args =(thread_number,))\rt.start()\rWe first use the BoundedSemaphore class to create our semaphore object. The parameter value determines how many parallel accesses we allow. In this case, we choose five.\rWith our access function, we try to access the semaphore. Here, this is also done with the acquire function. If there are less than five threads utilizing the semaphore, we can acquire it and continue with the code. But when it’s full, we need to wait until some other thread frees up one space.\rWhen we run this code, you will see that the first five threads will immediately run the code, whereas the remaining five threads will need to wait five seconds until the first threads release the semaphore.\rThis process makes a lot of sense when we have limited resources or limited computational power in a system and we want to limit the access to it.\nWith events we can manage our threads even better. We can pause a thread and wait for a certain event to happen, in order to continue it.\nimport threading\revent = threading.Event()\rdef function():\rprint ( \u0026#39;Waiting for event...\u0026#39; )\revent.wait()\rprint ( \u0026#39;Continuing!\u0026#39; )\rthread = threading.Thread( target =function)\rthread.start()\rx = input ( \u0026#39;Trigger event?\u0026#39; )\rif (x == \u0026#39;yes\u0026#39; ):\revent.set()\rTo define an event we use the Event class of the threading module. Now we define our function which waits for our event. This is done with the wait function. So we start the thread and it waits.\rThen we ask the user, if he wants to trigger the event. If the answer is yes, we trigger it by using the set function. Once the event is triggered, our function no longer waits and continues with the code.\nDaemon Threads\rSo-called daemon threads are a special kind of thread that runs in the background. This means that the program can be terminated even if this thread is still running. Daemon threads are typically used for background tasks like synchronizing, loading or cleaning up files that are not needed anymore. We define a thread as a daemon by setting the respective parameter in the constructor for Thread to True .\nimport threading\rimport time\rpath = \u0026#39;text.txt\u0026#39;\rtext = \u0026#39;\u0026#39;\rdef readFile():\rglobal path, text\rwhile True :\rwith open (path) as file:\rtext = file.read()\rtime.sleep( 3 )\rdef printloop():\rglobal text\rfor x in range ( 30 ):\rprint (text)\rtime.sleep( 1 )\rt1 = threading.Thread( target =readFile, daemon = True )\rt2 = threading.Thread( target =printloop)\rt1.start()\rt2.start()\rSo, here we have two functions. The first one constantly reads in the text from a file and saves it into the text variable. This is done in an interval of three seconds. The second one prints out the content of text every second but only 30 times.\nAs you can see, we start the readFile function in a daemon thread and the printloop function in an ordinary thread. So when we run this script and change the content of the text.txt file while it is running, we will see that it prints the actual content all the time. Of course, we first need to create that file manually.\nAfter it printed the content 30 times however, the whole script will stop, even though the daemon thread is still reading in the files. Since the ordinary threads are all finished, the program ends and the daemon thread just gets terminated With locking we can now let one function finish before the next function starts. Of course, in this example this is not very useful but we can do the same thing in much more complex situations.\n\r\r",
    "ref": "/blog/python-multithreading/"
  },{
    "title": "Python - Classes and Objects",
    "date": "",
    "description": "Introduction to Python Classes and Objects",
    "body": "\r\r\rClasses and Objects\r\rCreating Classes\r\rConstructor\rAdding Functions\rClass Variables\rDestructors\rCreating Objects\rHidden Attributes\r\rInheritence\r\r\r\r\rClasses and Objects\rPython is an object-oriented language which means that the code can be divided into individual units, namely objects . Each of these objects is an instance of a so-called class . You can think of the class as some sort of blueprint. For example, the blueprint of a car could be the class and an object would be the actual physical car. So a class has specific attributes and functions but the values vary from object to object.\nCreating Classes\rIn Python, we use the keyword class in order to define a new class. Whatever which is indented after the colon belongs to the class.\nclass Car:\rdef __init__ ( self , manufacturer, model, hp):\rself .manufacturer = manufacturer\rself .model = model\rself .hp = hp\rAfter the class keyword, we put the class name. In this example, this is Car .\nConstructor\rWhat we find first in this case, is a special function called init . This is the so-called constructor. Every time we create an instance or an object of our class, we use this constructor. As you can see, it accepts a couple of parameters. The first one is the parameter self and it is mandatory. Every function of the class needs to have at least this parameter.\rThe other parameters are just our custom attributes. In this case, we have chosen the manufacturer, the model and the horse power (hp).\rWhen we write self.attribute , we refer to the actual attribute of the respective object. We then assign the value of the parameters to it.\n\rAdding Functions\rWe can simply create and add functions to our class that perform certain actions. These functions can also access the attributes of the class.\nclass Car:\rdef __init__ ( self , manufacturer, model, hp):\rself .manufacturer = manufacturer\rself .model = model\rself .hp = hp\rdef print_info( self ):\rprint ( \u0026#39;Manufacturer: {}, Model: {}, HP; {}\u0026#39;.format( self .manufacturer, self.model,self.hp))\rHere we have the function print_info that prints out information about the attributes of the respective object. Notice that we also need the parameter self here.\n\rClass Variables\rIn the following code, you can see that we can use one and the same variable across all the objects of the class, when it is defined without referring to self .\nclass Car:\ramount_cars = 0\rdef __init__ ( self , manufacturer, model, hp):\rself .manufacturer = manufacturer\rself .model = model\rself .hp = hp\rCar.amount_cars += 1\rdef print_car_amount( self ):\rprint ( \u0026#39;Amount: {}\u0026#39;.format(Car.amount_cars)) def print_info( self ):\rprint ( \u0026#39;Manufacturer: {}, Model: {}, HP; {}\u0026#39;.format( self .manufacturer, self.model,self.hp)) \rThe variable amount_cars doesn’t belong to the individual object since it’s not addressed with self . It is a class variable and its value is the same for all objects or instances.\rWhenever we create a new car object, it increases by one. Then, every object can access and print the amount of existing cars.\n\rDestructors\rIn Python, we can also specify a method that gets called when our object gets destroyed or deleted and is no longer needed. This function is called destructor and it is the opposite of the constructor .\nclass Car:\ramount_cars = 0\rdef __init__ ( self , manufacturer, model, hp):\rself.manufacturer = manufacturer\rself.model = model\rself.hp = hp\rCar.amount_cars += 1\rdef __del__ ( self ):\rprint ( \u0026#39;Object gets deleted!\u0026#39; )\rCar.amount_cars -= 1 def print_car_amount( self ):\rprint ( \u0026#39;Amount: {}\u0026#39;.format(Car.amount_cars)) def print_info( self ):\rprint ( \u0026#39;Manufacturer: {}, Model: {}, HP; {}\u0026#39;.format( self .manufacturer, self.model,self.hp)) \rThe destructor function is called del . In this example, we print an informational message and decrease the amount of existing cars by one, when an object gets deleted.\n\rCreating Objects\rNow that we have implemented our class, we can start to create some objects of it.\nmyCar1 = Car( \u0026#39;Tesla1\u0026#39; , \u0026#39;Model X1\u0026#39; , 5251 )\rFirst, we specify the name of our object, like we do with ordinary variables. In this case, the object is called myCar1 . We then create an object of the Car class by writing the class name as a function. This calls the constructor, so we can pass our parameters. We can then use the functions of our car object.\nmyCar1.print_info()\r## Manufacturer: Tesla1, Model: Model X1, HP; 5251\rmyCar1.print_car_amount()\r## Amount: 1\rThe results look like this:\rManufacturer: Tesla, Model: Model X, HP; 525\rAmount: 1\rWhat you can also do is directly access the attributes of an object.\nprint (myCar1.manufacturer)\r## Tesla1\rprint (myCar1.model)\r## Model X1\rprint (myCar1.hp)\r## 5251\rNow we can create some more cars and see how the amount changes.\nmyCar1 = Car( \u0026#39;Tesla1\u0026#39; , \u0026#39;Model X1\u0026#39; , 525 )\r## Object gets deleted!\rmyCar2 = Car( \u0026#39;BMW1\u0026#39; , \u0026#39;X31\u0026#39; , 2001 )\rmyCar3 = Car( \u0026#39;VW1\u0026#39; , \u0026#39;Golf1\u0026#39; , 1001)\rmyCar4 = Car( \u0026#39;Porsche1\u0026#39; , \u0026#39;9111\u0026#39; , 5201 )\rdel myCar3\r## Object gets deleted!\rmyCar1.print_car_amount()\r## Amount: 3\rHere we first create four different car objects. We then delete one of them and finally we print out the car amount. The result is the following:\rObject gets deleted!\rAmount: 3\rNotice that all the objects get deleted automatically when our program ends. But we can manually delete them before that happens by using the del keyword.\n\rHidden Attributes\rIf we want to create hidden attributes that can only be accessed within the class, we can do this with underlines .\nclass MyClass:\rdef __init__ ( self ):\rself.__hidden = \u0026#39;Hello\u0026#39;\rprint ( self .__hidden) # Works\rm1 = MyClass()\r# print (m1.__hidden) # Doesn\u0026#39;t Work \r## Hello\rBy putting two underlines before the attribute name, we make it invisible from outside the class. The first print function works because it is inside of the class. But when we try to access this attribute from the object, we can’t.\n\r\rInheritence\rOne very important and powerful concept of object-oriented programming is inheritance . It allows us to use existing classes and to extend them with new attributes and functions.\rFor example, we could have the parent class which represents a Person and then we could have many child classes like Dancer, Policeman, Artist etc. All of these would be considered a person and they would have the same basic attributes. But they are special kinds of persons with more attributes and functions.\nclass Person:\rdef __init__ ( self , name, age):\rself .name = name\rself .age = age\rdef get_older( self , years):\rself .age += years\rclass Programmer(Person):\rdef __init__ ( self , name, age, language):\rsuper (Programmer, self ). __init__ (name, age)\rself .language = language\rdef print_language( self ):\rprint ( \u0026#39;Favorite Programming Language: {}\u0026#39;.format( self .language)) \r\r\r",
    "ref": "/blog/python-basics-2/"
  },{
    "title": "Python Strings - Basics"",
    "date": "",
    "description": "Introduction to Python Strings",
    "body": "\r\r\rSTRINGS\r\r\rSTRINGS\rA string defines characters sequences. Strings always need to be surrounded by quotation marks. Otherwise the interpreter will not realize that they are meant to be treated like text. The keyword for String in Python is str .\nTraversing a String\r\r# Traversing\rname = \u0026quot;Welcome\u0026quot;\rfor ch in name:\rprint(ch, \u0026#39;-\u0026#39;, end = \u0026#39; \u0026#39;)\r# Reversing a string\r## W - e - l - c - o - m - e -\rname = \u0026quot;Reverse me\u0026quot;\r# The Slice notation in python has the syntax -\r# list[\u0026lt;start\u0026gt;:\u0026lt;stop\u0026gt;:\u0026lt;step\u0026gt;]\r# So, when you do a[::-1], it starts from the end towards the first taking each element. So it reverses a. This is applicable for lists/tuples as well.\rprint(name[::-1])\r## em esreveR\rlgth = len(name)\rfor a in range(-1, (-lgth-1), -1):\rprint(name[a])\r## Split the string into a list of characters, reverse the list, then rejoin into a single string\r## e\r## m\r## ## e\r## s\r## r\r## e\r## v\r## e\r## R\rprint(\u0026#39;\u0026#39;.join(reversed(\u0026quot;Hello world\u0026quot;)))\r## dlrow olleH\r\rChecking identities of two strings\rYou use == when comparing values and is when comparing identities.\nlang = [\u0026#39;Java\u0026#39;,\u0026#39;Python\u0026#39;]\rmore_lang = lang\rprint(lang == more_lang) # -\u0026gt; True\r## True\rprint(lang is more_lang) # -\u0026gt; True\r## True\reven_more_lang = [\u0026#39;Java\u0026#39;,\u0026#39;Python\u0026#39;]\rprint(lang == even_more_lang) #-\u0026gt; True\r## True\rprint(lang is even_more_lang) #-\u0026gt; False\r## False\rprint(id(lang))\r## 593146056\rprint(id(more_lang))\r## 593146056\rprint(id(even_more_lang))\r\r## 593153032\r\rChecking capital letters\rThe istitle() method checks if each word is capitalized.\n\rprint( \u0026#39;The Hilman\u0026#39;.istitle() ) #=\u0026gt; True\r## True\rprint( \u0026#39;The Cat\u0026#39;.istitle() ) #=\u0026gt; False\r## True\rprint( \u0026#39;the rice\u0026#39;.istitle() ) #=\u0026gt; False\r## False\r\rChecking if string contains another\r\rprint( \u0026#39;A\u0026#39; in \u0026#39;The string containing A\u0026#39; ) #=\u0026gt; True\r## True\rprint( \u0026#39;Apple in\u0026#39; in \u0026#39;The string containing A\u0026#39; ) #=\u0026gt; False\r## False\r\rFinding the index of a substring in a string\r\rprint(\u0026#39;The\u0026#39;.find(\u0026#39;The string containing A\u0026#39;))\r\r## -1\r\r\r",
    "ref": "/blog/python-strings/"
  },{
    "title": "Python - Basics",
    "date": "",
    "description": "Introduction to Python.",
    "body": "\r\r\rVariables and Data Types\r\rNUMERICAL DATA TYPES\rSTRINGS\rBOOLEANS\rSEQUENCES\rCREATING VARIABLES\rUSING VARIABLES\rTYPECASTING\r\rPython loops and types\r\r1. For loop\r2. While loop\r3. Sequences - Lists\r4. Sequences - Lists - Operations\r5. Sequences - Lists - Functions\r6. Sequences - Lists - METHODS\r7. Sequences - Tupples\r\r\r\rVariables and Data Types\rVariables and Data types basically are just placeholders for values. In programming, that’s the same. The difference is that we have a lot of different data types, and variables cannot only store values of numbers but even of whole objects.\rIn this post we are going to take a look at variables in Python and the differences of the individual data types. Also, we will talk about type conversions.\nNUMERICAL DATA TYPES\rThe types you probably already know from mathematics are numerical data types. There are different kinds of numbers that can be used for mathematical operations.\n\r\rNUMERICAL DATA TYPES\r\r\r\r\r\rInteger\rint\rwhole number\r\rFloat\rfloat\rfloating point number\r\rComplex\rcomplex\rcomplex number\r\r\r\rAs you can see, it’s quite simple. An integer is just a regular whole number, which we can do basic calculations with. A float extends the integer and allows decimal places because it is a floating point number. And a complex number is what just a number that has a real and an imaginary component. If you don’t understand complex numbers mathematically, forget about them. You don’t need them for your programming right now.\n\rSTRINGS\rA string defines characters sequences. Strings always need to be surrounded by quotation marks. Otherwise the interpreter will not realize that they are meant to be treated like text. The keyword for String in Python is str .\nTraversing a String\r\r# Traversing\rname = \u0026quot;Welcome\u0026quot;\rfor ch in name:\rprint(ch, \u0026#39;-\u0026#39;, end = \u0026#39; \u0026#39;)\r# Reversing a string\r## W - e - l - c - o - m - e -\rname = \u0026quot;Reverse me\u0026quot;\r# The Slice notation in python has the syntax -\r# list[\u0026lt;start\u0026gt;:\u0026lt;stop\u0026gt;:\u0026lt;step\u0026gt;]\r# So, when you do a[::-1], it starts from the end towards the first taking each element. So it reverses a. This is applicable for lists/tuples as well.\rprint(name[::-1])\r## em esreveR\rlgth = len(name)\rfor a in range(-1, (-lgth-1), -1):\rprint(name[a])\r## Split the string into a list of characters, reverse the list, then rejoin into a single string\r## e\r## m\r## ## e\r## s\r## r\r## e\r## v\r## e\r## R\rprint(\u0026#39;\u0026#39;.join(reversed(\u0026quot;Hello world\u0026quot;)))\r## dlrow olleH\r\r\rBOOLEANS\rBoolean are the most simple data type in Python. They can only have one of two values, namely True or False . It’s a binary data type. We will use it a lot when we get to conditions and loops. The keyword here is bool .\n\rSEQUENCES\r\r\rSEQUENCE TYPES\r\r\r\r\r\rList\rlist\rColection of values\r\rTuple\rtuple\rImutable list\r\rDictionary\rdict\rList of key nd value pairs\r\r\r\r\rCREATING VARIABLES\rCreating variables in Python is very simple. We just choose a name and assign a value.\nmyNumber = 10\rmyText = \u0026#39;Hello\u0026#39; \rHere, we defined two variables. The first one is an integer and the second one a string. You can basically choose whatever name you want but there are some limitations. For example you are not allowed to use reserved keywords like int or dict . Also, the name is not allowed to start with a number or a special character other than the underline.\n\rUSING VARIABLES\rNow that we have defined our variables, we can start to use them. For example, we could print the values.\nprint (myNumber)\r## 10\rprint (myText)\r## Hello\rSince we are not using quotation marks, the text in the parentheses is treated like a variable name. Therefore, the interpreter prints out the values 10 and “Hello” .\n\rTYPECASTING\rSometimes, we will get a value in a data type that we can’t work with properly. For example we might get a string as an input but that string contains a number as its value. Here “10” is not same to 10 . We can’t do calculations with a string, even if the text represents a number. For that reason we need to typecast.\nvalue = \u0026#39;10\u0026#39;\rnumber = int (value)\rTypecasting is done by using the specific data type function. In this case we are converting a string to an integer by using the int keyword. You can also reverse this by using the str keyword. This is a very important thing and we will need it quite often.\n\r\rPython loops and types\r1. For loop\rnumbers = [10,20,30,40]\rfor num in numbers:\rprint(num)\r\r## 10\r## 20\r## 30\r## 40\rfor num in range(10,41,10):\rprint(num)\r## 10\r## 20\r## 30\r## 40\r\r2. While loop\rnumber = 0\rwhile number \u0026lt; 10:\rnumber += 1\rif number == 5:\rbreak\rprint(number)\r## 1\r## 2\r## 3\r## 4\rnum = 0\rwhile num \u0026lt; 10:\rnum += 1\rif num == 5:\rcontinue\rprint(num)\r## 1\r## 2\r## 3\r## 4\r## 6\r## 7\r## 8\r## 9\r## 10\r\r3. Sequences - Lists\rnumbers = [10, 20, 30 ,40]\rnames = [\u0026#39;Arun\u0026#39;,\u0026#39;Varun\u0026#39;,\u0026#39;Karun\u0026#39;]\rmixed = [10,\u0026#39;Arun\u0026#39;, 28.3,True ]\rprint(numbers[3])\r## 40\rprint(names[0])\r## Arun\rprint(mixed[3])\r## True\rnumbers[3] = 3\rnames[2] = \u0026#39;Bob\u0026#39;\rprint(numbers[3])\r## 3\rprint(names[2])\r## Bob\r\r4. Sequences - Lists - Operations\r\r\rLIST OPERATIONS\r\r\r\r\rOPERATION\rRESULT\r\r[10, 20, 30] + [40, 50, 60]\r[10, 20, 30, 40, 50, 60]\r\r[10, “Bob”] * 3\r[10, “Bob”, 10, “Bob”, 10, “Bob”]\r\r\r\r\r5. Sequences - Lists - Functions\r\r\rLIST FUNCTIONS\r\r\r\r\rFUNCTION\rDESCRIPTION\r\rlen(list)\rReturns the length of a list\r\rmax(list)\rReturns the item with maximum value\r\rmin(list)\rReturns the item with minimum value\r\rlist(element)\rTypecasts element into list\r\r\r\r\r6. Sequences - Lists - METHODS\r\r\rLIST METHODS\r\r\r\r\rMETHOD\rDESCRIPTION\r\rlist.append(x)\rAppends element to the list\r\rlist.count(x)\rCounts how many times an element appears in the list\r\rlist.index(x)\rReturns the first index at which the given element occurs\r\rlist.pop()\rRemoves and returns last element\r\rlist.reverse()\rReverses the order of the elements\r\rlist.sort()\rSorts the elements of a list\r\r\r\r\r7. Sequences - Tupples\rtpl = (10,20,30)\rlen(tpl)\r## 3\rmax(tpl)\r## 30\rmin(tpl)\r## 10\r\r\r",
    "ref": "/blog/python-basics/"
  },{
    "title": "Kontakt",
    "date": "",
    "description": "",
    "body": "",
    "ref": "/contact/"
  }]

[{
    "title": "Convolutional Neural Networks",
    "date": "",
    "description": "One of the important areas of Deep learning is Convolutional Neural Network. CNN deals in analysing visual images. These types of neural networks works by processing, classifying and segmenting images. CNN algorithm learns about images and then are able to predict about a given image when present.",
    "body": "\r\r\rConvolutional Neural Networks\rWhat are Convolutional Neural Networks:\rOne of the important areas of Deep learning is Convolutional Neural Network. CNN deals in analysing visual images. These types of neural networks works by processing, classifying and segmenting images. CNN algorithm learns about images and then are able to predict about a given image when present. CNN algorithms requires to be trained with tons of images and their possible predictor class. CNN are powerful tools for processing data having grid like topology. CNN involves convolution which means cross-corelation operations (instead of a fully connected layer) as one of its layers.\nThese neural networks are successful in many different real-life case studies and applications like:\n\rImage classification, Object detection, segmentation, face recognition\rSelf driving cars that leverage CNN based vision systems\rClassificationof crystal structure using a convolutional neural network\r\rConvolution is the key principle applied: in convolutional neural network architecture.\rConvolution is a way to identify patterns in data that is directly tied to space or time. Assume we have a one dimentional array of numbers\rsay Input = [a,b,c,d,e,f,g] and another set of numbers say K = [p,q]. Then convolution involves sliding the lower size list over the higher size list and then multiplying corresponding values and adding them therefore after convoluting K over Input we get [ap+bq, bp+cq, cp+dq, dp+eq, fp+gq]. The result identifies one of the feature of the Input. The result is known as the convolution of the Input and in practice only non-zero values are choosen for feature selection.If the input is a grid structure or in matrix form (in case of images) then the kernal is also choosen as the matrix of lower dimension and in this case matrix multiplication is perfomed to get the resultant convolution of the input image.\nComputer see images as matrices:Grayscale images have single channel (gray).\rSo, we can represent grayscale images in the form of 2D matrix, where each element represents the intensity of brightness in that particular pixel,\rwhere 0 means black and 255 means white. Color images have 3 channels RGB (red, green, blue).\nColor images can be represented as a 3D matrix with the depth of 3.\nFor example: Shape of a matrix representing a 480px by 852px color image will be (480, 852, 3)\rEach pixel of the color image has three numbers (ranging from 0 to 255) associated with it.\rThese numbers shows the intensity of red, green and blue color in that particular pixel.\nimport os\rimport cv2\rimport matplotlib.pyplot as plt # (optional) for plotting and showing images inline\rIMAGES_FOLDER = os.path.join(\u0026#39;../../static/img/main\u0026#39;) # images for visuals\rearth_fname = os.path.join(IMAGES_FOLDER,\u0026#39;earth.jpg\u0026#39;)\rearth_img1 = cv2.imread(earth_fname)\rprint(earth_img1.shape)\r## (480, 852, 3)\rThe CIFAR 10 dataset has the Input layer of 60000 32x32 colour images in 10 categories, with 6000 images per class.\n\rIn a covnolutional neural network:\rInput layer: Takes an image as input and preserves its spatial structure\rConvolution layer: extracts feature maps from the input, each responding to a specific pattern\rReLU layer: Introduces non-linearities in the network by putting the negative pixels to 0.\nKernel, stride and padding\nFilters also known as kernals, convolve square blocks of pixels into scalars in subsequent convolutional layers. In the animation above, we have a 3 x 3 filter with ones running on the diagonal and off-diagonal, scanning an image from left to right, top to bottom.\nThroughout the process, the filter performs element-wise multiplication and sums up all products, into a single value passed to the subsequent convolutional layer. Note that the filter is moving a pixel at a time. This is the stride, the stepsize of the sliding window the filter uses to convolve. Larger size of strides indicates more granular and smaller convolved features.\nExample Convolution layer in Python\nThe first layer takes input as set of images specified with input_shape. filters, kernal size, strides and padding  are the most important\rparameters to keras Cov2D. The parameter filters denote the number of filters. The task of a filter is to detect a feature in the image.\nfrom keras.models import Sequential\r## Using TensorFlow backend.\r## C:\\Users\\slaxm\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or \u0026#39;1type\u0026#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0026#39;(1,)type\u0026#39;.\r## _np_qint8 = np.dtype([(\u0026quot;qint8\u0026quot;, np.int8, 1)])\r## C:\\Users\\slaxm\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or \u0026#39;1type\u0026#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0026#39;(1,)type\u0026#39;.\r## _np_quint8 = np.dtype([(\u0026quot;quint8\u0026quot;, np.uint8, 1)])\r## C:\\Users\\slaxm\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or \u0026#39;1type\u0026#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0026#39;(1,)type\u0026#39;.\r## _np_qint16 = np.dtype([(\u0026quot;qint16\u0026quot;, np.int16, 1)])\r## C:\\Users\\slaxm\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or \u0026#39;1type\u0026#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0026#39;(1,)type\u0026#39;.\r## _np_quint16 = np.dtype([(\u0026quot;quint16\u0026quot;, np.uint16, 1)])\r## C:\\Users\\slaxm\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or \u0026#39;1type\u0026#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0026#39;(1,)type\u0026#39;.\r## _np_qint32 = np.dtype([(\u0026quot;qint32\u0026quot;, np.int32, 1)])\r## C:\\Users\\slaxm\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or \u0026#39;1type\u0026#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0026#39;(1,)type\u0026#39;.\r## np_resource = np.dtype([(\u0026quot;resource\u0026quot;, np.ubyte, 1)])\r## C:\\Users\\slaxm\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or \u0026#39;1type\u0026#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0026#39;(1,)type\u0026#39;.\r## _np_qint8 = np.dtype([(\u0026quot;qint8\u0026quot;, np.int8, 1)])\r## C:\\Users\\slaxm\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or \u0026#39;1type\u0026#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0026#39;(1,)type\u0026#39;.\r## _np_quint8 = np.dtype([(\u0026quot;quint8\u0026quot;, np.uint8, 1)])\r## C:\\Users\\slaxm\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or \u0026#39;1type\u0026#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0026#39;(1,)type\u0026#39;.\r## _np_qint16 = np.dtype([(\u0026quot;qint16\u0026quot;, np.int16, 1)])\r## C:\\Users\\slaxm\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or \u0026#39;1type\u0026#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0026#39;(1,)type\u0026#39;.\r## _np_quint16 = np.dtype([(\u0026quot;quint16\u0026quot;, np.uint16, 1)])\r## C:\\Users\\slaxm\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or \u0026#39;1type\u0026#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0026#39;(1,)type\u0026#39;.\r## _np_qint32 = np.dtype([(\u0026quot;qint32\u0026quot;, np.int32, 1)])\r## C:\\Users\\slaxm\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or \u0026#39;1type\u0026#39; as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \u0026#39;(1,)type\u0026#39;.\r## np_resource = np.dtype([(\u0026quot;resource\u0026quot;, np.ubyte, 1)])\rfrom keras.layers import Conv2D\rmodel = Sequential()\rmodel.add(Conv2D(filters=16, kernel_size = 3, padding = \u0026#39;same\u0026#39;,activation = \u0026#39;relu\u0026#39;,input_shape=(32,32,3)))\rmodel.summary()\r## Model: \u0026quot;sequential_1\u0026quot;\r## _________________________________________________________________\r## Layer (type) Output Shape Param # ## =================================================================\r## conv2d_1 (Conv2D) (None, 32, 32, 16) 448 ## =================================================================\r## Total params: 448\r## Trainable params: 448\r## Non-trainable params: 0\r## _________________________________________________________________\rExample Convolution layer in R\nlibrary(tensorflow)\rlibrary(keras)\rmodel \u0026lt;- keras_model_sequential()\rmodel%\u0026gt;% layer_conv_2d(filters = 16, kernel_size = c(3,3),activation = \u0026#39;relu\u0026#39;,input_shape = c(32,32,3))\rsummary(model)\r## Model: \u0026quot;sequential\u0026quot;\r## ________________________________________________________________________________\r## Layer (type) Output Shape Param # ## ================================================================================\r## conv2d (Conv2D) (None, 30, 30, 16) 448 ## ================================================================================\r## Total params: 448\r## Trainable params: 448\r## Non-trainable params: 0\r## ________________________________________________________________________________\rpooling layer: Down-samples the rectified feature maps, thus reducing the spatial dimensionality and retaining important features.\rThis prevents overfitting.\nExample max pooling layer in R\nmodel %\u0026gt;% layer_max_pooling_2d(pool_size = c(2,2)) %\u0026gt;%\rlayer_dropout(0.25)\rsummary(model)\r## Model: \u0026quot;sequential\u0026quot;\r## ________________________________________________________________________________\r## Layer (type) Output Shape Param # ## ================================================================================\r## conv2d (Conv2D) (None, 30, 30, 16) 448 ## ________________________________________________________________________________\r## max_pooling2d (MaxPooling2D) (None, 15, 15, 16) 0 ## ________________________________________________________________________________\r## dropout (Dropout) (None, 15, 15, 16) 0 ## ================================================================================\r## Total params: 448\r## Trainable params: 448\r## Non-trainable params: 0\r## ________________________________________________________________________________\rExample max pooling layer in Python\nfrom keras.layers import MaxPooling2D\rfrom keras.layers import Dropout\rmodel.add(MaxPooling2D(pool_size=(2, 2)))\r## WARNING:tensorflow:From C:\\Users\\slaxm\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\rmodel.add(Dropout(0.25))\rmodel.summary()\r## Model: \u0026quot;sequential_1\u0026quot;\r## _________________________________________________________________\r## Layer (type) Output Shape Param # ## =================================================================\r## conv2d_1 (Conv2D) (None, 32, 32, 16) 448 ## _________________________________________________________________\r## max_pooling2d_1 (MaxPooling2 (None, 16, 16, 16) 0 ## _________________________________________________________________\r## dropout_1 (Dropout) (None, 16, 16, 16) 0 ## =================================================================\r## Total params: 448\r## Trainable params: 448\r## Non-trainable params: 0\r## _________________________________________________________________\rFully-Connected layer: Learns non-linear combinations of the features and performs the classification task\rFlatteningFlattening converts last convolutional layer into a one-dimensional NN layer.\nExample\nfrom keras.layers import Dense, Flatten\rfrom keras.layers.normalization import BatchNormalization\rfrom keras.layers import Dense, Activation\rmodel.add(Flatten())\rmodel.add(Dense(512, kernel_initializer=\u0026quot;uniform\u0026quot;))\rmodel.add(Activation(\u0026quot;relu\u0026quot;))\rmodel.add(BatchNormalization())\rmodel.add(Dropout(0.5))\r# softmax classifier\rmodel.add(Activation(\u0026quot;softmax\u0026quot;))\rObject Detection using Convolutional Neural Network\nWe are going to use another Keras dataset, which contains numerous images of ten different categories. These are the following:\n[‘Plane’ , ‘Car’ , ‘Bird’ , ‘Cat’ , ‘Deer’ , ‘Dog’ , ‘Frog’ , ‘Horse’ , ‘Ship’ , ‘Truck’ ]\nThis dataset contains tens of thousands of images of different objects with their respective class. Our goal here is to train a convolutional neural network on that data, in order to then classify other images that the model has never seen before.\nImporting libraries\nimport cv2 as cv\rimport numpy as np\rimport matplotlib.pyplot as plt\rfrom tensorflow.keras import datasets, layers, models\r(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\rtrain_images, test_images = train_images / 255.0 , test_images / 255.0\rThis time we load the cifat10 dataset with the load_data method. We also normalize this data immediately after that, by dividing all values by 255. Since we are dealing with RGB values, and all values lie in between 0 and 255, we end up with values in between 0 and 1.\nNext, we define the possible class names in a list, so that we can label the final numerical results later on. The neural network will again produce a softmax result, which means that we will use the argmax function, to figure out the class name.\nclass_names = [ \u0026#39;Plane\u0026#39; , \u0026#39;Car\u0026#39; , \u0026#39;Bird\u0026#39; , \u0026#39;Cat\u0026#39; , \u0026#39;Deer\u0026#39; ,\r\u0026#39;Dog\u0026#39; , \u0026#39;Frog\u0026#39; , \u0026#39;Horse\u0026#39; , \u0026#39;Ship\u0026#39; , \u0026#39;Truck\u0026#39; ]\rNow we can visualize a section of the data, to see what this dataset looks like.\nFor this we run a for loop with 16 iterations and create a 4x4 grid of subplots. The x-ticks and the y-ticks will be set to empty lists, so that we don’t have annoying coordinates. After that, we use the imshow method, to visualize the individual images. The label of the image will then be the respective class name.\nThis dataset contains a lot of images.\nHere for example we only use the first 20,000 of the training images and the first 4,000 of the test images. Of course your model will be way more accurate if you use all the images. However, for weak computers this might take forever.\nBUILDING NEURAL NETWORK\nNow that we have prepared our data, we can start building the neural network.\nmodel = models.Sequential()\rmodel.add(layers.Conv2D( 32 , ( 3 , 3 ), activation = \u0026#39;relu\u0026#39; ,\rinput_shape =( 32 , 32 , 3 )))\rmodel.add(layers.MaxPooling2D(( 2 , 2 )))\rmodel.add(layers.Conv2D( 64 , ( 3 , 3 ), activation = \u0026#39;relu\u0026#39; ))\rmodel.add(layers.MaxPooling2D(( 2 , 2 )))\rmodel.add(layers.Conv2D( 64 , ( 3 , 3 ), activation = \u0026#39;relu\u0026#39; ))\rmodel.add(layers.Flatten())\rmodel.add(layers.Dense( 64 , activation = \u0026#39;relu\u0026#39; ))\rmodel.add(layers.Dense( 10 , activation = \u0026#39;softmax\u0026#39; ))\rHere we again define a Sequential model. Our inputs go directly into a convolutional layer (Conv2D ). This layer has 32 filters or channels in the shape of 3x3 matrices. The activation function is the ReLU function, which we already know and the input shape is 32x32x3. This is because we our images have a resolution of 32x32 pixels and three layers because of the RGB colors. The result is then forwarded into a MaxPooling2D layer that simplifies the output. Then the simplified output is again forwarded into the next convolutional layer. After that into another max-pooling layer and into another convolutional layer. This result is then being flattened by the Flatten layer, which means that it is transformed into a one-dimensional vector format. Then we forward the results into one dense hidden layer before it finally comes to the softmax output layer. There we find the final classification probabilities.\nTRAINING AND TESTING\nNow we are almost done. We just need to train and test the model before we can use it.\nmodel.compile( optimizer = \u0026#39;adam\u0026#39; ,\rloss = \u0026#39;sparse_categorical_crossentropy\u0026#39; ,\rmetrics =[ \u0026#39;accuracy\u0026#39; ])\rHere we again use the adam optimizer and the sparse categorical crossentropy loss function.\nmodel.fit(train_images,\rtrain_labels,\repochs = 10 ,\rvalidation_data =(test_images, test_labels))\rWe now train our model on our training data in ten epochs. Remember: This means that our model is going to see the same data ten times over and over again.\ntest_loss, test_acc = model.evaluate(test_images,\rtest_labels,\rverbose = 2 )\rWe use the evaluate function to test our model and get the loss and accuracy values. We set the parameter verbose to 2, so that we get as much information as possible.\n\r1s - loss: 0.8139 - acc: 0.7090\r\rCLASSIFYING OWN IMAGES\nHowever, the interesting part starts now. Since our model is trained, we can now go ahead and use our own images of cars, planes, horses etc. for classification.\nThe important thing is that we get these images down to 32x32 pixels because this is the required input format of our model. For this you can use any software like Gimp or Paint. You can either crop the images or scale them.\nNow we just have to load these images into our script, using OpenCV.\nimg1 = cv.imread( \u0026#39;car.jpg\u0026#39; )\rimg1 = cv.cvtColor(img1, cv.COLOR_BGR2RGB)\rimg2 = cv.imread( \u0026#39;horse.jpg\u0026#39; )\rimg2 = cv.cvtColor(img2, cv.COLOR_BGR2RGB)\rplt.imshow(img1, cmap =plt.cm.binary)\rplt.show()\rThe function imread loads the image into our script. Then we use the cvtColor method, in order to change the default color scheme of BGR (blue, green, red) to RGB (red, green, blue).\nplt.imshow(img1, cmap =plt.cm.binary)\rplt.show()\rWith the imshow function, we can show the image in our script, using Matplotlib.\nWe can now use the loaded images as the input for our model, in order to get a prediction.\nprediction = model.predict(np.array([img1]) / 255 )\rindex = np.argmax(prediction)\rprint (class_names[index])\rFirst we use the predict function to get the softmax result. Notice that we are converting our image into a NumPy array and dividing it by 255. This is because we need to normalize it, since our model was trained on normalized values. Then we use the argmax function to get the index of the highest softmax activation value. Finally, we print the class name of that index as a result.\nCar Horse\nThe results speak for themselves. These pictures were classified absolutely correct.\n\r\r",
    "ref": "/blog/convolutional-neural-networks/"
  },{
    "title": "Neural Network using Make Moons dataset",
    "date": "",
    "description": "The make_moons dataset is a swirl pattern, or two moons. It is a set of points in 2D making two interleaving half circles. It displays 2 disjunctive clusters of data in a 2-dimensional representation space ( with coordinates x1 and x2 for two features). The areas are formed like 2 moon crescents as shown in the figure below.",
    "body": "\r\r\rNeural Network using Make Moons dataset\rMake moons dataset\rThe make_moons dataset is a swirl pattern, or two moons. It is a set of points in 2D making two interleaving half circles.\rIt displays 2 disjunctive clusters of data in a 2-dimensional representation space ( with coordinates x1 and x2 for two features). The areas are formed like 2 moon crescents as shown in the figure below.\n\rImporting libraries\rfrom sklearn import datasets import numpy as np import matplotlib.pyplot as plt\rfrom sklearn.model_selection import train_test_split\rnumpy is used for scientific computing with Python. It is one of the fundamental packages you will use. Matplotlib library is used in Python for plotting graphs. The datasets package is the place from where you will import the make moons dataset. Sklearn library is used fo scientific computing. It has many features related to classification, regression and clustering algorithms including support vector machines.\n\rInitializing the dataset\rnp.random.seed(0)\rfeature_set_x, labels_y = datasets.make_moons(100, noise=0.10)\rX_train, X_test, y_train, y_test = train_test_split(feature_set_x, labels_y, test_size=0.33, random_state=42)\rIn the script above we import the datasets class from the sklearn library. To create non-linear dataset of 100 data-points, we use the make_moons method and pass it 100 as the first parameter. The method returns a dataset, which when plotted contains two interleaving half circles. Data cannot be separated by a single straight line, hence the perceptron cannot be used to correctly classify make moons dataset.\n\rBuild the neural network model using keras\rfrom keras.models import Sequential\rmodel = Sequential()\rBasically there are two types of models in Keras. Sequential and Functional.\nThe sequential API is generally used and helps in creating the models layer-by-layer for most problems. It is constrained in that it does not allow us to create models that share layers or have multiple inputs or outputs.\nThe functional API helps in creating models that have a lot more flexibility as we can easily define models where layers connect to more than just the previous and next layers. In fact, we can connect layers to any other layer. As a result, creating complex networks become possible.\nfrom keras.layers import Dense, Activation\rA dense layer is a Layer in which Each Input Neuron is connected to the output Neuron, like a Simple neural net, the parameters units just tells you the dimensionnality of your Output.\nmodel.add(Dense(50, input_dim=2, activation=\u0026#39;relu\u0026#39;))\rmodel.add(Dense(1, activation=\u0026#39;sigmoid\u0026#39;))\rSample neural network using two hidden layers\nFor example if you say Dense(50, input_dim = 2..) this means that input neurons having dimension of 2 i.e 2 input neurons are connected to dense layer having 50 neurons and at each of the neuron in dense layer is applying activation function as relu for computing the output of that neuron. These 50 neurons in the hidden layer are then connected to one neuron in the output layer with activation function of sigmoid.\nmodel.compile(loss=\u0026#39;binary_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics= [\u0026#39;accuracy\u0026#39;])\rThe output that we are trying to predict is having 2 classes therefore we are using ‘Binary Crossentropy’ loss function.\rAdam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models.\rAdam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.\nprint(model.summary())\r## Model: \u0026quot;sequential_1\u0026quot;\r## _________________________________________________________________\r## Layer (type) Output Shape Param # ## =================================================================\r## dense_1 (Dense) (None, 50) 150 ## _________________________________________________________________\r## dense_2 (Dense) (None, 1) 51 ## =================================================================\r## Total params: 201\r## Trainable params: 201\r## Non-trainable params: 0\r## _________________________________________________________________\r## None\rThe model summary gives the number of parameters input to the model. Theare are total 201 parameters including the parameters from the bias unit. The two input units and one bias unit are fed via connection links to 50 neurons totaling 150 parameters, there after the hidden layer is connected to the 1 output and 1 bias unit also is connected to output neuron. Thus the computation of the paraters will be\r150(Input, bias unit to hidden layer with 50 neurons) + 50(hidden layer to output) + 1 (bias unit in the output) = 201.\n\rTrain and evaluate the model\r\rresults = model.fit(X_train, y_train , nb_epoch=100)\rOnce the model is compiled then we use the training data to train the model via the fit function having parameters such as features set and the target label and number of iterations. Once the training is complete we evaluate the model to determine the loss and the accuracy.\nscore = model.evaluate(X_test, y_test, verbose=0)\rprint(\u0026#39;Test score:\u0026#39;, score[0])\r## Test score: 0.30231634956417663\rprint(\u0026#39;Test accuracy:\u0026#39;, score[1])\r## Test accuracy: 0.8787878751754761\rThe lower the loss, the better a model (unless the model has over-fitted to the training data). The loss is calculated on training and validation and its interpretation is how well the model is doing for these two sets. Unlike accuracy, loss is not a percentage. It is a summation of the errors made for each example in training or validation sets.\nThe accuracy of a model is usually determined after the model parameters are learned and fixed and no learning is taking place. Then the test samples are fed to the model and the number of mistakes (zero-one loss) the model makes are recorded, after comparison to the true targets. Then the percentage of miss classification is calculated.\nFor example, if the number of test samples is 1000 and model classifies 875 of those correctly, then the model’s accuracy is 87.5%.\n\rEvaluate prediction accuracy\rfrom sklearn import metrics\rprediction_values = model.predict_classes(X_test)\rprint(metrics.confusion_matrix(y_test, prediction_values))\r## [[14 2]\r## [ 2 15]]\rprint(metrics.classification_report(y_test, prediction_values))\r## precision recall f1-score support\r## ## 0 0.88 0.88 0.88 16\r## 1 0.88 0.88 0.88 17\r## ## accuracy 0.88 33\r## macro avg 0.88 0.88 0.88 33\r## weighted avg 0.88 0.88 0.88 33\rClassification report is used to evaluate a model’s predictive power. It provides Precision, Recall,F1-score,Support that will help in evaluating the model.\nWe can see here that on average the model has predicted 88% of the classification correctly.For Class 0 it has predicted 88% of the test data correctly.\nClassification_report is also useful when comparing two models with different specifications against each other and determining which model is better to use.\n\r\r",
    "ref": "/blog/neural-network-using-make-moons-dataset/"
  },{
    "title": "Single Layer Perceptron",
    "date": "",
    "description": "A Single layer perceptron is a type of neuron having multiple inputs and one output. Input has many dimensions i.e input  can be a vector for example  input x = ( I1, I2, .., In). Input nodes are connected to a node in the next layer. The node in the next layer takes the weighted sum of all its inputs.",
    "body": "\r\r\rSingle Layer Perceptron\rWhat is Single Layer Perceptron:\rA Single layer perceptron is a type of neuron having multiple inputs and one output. Input has many dimensions i.e input\rcan be a vector for example input x = ( I1, I2, .., In). Input nodes are connected to a node in the next layer. The\rnode in the next layer takes the weighted sum of all its inputs.\nfor example input x = ( I1, I2, I3) = ( 5, 3.2, 0.1 )\n\\[Summedinput = 5 w_1 + 3.2 w_2 + 0.1w_3 \\]\nThe output node having a threshold t. If summed input ≥ threshold, then it outputs y = 1 else it output y = 0.\nThe single layer perceptron does not have a previous knowledge, therefore the initial weights are assign randomly.\rSLP adds all of the weighted inputs and if the addition is above the threshold (any predetermined value), SLP is known to be in the activated state i.e output=1.\nThe perceptron receives the input values and does calculations to find the predicted output. If the predicted output is the same as that of the expected output, then the performance is considered satisfactory and no changes to the weights are made. But, if the predicted output does not match the expected output, then the weights need to be adjusted to reduce the error.\n\rThe algorithm is\r• Initially, assigning all the weights to some random values\n• Repeating (for many epochs):\nFeed the network with an input from one of the examples in the training set\n\rCompute the error between the output of the network and the desired output\n\rCorrect the error by adjusting the weights of the nodes\n\r\r• Until the error is very small\n]\nA single layer percepron is simplest form of network to solve a problem with step or linear activation functions.\n\rSingle Layer Perceptron for XNOR problem in R\rlibrary(neuralnet)\rXOR \u0026lt;- c(0,1,1,0)\rxor.data \u0026lt;- data.frame(expand.grid(c(0,1), c(0,1)), XOR)\rprint(xor.data)\rprint(net.xor \u0026lt;- neuralnet(XOR~Var1+Var2, xor.data, hidden=0, rep=5))\rplot(net.xor, rep=\u0026quot;best\u0026quot;)\rround(predict(net.xor, data.frame(xor.data)))\r\rImplementing Single Layer Perceptron for XNOR problem in Python\rimport numpy as np\rfrom keras.models import Sequential\rfrom keras.layers.core import Dense\rWe import numpy and alias it as np\nKeras offers two different APIs to construct a model: a functional and a sequential one. We’re using the sequential API hence the second import of Sequential from keras.models.\nNeural networks consist of different layers where input data flows through and gets transformed on its way. There are a bunch of different layer types available in Keras. These different types of layer help us to model individual kinds of neural nets for various machine learning tasks. In our specific case the Dense layer is what we want.\n# the four different states of the XOR gate\rtraining_data = np.array([[0,0],[0,1],[1,0],[1,1]], \u0026quot;float32\u0026quot;)\r# the four expected results in the same order\rtarget_data = np.array([[0],[1],[1],[0]], \u0026quot;float32\u0026quot;)\rWe initialize training_data as a two-dimensional array (an array of arrays) where each of the inner arrays has exactly two items.\nWe setup target_data as another two-dimensional array. All the inner arrays in target_data contain just a single item though. Each inner array of training_data relates to its counterpart in target_data.\nmodel = Sequential()\rmodel.add(Dense(1, input_dim=2, activation=\u0026#39;sigmoid\u0026#39;))\rSets up an empty model using the Sequential API. Add a Dense layer to our model in which We set input_dim=2 because each of our input samples is an array of length 2 ([0, 1], [1, 0] etc.). 1 stand the dimension of the output for this layer. Our model means that we have two input neurons (input_dim=2) spreading into 1 neuron in output layer without any hidden layer as we are trying to mimic a single layer perceptron.\nmodel.compile(loss=\u0026#39;mean_squared_error\u0026#39;,\roptimizer=\u0026#39;adam\u0026#39;,\rmetrics=[\u0026#39;binary_accuracy\u0026#39;])\r]\nWith neural nets we always want to calculate a number (loss) that tells us how bad our model performs and then try to get that number lower.\rMean_squared_error works as our loss function simply because it’s a well proven loss function. Then adam optimizer is usded to find the right adjustments for the weights. Last parameter metrics is the binary_accuracy which gives us access to a number that tells us exactly how accurate our predictions are.\nmodel.fit(training_data, target_data, nb_epoch=500, verbose=2)\rWe kick off the training by calling model.fit(…) which require first two params, which are training and target data, the third one is the number of epochs (learning iterations), the last one tells keras how much info to print out during the training.\nprint(model.predict(training_data).round())\rUsing model.predict we can do predictions.\n\rSummary:\rMcCulloch-Pitts neurons networks are computational devices, which are capable of performing any logical function. Single-Layer Perceptrons\rwith step activation functions are constrained in what they can do. Adding additional hidden layers to the network will make it more powerful such that even non-linear relations can be predicted using such a neural network.\n\r\r",
    "ref": "/blog/single-layer-perceptron/"
  },{
    "title": "Neural Networks",
    "date": "",
    "description": "Artificial neural networks are mathematical structures that are inspired by the human brain They consist of so-called neurons , which are interconnected with each other. The human brain consists of multiple billions of such neurons. Artificial neural networks use a similar principle.",
    "body": "\r\r\rArtificial Neural Networks\rArtificial neural networks are mathematical structures that are inspired by the human brain\rThey consist of so-called neurons , which are interconnected with each other.\rThe human brain consists of multiple billions of such neurons. Artificial neural networks use a similar principle.\nSTRUCTURE OF A NEURAL NETWORK\rThe structure of a neural network is quite simple. In the figure below, we can see multiple layers.\rThe first one is the input layer and the last one is the output layer . In between we have multiple so-called hidden layers .\rThe input layer is for the data that we want to feed into the neural network in order to get a result.\rHere we put all of the things that are “perceived” by or put into the neural network.\rFor example, if we want to know if a picture shows a cat or a dog, we would put all the values of the individual pixels into the input layer.\rIf we want to know if a person is overweight or not, we would enter parameters like height, weight etc.\rThe output layer then contains the results. There we can see the values generated by the neural network based on our inputs.\rFor example the classification of an animal or a prediction value for something.\rEverything in between are abstraction layers that we call hidden layers.\rThese increase the complexity and the sophistication of the model and they expand the internal decision making.\rAs a rule of thumb we could say that the more hidden layers and the more neurons we have, the more complex our model is\nIn the figure above, we can see three layers. First the input layer , at the end the output layer and in between the hidden layer .\nObviously the input layer is where our inputs go. There we put all the things which are being entered or sensed by the script or the machine. Basically these are our features.\nWe can use neural networks to classify data or to act on inputs and the output layer is where we get our results. These results might be a class or action steps. Maybe when we input a high temperature into our model, the output will be the action of cooling down the machine.\nAll the layers between input and output are called hidden layers . They make the model more abstract and more complex. They extend the internal logic.\nThe more hidden layers and neurons you add, the more sophisticated the model gets.\nHere for example we have two hidden layers, one with four neurons and one with three neurons. Notice that every neuron of a layer is connected to every neuron of the next layer.\n\rSTRUCTURE OF A NEURON\rIn order to understand how a neural network works in general, we need to understand how the individual neurons work.\nAs you can see every neuron gets a certain input, which is either the output of a previous neuron or the raw input of the input layer.\rThis input is a numerical value and it then gets multiplied by each individual weight (w1, w2, w3…) .\rAt the end we then subtract the bias (b) .\nThe result is the output of that particular connection.\nThese outputs are that forwarded to the next layer of neurons.\nThe diagram below shows the neural network for computing the square root of a given number.\n\rACTIVATION FUNCTIONS\rThere are a lot of different so-called activation functions which make everything more complex.\rThese functions determine the output of a neuron. Basically what we do is: We take the input of our neuron and feed\rthe value into an activation function. This function then returns the output value.\n\rSIGMOID ACTIVATION FUNCTION\rA commonly used and popular activation function is the so-called sigmoid activation function .\rThis function always returns a value between zero and one, no matter what the input is.\rThe smaller the input, the closer the output will be to zero. The greater the input, the closer the output will be to one.\nThe mathematical formula looks like this:\n\\(g(z) = \\dfrac{1}{1 + e^{-z}}\\)\nExample in Python:\nfrom keras.models import Sequential\rmodel = Sequential()\r..\r..\rmodel.add(Activation(\u0026#39;sigmoid\u0026#39;))\rExample in R:\nmodel \u0026lt;- keras_model_sequential()\rmodel %\u0026gt;% layer_dense(units = 8, activation = \u0026#39;sigmoid\u0026#39;, input_shape = c(4)) %\u0026gt;% layer_dense(units = 3, activation = \u0026#39;softmax\u0026#39;)\rOutput layer creates 3 output values, one for each Iris class. The first layer, which contains 8 hidden notes, on the other hand, has an input_shape of 4. This is because your training data has 4 columns. \r\rRELU ACTIVATION FUNCTION\rThe probably most commonly used activation function is the so-called ReLU function .\rThis stands for rectified linear unit . This function is very simple but also very useful. Whenever the input value is\rnegative, it will return zero. Whenever it is positive, the output will just be the input.\n\\(g(z) = \\max(0,z)\\)\nExample in Python:\nfrom keras.models import Sequential\r(Creates a sequential keras model and add an activation function to it)\rmodel = Sequential()\r..\r..\rmodel.add(Activation(\u0026#39;relu\u0026#39;))\rExample in R:\nmodel \u0026lt;- keras_model_sequential()\r(Output layer creates 3 output values, one for each class. The first layer, which contains 8 hidden notes, on the other hand, has an input_shape of 4. This is because your training data has 4 columns.)\rmodel %\u0026gt;% layer_dense(units = 8, activation = \u0026#39;relu\u0026#39;, input_shape = c(4)) %\u0026gt;% layer_dense(units = 3, activation = \u0026#39;softmax\u0026#39;)\r\rTYPES OF NEURAL NETWORKS\rNeural networks are not only different because of the activation functions of their individual layers.\rThere are also different types of layers and networks\nFEED FORWARD NEURAL NETWORKS\rThe so-called feed forward neural networks could be seen as the classic neural networks. Up until now we have primarily talked about these.\rIn this type of network the information only flows into one direction – from the input layer to the output layer. There are no circles or cycles.\n\rRECURRENT NEURAL NETWORKS\rSo-called recurrent neural networks on the other hand work differently. In these networks we have layers with neurons that not only connect\rto the neurons next layer but also to neurons of the previous or of their own layer. This can also be called feedback .If we take the output of a neuron and use it as an input of the same neuron, we are talking about direct feedback . Connecting the output to neurons of the same layer is called lateral feedback . And if we take the output and feed it into neurons of the previous layer, we are talking about indirect feedback\rThe advantage of such a recurrent neural network is that it has a little memory and doesn’t only take the immediate present data into account. We could say that it “looks back” a couple of iterations.\nThis kind of neural networks is oftentimes used when the tasks requires the processing of sequential data like text or speech. The feedback is very useful in this kind of tasks. However it is not very useful when dealing with image recognition or image processing.\n\rCONVOLUTIONAL NEURAL NETWORKS\rFor this purpose we have the so-called convolutional neural networks . This type is primarily used for processing images and sound. It is especially useful when pattern recognition in noisy data is needed. This data may be image data, sound data or video data. It doesn’t matter.\nLet’s look at a simple example above. Here we have multiple Xs and Os as examples in a 16x16 pixels format. Each pixel is an input neuron and will be processed. At the end our neural network shall classify the image as either an X or an O.\n\r\rSUMMARY\r\rActivation functions determine the activation of a neuron which then influences the outputs.\n\rThe classic neural networks are feed forward neural networks. The information only flows into one direction.\n\rIn recurrent neural networks we work with feedback and it is possible to take the output of future layers as the input of neurons. This creates something like a memory.\n\rConvolutional neural networks are primarily used for images, audio data and other data which requires pattern recognition. They split the data into features.\n\rUsually we use 80% of the data we have as training data and 20% as testing data.\n\rThe error indicates how much percent of the data was classified incorrectly.\n\rThe loss is a numerical value which is calculated with a loss function. This is the value that we want to minimize in order to optimize our model.\n\rFor the minimization of the output we use the gradient descent algorithm. It finds the local minimum of a function.\n\rBackpropagation is the algorithm which calculates the gradient for the gradient descent algorithm. This is done by starting from the output layer and reverse engineering the desired changes.\n\r\r\r\r",
    "ref": "/blog/neural-networks/"
  },{
    "title": "React Native",
    "date": "",
    "description": "React Native",
    "body": "\r\r\rReact Native\rReact Native is framework to building mobile apps using JavaScript and React.\rReact Native converts React code to Java for Android or Objective-C for iOS apps.\rReact Native uses most of the React concepts, like components, props, state and lifecycle methods.\n\rAdvantages of React Native:\r\r\rYou code one time, and you get two native apps (Android and iOS)\n\r\rYou need not to have experience with Java, Objective-C, or Swift\n\r\rFaster development\n\r\rMIT license (open source)\n\r\rRequirements for Windows:\r\r\rAndroid Studio\n\r\rAndroid SDK (\u0026gt;= 7.0 Nougat)\n\r\rAndroid AVD\n\rCreating first React Native Application\rIn order to create our new React Native application, we need to install the react-native-cli package:\nnpm install -g react-native-cli\rNow, to create our first app:\nLet’s do it with this command:\r\r react-native init MyFirstReactNativeApp\rOnce we built our React Native app, we need install Watchman, which is a file-watching service required by React Native. To install it, go to https://facebook.github.io/watchman/docs/install.html and download the latest version for your OS (Windows, Mac, or Linux).\nIn this case, we are going to use Homebrew to install it for Mac. If you don’t have Homebrew, you can install it with this command:\r\r /usr/bin/ruby -e \u0026#39;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\u0026#39;\rTo install Watchman, you need to run:\r\r brew update brew install watchman\rTo start the React Native project, we need to use:\r\r react-native start\rIf everything works fine, you should see this:\nOpen a new terminal (Cmd + T) and run this command (depending on the device you want to use):\r\r react-native run-ios or\rreact-native run-android\rIf there are no errors, you should see the simulator running the default application:\nNow that we have our application running, let’s open our code and modify it a bit:\nChange the App.js file:\r\r ...\rexport default class App extends Component\u0026lt;Props\u0026gt; {\rrender() {\rreturn (\r\u0026lt;View style={styles.container}\u0026gt;\r\u0026lt;Text style={styles.welcome}\u0026gt;\rThis is my first React Native App!\r\u0026lt;/Text\u0026gt;\r\u0026lt;Text style={styles.instructions}\u0026gt;\rTo get started, edit App.js\r\u0026lt;/Text\u0026gt;\r\u0026lt;Text style={styles.instructions}\u0026gt;{instructions}\u0026lt;/Text\u0026gt;\r\u0026lt;/View\u0026gt;\r);\r}\r}\r...\rFile: App.js\rIf you go to the simulator again, you will need to press Cmd + R to reload the app to see the new changes reflected:\r\r\rSummary:\rReact Native is an framework which helps web developers in creating the robust\rmobile applications using their existing JavaScript knowledge. It offers more faster mobile\rdevelopment and more efficient code sharing between iOS, Android, and the web\rwithout sacrificing the end user’s experience or application quality.\n\r\r",
    "ref": "/blog/react-native/"
  },{
    "title": "About",
    "date": "",
    "description": "",
    "body": "I have always loved programming with computers since early days of the internet and a few year ago discovered Python, R and ReactJS. Soon it became clear to me that these programming languages can be used to arrive at a simple solution to many complex problems. But learning new languages can be challenging task for us and even more important is to retain that learning. In my personal experience, I had to read several books and blogs and do countless searches on Google to solve specific problems. This process was time consuming. So I decided to organize what I learned on this blog. I hope that you will find what you are looking for on this web-site without wasting too much time.\n",
    "ref": "/about/"
  },{
    "title": "React- JS",
    "date": "",
    "description": "React JS Programming",
    "body": "\r\r\r*** React-JS\nSetup\rfirst, we need to create our React application using create-react-app. Once that is done, you can proceed to create your first React component.\nInstall create-react-app globally by typing this command in your Terminal:\n npm install -g create-react-app\rOr you can use a shortcut:\n npm i -g create-react-app\r\rBuild the App\rLet’s build our first React application by following these steps:\nCreate our React application with the following command:\ncreate-react-app my-first-react-app\rGo to the new application with cd my-first-react-app and start it with npm start.\rThe application should now be running at http://localhost:3000.\nCreate a new file called Home.js inside your src folder:\n\rimport React, { Component } from \u0026#39;react\u0026#39;;\rclass Home extends Component {\rrender() {\rreturn \u0026lt;h1\u0026gt;I\u0026#39;m Home Component\u0026lt;/h1\u0026gt;;\r}\r}\rexport default Home;\rYou may have noticed that we are exporting our class component at the end of the file, but it’s fine to export it directly on the class declaration, like this:\n import React, { Component } from \u0026#39;react\u0026#39;;\rexport default class Home extends Component {\rrender() {\rreturn \u0026lt;h1\u0026gt;I\u0026#39;m Home Component\u0026lt;/h1\u0026gt;;\r}\r}\rNow that we have created the first component, we need to render it. So we need to open the App.js file, import the Home component, and then add it to the render method of the App component. If we are opening this file for the first time, we will probably see a code like this in File: src/App.js\n import React, { Component } from \u0026#39;react\u0026#39;;\rimport logo from \u0026#39;./logo.svg\u0026#39;;\rimport \u0026#39;./App.css\u0026#39;;\rclass App extends Component {\rrender() {\rreturn (\r\u0026lt;div className=\u0026#39;App\u0026#39;\u0026gt;\r\u0026lt;header className=\u0026#39;App-header\u0026#39;\u0026gt;\r\u0026lt;img src={logo} className=\u0026#39;App-logo\u0026#39; alt=\u0026#39;logo\u0026#39; /\u0026gt;\r\u0026lt;h1 className=\u0026#39;App-title\u0026#39;\u0026gt;Welcome to React\u0026lt;/h1\u0026gt;\r\u0026lt;/header\u0026gt;\r\u0026lt;p className=\u0026#39;App-intro\u0026#39;\u0026gt;\rTo get started, edit \u0026lt;code\u0026gt;src/App.js\u0026lt;/code\u0026gt; and save to reload.\r\u0026lt;/p\u0026gt;\r\u0026lt;/div\u0026gt;\r);\r}\r}\rexport default App;\rLet’s change this code a little bit. As I said before, we need to import our Home component and then add it to the JSX. We also need to replace the\r\relement with our component, like this in File: src/App.js\n import React, { Component } from \u0026#39;react\u0026#39;;\rimport logo from \u0026#39;./logo.svg\u0026#39;;\r// We import our Home component here...\rimport Home from \u0026#39;./Home\u0026#39;;\rimport \u0026#39;./App.css\u0026#39;;\rclass App extends Component {\rrender() {\rreturn (\r\u0026lt;div className=\u0026#39;App\u0026#39;\u0026gt;\r\u0026lt;header className=\u0026#39;App-header\u0026#39;\u0026gt;\r\u0026lt;img src={logo} className=\u0026#39;App-logo\u0026#39; alt=\u0026#39;logo\u0026#39; /\u0026gt;\r\u0026lt;h1 className=\u0026#39;App-title\u0026#39;\u0026gt;Welcome to React\u0026lt;/h1\u0026gt;\r\u0026lt;/header\u0026gt;\r{/* Here we add our Home component to be render it */}\r\u0026lt;Home /\u0026gt;\r\u0026lt;/div\u0026gt;\r);\r}\r}\rexport default App;\rAs you can see, we imported React and Component from the React library. You probably noticed that we are not using the React object directly. To write code in JSX, you need to import React. JSX is similar to HTML, but with a few differences. In the following recipes, you will learn more about JSX.\nThis component is called a class component (React.Component), and there are different types: pure components (React.PureComponent) and functional components, also known as stateless components, which we will cover in the following recipes.\nIf you run the application, you should see something like this:\n\r\r",
    "ref": "/blog/react-js/"
  },{
    "title": "Web Scrapping Basics",
    "date": "",
    "description": "A beginer level introduction to Web Scrapping using Python",
    "body": "\r\r\rWhat is web scrapping\rWebscrapping also known as the the automated gathering of data from the Internet.\ritself. This is usually accomplished by writing an automated program\rthat queries a web server, requests data , and then parses that data to extract needed informa‐\rtion. BeautifulSoup, Python requests, LXML, Mechanical Soup and Scrappy are most common libraries used for web scraping.\nFeatures:\n\rLarge amounts of data can be extracted from websites\n\rConvert unstructred data to structured data\n\rThere is a Crawler in webscrapping to index and search content\n\rThere is a Scrapper in webscrapping which extracts the data from the web page\n\rWebsites, News resources, RSS feeds, Pricing Websites, Social Media, Company information, Schemas/charts/tables/graphs are the main sources of unstructured data which a webscrapper uses to get data.\n\r\rWhy Web Scraping?\rBrowsers displays the contents in the human readable format but it can not answers to specific queries that can be used to integrate with other systems.\rFor example if a program requires information such as cheapest flights to newyork then the browser will provide lots of information containing images,\radvertisements etc. but a web scrapping program will get the specif answer to our query. Practically web scrapping involves a wide variety of programming techniques\rand technologies, such as data analysis and information security.\n\r\rHow it works\r\rGet Request is sent using http protocol to the site the scrapper is targetting\n\rWeb server processes the request and then allowed to read and extract the html of the web page\n\rThe data is retrieved in html format after which it is carefully parsed to extricate the raw data we want from th noise surrounding it.\n\rFinally the data is stored in the format to exact the specifications of the project.\n\r\r\rFlow\r\rRequest\n\rCheck Response\n\rParse\n\rFilter\n\rDownload\n\r\r\rCommon python libraries for web-scrapping\rBeautifulSoup\rThe most common library is BeautifulSoup.\n\rIt parses html document\n\rIt extracts text from it\n\rIt searches tags by their attributes\n\rIt has findAll and find functions are commonly used to find all attributes.\n\r\rExample#1: Getting covid-19 data from the web\nimport requests\rfrom bs4 import BeautifulSoup\rurl = \u0026quot;https://www.worldometers.info/coronavirus/\u0026quot;\rpage = requests.get(url)\rsoup = BeautifulSoup(page.text,\u0026#39;html.parser\u0026#39;)\rtotal = soup.find(\u0026quot;div\u0026quot;,class_ = \u0026quot;maincounter-number\u0026quot;).text\rtotal = total[1:len(total)-1]\rother = soup.find_all(\u0026quot;span\u0026quot;,class_=\u0026quot;number-table\u0026quot;)\rrecovered = other[2].text\rdeaths = other[3].text\rdeaths = deaths[1:]\rans = {\u0026quot;total cases\u0026quot;: total, \u0026quot;recovered\u0026quot;: recovered, \u0026quot;deaths\u0026quot;: deaths}\rprint(ans)\r## {\u0026#39;total cases\u0026#39;: \u0026#39;99,437,643 \u0026#39;, \u0026#39;recovered\u0026#39;: \u0026#39;71,500,196\u0026#39;, \u0026#39;deaths\u0026#39;: \u0026#39;2,132,432\u0026#39;}\rExample#2: Scrapping yourdictionary.com\nimport requests\rfrom bs4 import BeautifulSoup\rurl = \u0026quot;https://examples.yourdictionary.com/20-words-to-avoid-on-your-resume.html\u0026quot;\rpage = requests.get(url)\rsoup = BeautifulSoup(page.text,\u0026#39;html.parser\u0026#39;)\rparas = soup.findAll(\u0026#39;p\u0026#39;)\rfor p in paras:\rprint(p.text + \u0026#39;\\n\u0026#39;)\rExample#2: Getting top mathematicians from web\nfrom urllib.request import urlopen as uReq\rfrom bs4 import BeautifulSoup as BeautifulSoup, Tag\rimport pandas as pd\rhtml = uReq(\u0026quot;http://www.fabpedigree.com/james/gmat200.htm\u0026quot;).read()\rsoup = BeautifulSoup(html,\u0026#39;html5lib\u0026#39;)\rnames = []\rfor item in soup.find_all(\u0026#39;li\u0026#39;):\rif isinstance(item, Tag):\rnames.append(item.text.rstrip())\rnames_df = pd.DataFrame(names) print(names_df.head()) \r## 0\r## 0 Isaac Newton\r## 1 Archimedes\r## 2 Carl F. Gauss\r## 3 Leonhard Euler\r## 4 Bernhard Riemann\r\rLXML\rPython provides lxml library which is easier to use and has lots of features. lxml and Beautiful soup have similarity. It allows to parse XML and HTML documents easily. Ease of use and performance are the key features of lxml library.\nExample:\nfrom lxml import html\rimport requests\rpage = requests.get(\u0026#39;https://projecteuler.net/problem=1\u0026#39;)\rtree = html.fromstring(page.content)\rtext=tree.xpath(\u0026#39;//div[@role=\u0026quot;problem\u0026quot;]/p/text()\u0026#39;)\rprint (text)\r## [\u0026#39;If we list all the natural numbers below 10 that are multiples of 3 or 5, we get 3, 5, 6 and 9. The sum of these multiples is 23.\u0026#39;, \u0026#39;Find the sum of all the multiples of 3 or 5 below 1000.\u0026#39;]\r\rMachenical Soup\rIt is a library for automating interaction with the websites. User can login and logout of the website, submit forms etc.\n\rPython Requests\rSubmitting a form with the Requests library can be done in four lines, including the\rimport and the instruction to print the content (yes, it’s that easy):\nExample#3 Getting exchange rates\n\rimport requests\rimport pandas as pd\r# base_url variable store base url base_url = \u0026quot;https://www.alphavantage.co/query?function=CURRENCY_EXCHANGE_RATE\u0026quot;\rfrom_currency = \u0026quot;USD\u0026quot;\rto_currency = \u0026quot;INR\u0026quot;\rmain_url = base_url + \u0026quot;\u0026amp;from_currency=\u0026quot; + from_currency + \u0026quot;\u0026amp;to_currency=\u0026quot; + to_currency + \u0026quot;\u0026amp;apikey=4RSKNH0KOBS5TMP1\u0026quot;\rreq_ob = requests.get(main_url) result = req_ob.json() oneusdequals = result[\u0026quot;Realtime Currency Exchange Rate\u0026quot;][\u0026#39;5. Exchange Rate\u0026#39;]\rprint(float(oneusdequals))\r\r## 72.976\r\rSummary\rThere are different ways to scrape data from the internet.\rRegular expressions can be useful for a one-off scrape or to avoid the overhead of parsing the entire web page.\rBeautifulSoup provides a high-level interface while avoiding any difficult dependencies.\rWeb scraping services provide an essential service at a low cost.\rIt is used to scrape Price and Products for Comparison Sites and many such use cases to provide useful data for further processing.\n\r\r",
    "ref": "/blog/web-scrapping/"
  },{
    "title": "Clustering",
    "date": "",
    "description": "Clustering is Machine Learning method that helps in finding the pattern of similarity and relationships among data samples in dataset and then cluster these samples into various groups.",
    "body": "\r\r\rClustering\rClustering is Machine Learning method that helps in finding the pattern of similarity and relationships among data samples in dataset and then cluster these samples into various groups.\nThe clustering algorithm gets raw data and tries to divide it up into clusters . K-Means-Clustering is the method that we are going to use here. Similar to K-Nearest-Neighbors, the K states the amount of clusters we want.\nHOW CLUSTERING WORKS\rThe clustering itself works with so-called centroids . These are the points, which lie in the center of the respective clusters.\nThe figure above illustrates quite well how clustering works. First, we randomly place the centroids somewhere in our data. This is the initialization . Here, we have defined three clusters, which is why we also have three centroids.\nThen, we look at each individual data point and assign the cluster of the nearest centroid to it. When we have done this, we continue by realigning our centroids. We place them in the middle of all points of their cluster.\nAfter that, we again reassign the points to the new centroids. We continue doing this over and over again until almost nothing changes anymore. Then we will hopefully end up with the optimal clusters. The result then looks like this:\nTypes of Clustering methods\n• Centroid based methods such as K-means and K-medoids\n• Hierarchical clustering methods such as agglomerative and divisive (Ward’s, affinity\rpropagation)\n• Distribution based clustering methods such as Gaussian mixture models\n• Density based methods such as dbscan and optics.\n\rLOADING DATA\rFor the clustering algorithm, we will use a dataset of handwritten digits. Since we are using unsupervised learning, we are not going to classify the digits. We are just going to put them into clusters. The following imports are necessary:\nfrom sklearn.cluster import KMeans\rfrom sklearn.preprocessing import scale\rfrom sklearn.datasets import load_digits\rBesides the KMeans module and the load_digits dataset, we are also importing the function scale from the preprocessing library. We will use this function for preparing our data.\ndigits = load_digits()\rdata = scale(digits.data)\rAfter loading our dataset we use the scale function, to standardize our data. We are dealing with quite large values here and by scaling them down to smaller values we save computation time.\n\rTRAINING AND PREDICTING\rWe can now train our model in the same way we trained the supervised learning models up until now.\nclf = KMeans( n_clusters = 10 , init = \u0026#39;random\u0026#39; , n_init = 10 )\rclf.fit(data)\r## KMeans(init=\u0026#39;random\u0026#39;, n_clusters=10)\rcentroids somewhere. Alternatively, we could use k-means++ for intelligent placing.\rThe last parameter (n_init ) states how many times the algorithm will be run with different centroid seeds to find the best clusters.\nSince we are dealing with unsupervised learning here, scoring the model is not really possible. You won’t be able to really score if the model is clustering right or not. We could only benchmark certain statistics like completeness or homogeneity .\nWhat we can do however is to predict which cluster a new input belongs to.\nclf.predict([…])\nIn this case, inputting data might be quite hard, since we would need to manually put in all the pixels. You could either try to write a script what converts images into NumPy arrays or you could work with a much simpler data set.\nAlso, since we are working with huge dimensions here, visualization is quite hard. When you work with two- or three-dimensional data, you can use the Matplotlib knowledge from volume three, in order to visualize your model.\n\r\r",
    "ref": "/blog/clustering/"
  },{
    "title": "K nearest Neighbours",
    "date": "",
    "description": "K nearest Neighbours",
    "body": "\r\r\rK nearest Neighbours\rIf there are several groups of labeled samples and the items present in the groups are homogeneous in nature.\rNow, assume that we have an unlabeled example which needs to be classified into one of those several labeled groups.\rHow do we do that? Using kNN Algorithm. k nearest neighbors is an algorithm that knows all the available cases and classify the new cases by a majority vote of its k neighbors. This algorithms segregates unlabeled data points into well defined groups. KNN classifies a categorical value using the majority votes of nearest neighbors.\nInput/Output/parameter\rThe input to a KNN classifier can be both quantitative and qualitative. The output from a KNN classifer are categorical values which typically are the class of the data.\n\rDistance metric\rA distance metric is used to define proximity between ant two data points. The examples are Euclidean distance, Manhattan distance or Hamming distance.\nThe Euclidean distance is used to be applied to categorical data.\nManhattan distance is used over the Euclidean distance when there is high dimensionality in the data.\rManhattan distance (or block distance) is appropriate when we have a discrete data set and the Euclidean distance is appropriate when we have continuous numerical variables.\nBoth Euclidean and Manhattan distances are used in case of discrete/continuous variables, whereas hamming distance is used in case of categorical variable.\n\rExample\rLet us assume that we have a blind tasting experience, in which we need to classify what food we tasted as fruit, protein, or vegetable. Suppose that prior to eating the mystery item, we created a taste dataset in which we recorded two features of each ingredient: A measure from 1 to 10 on how crunchy the food is and second a score from 1 to 10 of how sweet the ingredient tastes. Then we labeled each ingredient as one of the three types of food: fruits,vegetables or proteins. We could have a table such as:\nimport pandas as pd\ringredient = [\u0026#39;apple\u0026#39;, \u0026#39;bacon\u0026#39;, \u0026#39;banana\u0026#39;, \u0026#39;carrot\u0026#39;, \u0026#39;celery\u0026#39;, \u0026#39;cheese\u0026#39;,\u0026#39;grape\u0026#39;,\u0026#39;green bean\u0026#39;,\u0026#39;nuts\u0026#39;,\u0026#39;orange\u0026#39;]\rsweetness = [10,1,10,7,3,1,8,3,3,7]\rcrunchiness = [9,4,1,10,10,1,5,7,6,3]\rfood_type = [\u0026#39;fruit\u0026#39;,\u0026#39;protein\u0026#39;,\u0026#39;fruit\u0026#39;,\u0026#39;vegetable\u0026#39;,\u0026#39;vegetable\u0026#39;,\u0026#39;protein\u0026#39;,\u0026#39;fruit\u0026#39;,\u0026#39;vegetable\u0026#39;,\u0026#39;protein\u0026#39;,\u0026#39;fruit\u0026#39;]\rdf = pd.DataFrame(list(zip(ingredient,sweetness,crunchiness,food_type)), columns =[\u0026#39;ingredient\u0026#39;, \u0026#39;sweetness\u0026#39;,\u0026#39;crunchiness\u0026#39;,\u0026#39;food_type\u0026#39;])\rprint(df)\r## ingredient sweetness crunchiness food_type\r## 0 apple 10 9 fruit\r## 1 bacon 1 4 protein\r## 2 banana 10 1 fruit\r## 3 carrot 7 10 vegetable\r## 4 celery 3 10 vegetable\r## 5 cheese 1 1 protein\r## 6 grape 8 5 fruit\r## 7 green bean 3 7 vegetable\r## 8 nuts 3 6 protein\r## 9 orange 7 3 fruit\rThe kNN algorithm treats the features as coordinates in a multidimensional feature space. As our dataset includes only two\rfeatures, the feature space is 2 dimensional, with the x dimension indicating the ingredient sweetness and the y dimension indicating the crunchiness.\nimport matplotlib.pyplot as plt\rx = sweetness\ry = crunchiness\rlabs = food_type\rplt.scatter(x, y)\rfor i, txt in enumerate(labs):\rplt.annotate(txt, (x[i]+.25, y[i]), fontsize=12)\rSimilar types of food tend to be grouped closely together.As illustrated in the next figure, vegetables tend to be crunchy but not sweet, fruits tend to be sweet and either crunchy or not crunchy, while proteins tend to be neither crunchy nor sweet\nSuppose that after constructing this dataset we need to find if tomato a fruit or a vegetable. We can use a nearest neighbor approach to determine which class is a better fit as shown in the following figure:\n\rCalculating distance:\rLocating an object nearest neighbors requires a distance function or a formula that will measure the similarity between two instances.\rTraditionally the kNN algorithm uses the Euclidian distance. Euclidian distance is specified by the following formula, where p\rand q are instances to be compared, each having n features\nThe term p1 refers to the value of the first feature in p, while q1 refers to\rthe first feature of q.\n\\[dist (p, q) = √(𝑝1 − 𝑞1)^2 + (𝑝2 − 𝑞2)^2 + ⋯ (𝑝^ n − 𝑞^𝑛)\\]\nThe distance formula involves comparing the values of each feature. For example, to\rcalculate the distance between the tomato (sweetness = 6, crunchiness = 4), and the\rgreen bean (sweetness = 3, crunchiness = 7), we can use the formula as follows:\n\\[dist(tomato, green bean) = \\sqrt(6-3)^2 + (4−7)^2 = 4.2\\]\nIn a similar vein, we can calculate the distance between the tomato and several of its\rclosest neighbors as follows:\n\r\ringradient\rsweetness\rcrunchness\rfood type\rdistanct to the tomato\r\r\r\rgrape\r8\r5\rfruit\r\\[\\sqrt((6 - 8)^2 + (4 - 5)^2) = 2.2 \\]\r\rgreen bean\r3\r7\rvegetable\r\\[\\sqrt((6 - 3)^2 + (4 - 7)^2) = 4.2 \\]\r\rnuts\r3\r6\rprotein\r\\[\\sqrt((6 - 3)^2 + (4 - 6)^2) = 3.6 \\]\r\rorange\r7\r3\rfruit\r\\[\\sqrt((6 - 7)^2 + (4 - 3)^2) = 1.4 \\]\r\r\r\rTo classify the tomato as vegetable, fruit or protein, we’ll begin by assigning the tomato the food type of its single nearest neighbor.\rThis is called 1NN because k =1. The orange is the nearest neighbor to the tomato, with a distance of 1.4. As orange is a fruit,\rthe 1NN algorithm would classify tomato as a fruit. If we use the kNN algorithm with k=3 instead, it performs a vote among the three nearest neighbors: orange, grape, and nuts.\nApplying KNNClassifier:\nThe features list consists of the sweetness and crunchiness values. The target which is a categorical value is\rencoded using LabelEncoder.\nfrom sklearn.preprocessing import LabelEncoder as le\rfeatures = list(zip (sweetness, crunchiness))\rfood_type = [\u0026#39;fruit\u0026#39;,\u0026#39;protein\u0026#39;,\u0026#39;fruit\u0026#39;,\u0026#39;vegetable\u0026#39;,\u0026#39;vegetable\u0026#39;,\u0026#39;protein\u0026#39;,\u0026#39;fruit\u0026#39;,\u0026#39;vegetable\u0026#39;,\u0026#39;protein\u0026#39;,\u0026#39;fruit\u0026#39;]\rtarget = le.fit_transform(food_type)\rtarget_lables = food_type\rtarget_df = pd.DataFrame(food_type,target)\rtarget_df\r0:fruit,\r1:protein,\r2:vegetable\nKNeighborsClassifier created with neighbors = 2. Inside the model we fit the data with the features and encoded target\nmodel = KNeighborsClassifier(n_neighbors=2)\rmodel.fit(features,target)\rAfter that we using the model we predict the food_type for new data (‘Orange’,sweetness = 6, crunchines = 4)\npredicted= model.predict([[6,4]]) print(predicted)\r[0]\nThe predicted class 0 belongs to the food type of ‘fruit’.\n\rChoosing an appropriate k\rDeciding how many neighbrs to be used for kNN determines how well the model will generalize to future data. The balance between\roverfitting or underfitting the training data is a problem called as the bias/variance tradeoff. Choosing greater k reduce the variance caused by noisy data, but it can bias the learner and can run the risk of ignoring small but important patterns.\nSmaller k values cause complex decision boundaries that more carefully fit the training data and can overfit.\nIn practice, choosing k-value depends on the complexity of the concept to be learned and the number of records in the training data.\nGenerally, k is set between 3 and 10. One common practice is setting the k equal to the square root of the number of training example.\nIn the food classifier, we can set it to 4 assuming that there are 15 examples in the training data set.\rAn alternative value is to test multiple values of k on different test datasets and select the one that delivers the best classification performance.\nIn the example above because the majority class among these neighbors is fruit (2 out of 3 votes), the tomato again is classified as a fruit.\n\rAlgorithm\rThe KNN classification is performed using the following four steps\n\rCompute the distance metric between the test data point and all the labeled data points\n\rOrder the labeled data points in the increasing order of this distance metric\n\rSelect the top k labeled data points and look at the class labels\n\rFind the class label that the majority of these k labeled data points have and assign it to the test data point.\n\r\rThere are various things like Parameter selection, Presence of noise, Feature selection and scaling, Curse of dimensionality we need to consider before applying KNN algorithm\n\rK fold cross validation\rIn order to evaluate the performance of KNN classifier there are different approaches.\rOne approach is separating the data have into a training and test set. but it can be difficult to decide what the ratio should be between the\rsets. Outliers and other factor will greatly influence the results of how the classifier is built and the\rreported accuracy.\nAnother approach is called as k-fold cross validation. In this approach, the data is divided up in to k equally sized pieces. One of these pieces is saved for test data and the remaining k − 1 pieces are used for training. We then buiid the model k times, rotating the piece we use for testing. We can judge the performance of the classifier on its average accuracy and standard dev.\n\rk fold example\rSplit the dataset into K equal partitions (or “folds”) therefore if k = 4 and dataset has 120 observations then each of the 4 folds will have 30 observations. After that make use of the fold 1 as the testing set and the union of the other folds as the training set. Therefore in this case we will have testing set = 30 observations (fold 1), Training set = 90 observations (folds 2-4). After that calculate the testing accuracy. There after by repeating above steps K times, using a different fold as the testing set each time. We will need to repeat the process 4 times. In 2nd iteration fold 2 will be testing set and union of 1,3,4 will be training set and so on..Finally use the mean testing accuracy as the estimate of out-of-sample accuracy.\n# simulate splitting a dataset of 25 observations into 5 folds\rfrom sklearn.model_selection import KFold\rimport numpy as np\rkf = KFold(n_splits=5, random_state=None, shuffle=False)\rVec = np.arange(0,25)\r# print the contents of each training and testing set\rprint(\u0026#39;{} {:^61} {}\u0026#39;.format(\u0026#39;Iteration\u0026#39;, \u0026#39;Training set observations\u0026#39;, \u0026#39;Testing set observations\u0026#39;))\r## Iteration Training set observations Testing set observations\rfor iteration, data in enumerate(kf.split(Vec), start=1):\rprint(\u0026#39;{:^9} {} {!s:^25}\u0026#39;.format(iteration, data[0], data[1]))\r## 1 [ 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] [0 1 2 3 4] ## 2 [ 0 1 2 3 4 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] [5 6 7 8 9] ## 3 [ 0 1 2 3 4 5 6 7 8 9 15 16 17 18 19 20 21 22 23 24] [10 11 12 13 14] ## 4 [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 20 21 22 23 24] [15 16 17 18 19] ## 5 [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19] [20 21 22 23 24]\r\r\r",
    "ref": "/blog/k-nearest-neighbours/"
  },{
    "title": "Support Vector Machines",
    "date": "",
    "description": "In machine learning, support-vector machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.",
    "body": "\r\r\rSupport Vector Machines\rSupport Vector Machines are very powerful, very efficient machine learning algorithms and they even achieve much better results than neural networks in some areas. We are again dealing with classification here but the methodology is quite different.\nWhat we are looking for is a hyperplane that distinctly classifies our data points and has the maximum margin to all of our points. We want our model to be as generalized as possible.\nIn the graph above the model is very general and the line is the optimal function to separate our data. We can use an endless amount of lines to separate the two classes but we don’t want to overfit our model so that it only works for the data we already have. We also want it to work for unknown data.\nHere our model also separates the data we already have perfectly. But we’ve got a new red data point here. When we just look at this with our intuition it is obvious that this point belongs to the orange triangles. However, our model classifies it as a blue circle because it is overfitting our current data.\rTo find our perfect line we are using so-called support vectors , which are parallel lines.\nWe are looking for the two points that are the farthest away from the other class. In between of those, we draw our hyperplane so that the distance to both points is the same and as large as possible. The two parallel lines are the support vectors. In between the orange and the blue line there are no data points. This is our margin. We want this margin to be as big as possible because it makes our predictions more reliable.\n\rKERNELS\rThe data we have looked at so far is relatively easy to classify because it is clearly separated. Such data can almost never be found in the real world. Also, we are oftentimes working in higher dimensions with many features. This makes things more complicated.\nData taken from the real world often looks like this in figure. Here it is impossible to draw a straight line, and even a quadratic or cubic function does not help us here. In such cases we can use so-called kernels . These add a new dimension to our data. By doing that, we hope to increase the complexity of the data and possibly use a hyperplane as a separator.\nNotice that the kernel (a.k.a. the additional dimension) should be derived from the data that we already have. We are just making it more abstract. A kernel is not some random feature but a combination of the features we already have. But that wouldn’t be reasonable or helpful. Therefore, there are pre-defined and effective kernels that we can choose from.\n\rSOFT MARGIN\rSometimes, we will encounter statistical outliers in our data. It would be very easy to draw a hyperplane that separates the data into the classes, if it wasn’t for these outliers.\nn the figure above, you can see such a data set. We can see that almost all of the orange triangles are in the top first third, whereas almost all the blue dots are in the bottom two thirds. The problem here is with the outliers.\rNow instead of using a kernel or a polynomial function to solve this problem, we can define a so-called soft margin. With this, we allow for conscious misclassification of outliers in order to create a more accurate model. Caring too much about these outliers would again mean overfitting the model.\nAs you can see, even though we are misclassifying two data points our model is very accurate.\n\rLOADING DATA\rNow that we understand how SVMs work, let’s get into the coding. For this machine learning algorithm, we are going to once again use the breast cancer data set. We will need the following imports:\nfrom sklearn.svm import SVC\rfrom sklearn.datasets import load_breast_cancer\rfrom sklearn.neighbors import KNeighborsClassifier\rfrom sklearn.model_selection import train_test_split \rBesides the libraries we already know, we are importing the SVC module. This is the support vector classifier that we are going to use as our model. Notice that we are also importing the KNeighborsClassifier again, since we are going to compare the accuracies at the end.\ndata = load_breast_cancer()\rX = data.data\rY = data.target\rX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1 , random_state = 30 )\rThis time we use a new parameter named random_state . It is a seed that always produces the exact same split of our data. Usually, the data gets split randomly every time we run the script. You can use whatever number you want here. Each number creates a certain split which doesn’t change no matter how many times we run the script. We do this in order to be able to objectively compare the different classifiers.\n\rTRAINING AND TESTING\rSo first we define our support vector classifier and start training it.\nmodel = SVC( kernel = \u0026#39;linear\u0026#39; , C = 3 )\rmodel.fit(X_train, Y_train)\r## SVC(C=3, kernel=\u0026#39;linear\u0026#39;)\rWe are using two parameters when creating an instance of the SVC class. The first one is our kernel and the second one is C which is our soft margin. Here we choose a linear kernel and allow for three misclassifications. Alternatively we could choose poly, rbf, sigmoid, precomputed or a self-defined kernel. Some are more effective in certain situations but also a lot more time-intensive than linear kernels.\naccuracy = model.score(X_test, Y_test)\rprint (accuracy)\r## 0.9649122807017544\rWhen we now score our model, we will see a very good result.\r0.9649122807017544\n\r",
    "ref": "/blog/support-vector-machines/"
  },{
    "title": "Decision Trees",
    "date": "",
    "description": "Decision trees area tree-like tool which can be used to represent a cause and its effect",
    "body": "\r\r\rDecision trees\rDecision trees area tree-like tool which can be used to represent a cause and its effect. In Machine Learning Decision trees are a type of Supervised machine learning where data is split according to given parameter while constructing a tree to solve a given problem. In decision tree there is a predictor variable and target variable or the desired output. The predictor variable could be anything such as technical indicators etc and the target variable could be desired output for example whether to invest in a given financial security or not.\n\rHow it works\rA decision basically gives flowchart of how to make some decision.You have some dependent variable, like whether to buy the stock depending on factors like RSI, MACD etc. When you have a decision like that that depends on multiple attributes or multiple variables, a decision tree could be a good choice.\nThere are many different aspects of the weather that might influence my decision to buy a given stock. It might have to do with the stock closing prise today, the RSI,MACD, EMA etc. A decision tree can look at all these different features of the stock, and decide what are the thresholds. What are those factors which affects the stock movement.\nFor example, Factors affecting the stock movement is shown by using decision tree\n\rUnderstanding Decision Tree\rAt each iteration of the decision tree flowchart, we find the property that we can partition our data on which minimizes the entropy of the data at the next step. So we have a resulting set of classes in this case “BUY” or “SELL”, and we want to choose the attribute decision at that step that will minimize the entropy at the next step. So we just walk down the tree, minimize entropy at each step by choosing the right attribute to decide on, and we keep on going until we run out.\n\rThe decision to buy the stock in majority of the cases is dependent on the stock fundamentals. In 64% of the cases buying of stock is supported by decision tree if the fundamentals of the company are strong.\n\r\rDecision tree example\rLet’s say I want to build a system that will automatically predicts the stock movement at the end of the day based on the opening price of the stock. Given a stock the system should decide whether that stock is going to have upward movment or downward movement during intraday trading so that investor can make a decision of investment in that stock.\nSo let’s make some totally fabricated stock data that we’re going to use in this example:\nIn the preceding table, we have stock prices along with technical indicators. We are going to pick some attributes that we think might be interesting or helpful to predict whether or not they can predict movement of the stock (UP or DOWN). How much is William %R ? What is exponential moving average ? What is the value of stochastic momentun index ? Is the stock overbought/oversold (RSI) ? Depending on the factors which affect the stock price we can predict whether it will go up or down.\nNow, obviously there’s a lot of information that isn’t in this model that might be very important, but the decision tree that we train from this data might actually be useful in doing an initial pass at weeding out some candidates. What we end up with might be a tree that looks like the following:\nSo it just turns out that in totally fabricated data, if the william %R (WPR) is below 0.17 then the stock will go UP. So the first questioon decision tree related to WPR, i.e if WPR is above 0.17 then go to left. At node 2 check if the exponential moving average is above 9.7, if it is then we end up at the leaf node predicted value to stock going down. If at node 2 exponential moving average is below 9.7 then go to node 5 and check the value of stochastic mementun index, if it is above 42 then go to right at leaf node 11 having stock predicted value of UP and so on.\n\rWalking through a decision tree\rSo that’s how we walk through the results of a decision tree. It’s just like going through a flowchart, and it’s kind of awesome that an algorithm can produce this for us. The algorithm itself is actually very simple. Let me explain how the algorithm works.\nAt each step of the decision tree flowchart, we see the attribute that we can partition our data on that minimizes the entropy of the data at the next step. So we have a resulting set of classifications in this case UP or DOWN, and we want to choose the attribute decision at that step that will minimize the entropy at the next step.\n\rEntropy meausures the level of impurity in a group. The group having minimum entropy is helps determinie attribute most useful for descriminating between classes to be learned.\n\rAt each step we want to make all of the remaining choices result in either as many downs or as many up decisions as possible. We want to make that data more and more uniform so as we work our way down the flowchart, and we ultimately end up with a set of candidates that are either all UPS or all DOWNS so we can classify into yes/no decisions on a decision tree. So we just walk down the tree, minimize entropy at each step by choosing the right attribute to decide on, and we keep on going until we run out.\nThere’s a fancy name for this algorithm. It’s called ID3 ( Iterative Dichotomiser 3 ). It is what’s known as a greedy algorithm. So as it goes down the tree, it just picks the attribute that will minimize entropy at that point. Now that might not actually result in an optimal tree that minimizes the number of choices that you have to make, but it will result in a tree that works, given the data that you gave it.\nThe tree starts at the top and finds the best data to split into nodes. It does this by recursive binary splitting using either the Gini index or cross-entropy measure. The Gini index is defined as:\n\\[G = \\sum_{k=1}^K \\hat{p}_{mk}(1 - \\hat{p}_{mk})\\]\nand is also referred as a measure of node purity, i.e., a smaller value indicates a node contains observations primarily from a single class.\nCross-entropy is similar to the Gini index in that it will take a small value if the node is pure. It is defined as:\n\\[D = -\\sum_{k=1}^K \\hat{p}_{mk}log\\hat{p}_{mk}\\]\nThe Gini index and Cross-entropy measures dictate when a node split will occur in order to keep each node as pure as possible to reduce the total value of the Gini index or cross-entropy measures.\nStart at the top, or root of the tree. 57% of the stock movements will have an DOWN movement with 43% going in UPWARD direction. If the William % R rating was equal to or above 0.17, we look left, otherwise you move right. To the right, we see only 17% of values having WPR below 0.17 will have UP movement, so the overall terminal node ends with the bucket having UP stock movement and so on.\n\rEvaluating the Decision Tree Model\r DOWN UP\rDOWN 231 78\rUP 87 220\r\rThe decision tree model have accuracy of 73%.\n\rBenefit of Decision Tree over Neural network\rThe benefit of using Decision trees over Neural Network are:\nThey are easy to program.\n\rThe top nodes in the tree will give the information about what data affects the prediction.\n\rTrees are interpretable and provide visual representation of data.\n\rPerforms faster than Neural Networks after training.\n\r\r\rIssues with DT\rNow one problem with decision trees is that they are very prone to overfitting, so you can end up with a decision tree that works well for the data that we trained it on, but it might not be that great for actually predicting the correct classification for values that it hasn’t seen before. Decision trees are all about arriving at the right decision for the training data that we gave it, but maybe we didn’t really take into account the right attributes, maybe we didn’t give it enough of a representative sample of values to learn from. This can result in real problems.\nSo to combat this issue, we use a technique called random forests, where the idea is that we sample the data that we train on, in different ways, for multiple different decision trees. Each decision tree takes a different random sample from our set of training data and constructs a tree from it. Then each resulting tree can vote on the right result.\nNow that technique of randomly resampling our data with the same model is a term called bootstrap aggregating, or bagging. This is a form of what we call ensemble learning. The basic idea is that we have multiple trees, a forest of trees, each uses a random subsample of the data that we have to train on. Then each of these trees can vote on the final result, and that will help us combat overfitting for a given set of training data.\nThe other thing random forests can do is actually restrict the number of attributes that it can choose, between at each stage, while it is trying to minimize the entropy as it goes. And we can randomly pick which attributes it can choose from at each level. So that also gives us more variation from tree to tree, and therefore we get more of a variety of algorithms that can compete with each other. They can all vote on the final result using slightly different approaches to arriving at the same answer.\nSo that’s how random forests work. Basically, it is a forest of decision trees where they are drawing from different samples and also different sets of attributes at each stage that it can choose between.\n\rSummary\rWe have a better understanding of decision trees now; why they are being used frequently in predictive modeling, how they are created, and how they can be optimized for best results. Decision trees are a powerful tool for data scientists, but they must be handled with care. All the repetitive tasks are achieved by use of computers (hence the term machine learning), all aspects of the process must be overseen by an experienced data scientist in order to create the most accurate model.\n\r",
    "ref": "/blog/linear-regression/"
  },{
    "title": "Applying Logistic Regression",
    "date": "",
    "description": "A case study on applying logistic regression to stock market",
    "body": "1:Loading Stock data\n1.1:Importing libraries and data\nimport investpy\rfrom datetime import datetime\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.metrics import confusion_matrix\rfrom sklearn import metrics\rfrom scipy import stats import numpy as np\rimport pandas as pd\rimport seaborn as sn\rimport matplotlib.pyplot as plt\rimport talib\rimport quandl\rfrom sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score\rfrom sklearn.preprocessing import MinMaxScaler import math\rfrom sklearn.metrics import mean_squared_error\r\n1.2:Fetching the data\nTo fetch the stock data we use investpy library. This library fetchs data from investing for example:\n1.2.1: Determining position based on volume and close\n## Open High Low Close Volume Pos\r## Date ## 2021-01-22 67135.0 67135.0 65452.0 66612.0 228.940 Short\r## 2021-01-25 66800.0 67133.0 65906.0 66483.0 202.718 Short\r## 2021-01-27 66020.0 66586.0 65088.0 66512.0 221.619 Long\r## 2021-01-28 66017.0 70380.0 65510.0 67608.0 394.160 Long\r## 2021-01-29 67950.0 71109.0 67950.0 69654.0 376.318 Short\r## Open High Low Close Volume Pos gain\r## Date ## 2020-09-23 60950.0 60950.0 58079.0 58509.0 284.514 Short 2441.0\r## 2020-09-30 62100.0 62100.0 59482.0 59916.0 262.423 Short 2184.0\r## 2020-10-22 63222.0 63222.0 61855.0 62594.0 188.429 Short 628.0\r## 2020-11-04 62500.0 62500.0 60850.0 61406.0 306.039 Short 1094.0\r## 2020-11-24 60300.0 60300.0 59001.0 59759.0 1.272 Short 541.0\r## 2020-11-26 60538.0 60538.0 59552.0 59950.0 0.285 Short 588.0\r## 2020-12-09 64950.0 64950.0 63210.0 63550.0 254.773 Short 1400.0\r## 2021-01-08 69838.0 69838.0 63621.0 64348.0 437.809 Short 5490.0\r## 2021-01-15 66450.0 66450.0 64151.0 64745.0 274.157 Short 1705.0\r## 2021-01-22 67135.0 67135.0 65452.0 66612.0 228.940 Short 523.0\r## Open High Low Close Volume Pos gain\r## Date ## 2020-11-02 61150.0 62240.0 61150.0 62008.0 201.612 Short 858.0\r## 2020-11-05 61500.0 64352.0 61500.0 64223.0 263.737 Short 2723.0\r## 2020-11-20 61555.0 62750.0 61555.0 62114.0 160.982 Short 559.0\r## 2020-12-01 60500.0 63422.0 60500.0 63141.0 218.978 Long 2641.0\r## 2020-12-16 64950.0 66720.0 64950.0 65887.0 276.515 Long 937.0\r## 2020-12-17 66187.0 68334.0 66187.0 68183.0 270.267 Short 1996.0\r## 2020-12-28 67450.0 69758.0 67450.0 68765.0 243.974 Long 1315.0\r## 2021-01-04 68410.0 70600.0 68410.0 69905.0 282.309 Long 1495.0\r## 2021-01-07 69252.0 70198.0 69252.0 69898.0 238.639 Short 646.0\r## 2021-01-29 67950.0 71109.0 67950.0 69654.0 376.318 Short 1704.0\r1.2.2: Determining Average monthly closing prices\n## Open High Low Close Volume\r## Date ## 2015-01-01 35980.0 36243.0 35980.0 36229.0 13.298\r## 2015-01-02 36250.0 36888.0 35710.0 36281.0 97.968\r## 2015-01-05 36321.0 37232.0 36321.0 37045.0 97.992\r## 2015-01-06 37200.0 38130.0 37079.0 37919.0 86.924\r## 2015-01-07 37921.0 37936.0 37233.0 37797.0 74.335\r## ... ... ... ... ... ...\r## 2021-01-22 67135.0 67135.0 65452.0 66612.0 228.940\r## 2021-01-25 66800.0 67133.0 65906.0 66483.0 202.718\r## 2021-01-27 66020.0 66586.0 65088.0 66512.0 221.619\r## 2021-01-28 66017.0 70380.0 65510.0 67608.0 394.160\r## 2021-01-29 67950.0 71109.0 67950.0 69654.0 376.318\r## ## [1567 rows x 5 columns]\r## Open High Low Close Volume Currency VolChange \\\r## Date ## 2015-01-01 35.980 36.243 35.980 36.229 13.298 INR 2.455680 ## 2015-01-02 36.250 36.888 35.710 36.281 97.968 INR 6.367123 ## 2015-01-05 36.321 37.232 36.321 37.045 97.992 INR 0.000245 ## 2015-01-06 37.200 38.130 37.079 37.919 86.924 INR -0.112948 ## 2015-01-07 37.921 37.936 37.233 37.797 74.335 INR -0.144828 ## ... ... ... ... ... ... ... ... ## 2021-01-22 67.135 67.135 65.452 66.612 228.940 INR 0.292592 ## 2021-01-25 66.800 67.133 65.906 66.483 202.718 INR -0.114537 ## 2021-01-27 66.020 66.586 65.088 66.512 221.619 INR 0.093238 ## 2021-01-28 66.017 70.380 65.510 67.608 394.160 INR 0.778548 ## 2021-01-29 67.950 71.109 67.950 69.654 376.318 INR -0.045266 ## ## CloseChange ## Date ## 2015-01-01 0.000531 ## 2015-01-02 0.001435 ## 2015-01-05 0.021058 ## 2015-01-06 0.023593 ## 2015-01-07 -0.003217 ## ... ... ## 2021-01-22 -0.009531 ## 2021-01-25 -0.001937 ## 2021-01-27 0.000436 ## 2021-01-28 0.016478 ## 2021-01-29 0.030263 ## ## [1567 rows x 8 columns]\r## Date\r## 2015-01-31 38.321174\r## 2015-02-28 37.104955\r## 2015-03-31 36.604739\r## 2015-04-30 36.794619\r## 2015-05-31 38.679571\r## ... ## 2020-09-30 65.459455\r## 2020-10-31 61.675571\r## 2020-11-30 62.081364\r## 2020-12-31 65.897045\r## 2021-01-31 67.184300\r## Freq: M, Name: Close, Length: 73, dtype: float64\r## meanprice is 42.25600319081047\r1.2.3: Technical moving averages\n## period sma_value sma_signal ema_value ema_signal\r## 0 5 67393.00 buy 67820.1716 buy\r## 1 10 66735.80 buy 67412.2135 buy\r## 2 20 67189.10 buy 66977.3587 buy\r## 3 50 65553.52 buy 65970.8972 buy\r## 4 100 64163.40 buy 63783.1202 buy\r1.2.4: Determining the z-scores\n## Date\r## 2021-01-22 3.035618\r## 2021-01-25 3.019540\r## 2021-01-27 3.023155\r## 2021-01-28 3.159755\r## 2021-01-29 3.414759\r## Name: zscore, dtype: float64\r## Date\r## 2021-01-22 2.435868\r## 2021-01-25 2.035198\r## 2021-01-27 2.324004\r## 2021-01-28 4.960416\r## 2021-01-29 4.687792\r## Name: zscorevolume, dtype: float64\r1.2.5: Determining the daily support and resistance levels\n## PP R1 S1 R2 S2 R3 \\\r## Date ## 2021-01-22 66.399667 67.347333 65.664333 68.082667 64.716667 69.030333 ## 2021-01-25 66.507333 67.108667 65.881667 67.734333 65.280333 68.335667 ## 2021-01-27 66.062000 67.036000 65.538000 67.560000 64.564000 68.534000 ## 2021-01-28 67.832667 70.155333 65.285333 72.702667 62.962667 75.025333 ## 2021-01-29 69.571000 71.192000 68.033000 72.730000 66.412000 74.351000 ## ## S3 ## Date ## 2021-01-22 63.981333 ## 2021-01-25 64.654667 ## 2021-01-27 64.040000 ## 2021-01-28 60.415333 ## 2021-01-29 64.874000\r1.2.6: Fibonnacci retracement levels\n## Retracement levels for rising price\r## 0\r## 0 {'min': [67.95]}\r## 1 {'level5(61.8)': [69.156738]}\r## 2 {'level4(50)': [69.5295]}\r## 3 {'level3(38.2)': [69.902262]}\r## 4 {'level2(23.6)': [70.36347599999999]}\r## 5 {'zero': [71.109]}\r## Retracement levels for falling price\r## 0\r## 0 {'zero': [71.109]}\r## 1 {'level2(23.6)': [68.695524]}\r## 2 {'level3(38.2)': [69.156738]}\r## 3 {'level4(50)': [69.5295]}\r## 4 {'level5(61.8)': [69.902262]}\r## 5 {'min': [67.95]}\rAs we can see, we now have a data frame with all the entries from start date to end date. We have multiple columns here and not only the closing stock price of the respective day. Let’s take a quick look at the individual columns and their meaning.\nOpen: That’s the share price the stock had when the markets opened that day.\nClose: That’s the share price the stock had when the markets closed that day.\nHigh: That’s the highest share price that the stock had that day.\nLow: That’s the lowest share price that the stock had that day.\nVolume: Amount of shares that changed hands that day.\n1.3:Reading individual values\nSince our data is stored in a Pandas data frame, we can use the indexing we already know, to get individual values. For example, we could only print the closing values using print (df[ 'Close' ])\nAlso, we can go ahead and print the closing value of a specific date that we are interested in. This is possible because the date is our index column.\nprint (df[ \u0026#39;Close\u0026#39; ][ \u0026#39;2020-07-14\u0026#39; ]) ## 52.701\rBut we could also use simple indexing to access certain positions.\nprint (df[ \u0026#39;Close\u0026#39; ][ 5 ]) ## 36.989\rHere we printed the closing price of the fifth entry.\n2:Graphical Visualization\nEven though tables are nice and useful, we want to visualize our financial data, in order to get a better overview. We want to look at the development of the share price.\nActually plotting our share price curve with Pandas and Matplotlib is very simple. Since Pandas builds on top of Matplotlib, we can just select the column we are interested in and apply the plot method. The results are amazing. Since the date is the index of our data frame, Matplotlib uses it for the x-axis. The y-values are then our adjusted close values.\n2.1:CandleStick Charts\nThe best way to visualize stock data is to use so-called candlestick charts . This type of chart gives us information about four different values at the same time, namely the high, the low, the open and the close value. In order to plot candlestick charts, we will need to import a function of the MPL-Finance library.\nimport mplfinance as fplt We are importing the candlestick_ohlc function. Notice that there also exists a candlestick_ochl function that takes in the data in a different order. Also, for our candlestick chart, we will need a different date format provided by Matplotlib. Therefore, we need to import the respective module as well. We give it the alias mdates .\nimport matplotlib.dates as mdates 2.2: Preparing the data for CandleStick charts\nNow in order to plot our stock data, we need to select the four columns in the right order.\ndf1 = df[[ \u0026#39;Open\u0026#39; , \u0026#39;High\u0026#39; , \u0026#39;Low\u0026#39; , \u0026#39;Close\u0026#39; ]] Now, we have our columns in the right order but there is still a problem. Our date doesn’t have the right format and since it is the index, we cannot manipulate it. Therefore, we need to reset the index and then convert our datetime to a number.\ndf1.reset_index( inplace = True ) df1[ \u0026#39;Date\u0026#39; ] = df1[ \u0026#39;Date\u0026#39; ].map(mdates.date2num) ## C:/Users/slaxm/AppData/Local/r-miniconda/envs/r-reticulate/python.exe:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame.\r## Try using .loc[row_indexer,col_indexer] = value instead\r## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\rFor this, we use the reset_index function so that we can manipulate our Date column. Notice that we are using the inplace parameter to replace the data frame by the new one. After that, we map the date2num function of the matplotlib.dates module on all of our values. That converts our dates into numbers that we can work with.\n2.3:Plotting the data\nNow we can start plotting our graph. For this, we just define a subplot (because we need to pass one to our function) and call our candlestick_ohlc function.\nOne candlestick gives us the information about all four values of one specific day. The highest point of the stick is the high and the lowest point is the low of that day. The colored area is the difference between the open and the close price. If the stick is green, the close value is at the top and the open value at the bottom, since the close must be higher than the open. If it is red, it is the other way around.\n***2.4:Analysis and Statistics ***\nNow let’s get a little bit deeper into the numbers here and away from the visual. From our data we can derive some statistical values that will help us to analyze it.\nPERCENTAGE CHANGE\nOne value that we can calculate is the percentage change of that day. This means by how many percent the share price increased or decreased that day.\nThe calculation is quite simple. We create a new column with the name PCT_Change and the values are just the difference of the closing and opening values divided by the opening values. Since the open value is the beginning value of that day, we take it as a basis. We could also multiply the result by 100 to get the actual percentage.\n## PCT_Change\r## count 1567.000000\r## mean 0.000045\r## std 0.014256\r## min -0.107200\r## 25% -0.005880\r## 50% 0.000105\r## 75% 0.006103\r## max 0.062274\r## Close\r## Date ## 2021-01-29 69.654\r*** HIGH LOW PERCENTAGE ***\nAnother interesting statistic is the high low percentage. Here we just calculate the difference between the highest and the lowest value and divide it by the closing value.\nBy doing that we can get a feeling of how volatile the stock is.\n## Open High Low Close Volume ... Open-Open zscore \\\r## Date ... ## 2021-01-22 67.135 67.135 65.452 66.612 228.940 ... 0.035 3.035618 ## 2021-01-25 66.800 67.133 65.906 66.483 202.718 ... -0.335 3.019540 ## 2021-01-27 66.020 66.586 65.088 66.512 221.619 ... -0.780 3.023155 ## 2021-01-28 66.017 70.380 65.510 67.608 394.160 ... -0.003 3.159755 ## 2021-01-29 67.950 71.109 67.950 69.654 376.318 ... 1.933 3.414759 ## ## zscorevolume PCT_Change HL_PCT ## Date ## 2021-01-22 2.435868 -0.007790 0.025266 ## 2021-01-25 2.035198 -0.004746 0.018456 ## 2021-01-27 2.324004 0.007452 0.022522 ## 2021-01-28 4.960416 0.024100 0.072033 ## 2021-01-29 4.687792 0.025077 0.045353 ## ## [5 rows x 16 columns]\rThese statistical values can be used with many others to get a lot of valuable information about specific stocks. This improves the decision making\n*** MOVING AVERAGE ***\nwe are going to derive the different moving averages . It is the arithmetic mean of all the values of the past n days. Of course this is not the only key statistic that we can derive, but it is the one we are going to use now. We can play around with other functions as well.\nWhat we are going to do with this value is to include it into our data frame and to compare it with the share price of that day.\nFor this, we will first need to create a new column. Pandas does this automatically when we assign values to a column name. This means that we don’t have to explicitly define that we are creating a new column.\n## Close 5d_ma 20d_ma 50d_ma 100d_ma ... 5d_ema 20d_ema \\\r## Date ... ## 2021-01-20 66.966 65.96 67.40 65.06 64.20 ... 66.22 66.58 ## 2021-01-21 67.253 66.08 67.39 65.15 64.19 ... 66.56 66.64 ## 2021-01-22 66.612 66.45 67.34 65.21 64.18 ... 66.58 66.64 ## 2021-01-25 66.483 66.67 67.23 65.27 64.18 ... 66.55 66.62 ## 2021-01-27 66.512 66.77 67.15 65.32 64.16 ... 66.54 66.61 ## 2021-01-28 67.608 66.89 67.11 65.41 64.15 ... 66.89 66.71 ## 2021-01-29 69.654 67.37 67.18 65.55 64.16 ... 67.81 66.99 ## ## 50d_ema 100d_ema 200d_ema ## Date ## 2021-01-20 65.58 63.77 59.75 ## 2021-01-21 65.65 63.84 59.82 ## 2021-01-22 65.68 63.89 59.89 ## 2021-01-25 65.71 63.94 59.95 ## 2021-01-27 65.75 63.99 60.02 ## 2021-01-28 65.82 64.06 60.09 ## 2021-01-29 65.97 64.18 60.19 ## ## [7 rows x 11 columns]\rHere we define a three new columns with the name 20d_ma, 50d_ma, 100d_ma,200d_ma . We now fill this column with the mean values of every n entries. The rolling function stacks a specific amount of entries, in order to make a statistical calculation possible. The window parameter is the one which defines how many entries we are going to stack. But there is also the min_periods parameter. This one defines how many entries we need to have as a minimum in order to perform the calculation. This is relevant because the first entries of our data frame won’t have a n entries previous to them. By setting this value to zero we start the calculations already with the first number, even if there is not a single previous value. This has the effect that the first value will be just the first number, the second one will be the mean of the first two numbers and so on, until we get to a b values.\nBy using the mean function, we are obviously calculating the arithmetic mean. However, we can use a bunch of other functions like max, min or median if we like to.\n*** Standard Deviation ***\nThe variability of the closing stock prices determinies how vo widely prices are dispersed from the average price. If the prices are trading in narrow trading range the standard deviation will return a low value that indicates low volatility. If the prices are trading in wide trading range the standard deviation will return high value that indicates high volatility.\n## Date\r## 2021-01-20 0.741700\r## 2021-01-21 0.885698\r## 2021-01-22 0.898690\r## 2021-01-25 0.886771\r## 2021-01-27 0.606634\r## 2021-01-28 0.534054\r## 2021-01-29 1.118906\r## Name: Std_dev, dtype: float64\r*** Relative Strength Index ***\nThe relative strength index is a indicator of mementum used in technical analysis that measures the magnitude of current price changes to know overbought or oversold conditions in the price of a stock or other asset. If RSI is above 70 then it is overbought. If RSI is below 30 then it is oversold condition.\n## RSI\r## Date ## 2021-01-22 49.627209\r## 2021-01-25 48.780375\r## 2021-01-27 49.000467\r## 2021-01-28 56.878659\r## 2021-01-29 67.441345\r*** Wiliams %R ***\nWilliams %R, or just %R, is a technical analysis oscillator showing the current closing price in relation to the high and low of the past N days.The oscillator is on a negative scale, from −100 (lowest) up to 0 (highest). A value of −100 means the close today was the lowest low of the past N days, and 0 means today\u0026rsquo;s close was the highest high of the past N days.\n## Williams %R\r## Date ## 2021-01-20 -6.135378\r## 2021-01-21 -14.285714\r## 2021-01-22 -31.997789\r## 2021-01-25 -35.562310\r## 2021-01-27 -40.977199\r## 2021-01-28 -52.380952\r## 2021-01-29 -24.165421\rReadings below -80 represent oversold territory and readings above -20 represent overbought.\n*** ADX ***\nADX is used to quantify trend strength. ADX calculations are based on a moving average of price range expansion over a given period of time. The average directional index (ADX) is used to determine when the price is trending strongly.\n 0-25:\tAbsent or Weak Trend\n  25-50:\tStrong Trend\n  50-75:\tVery Strong Trend\n  75-100:\tExtremely Strong Trend\n ## ADX\r## Date ## 2021-01-20 32.699657\r## 2021-01-21 30.242403\r## 2021-01-22 27.866072\r## 2021-01-25 25.829218\r## 2021-01-27 26.050250\r## 2021-01-28 26.843875\r## 2021-01-29 28.442963\r## CCI\r## Date ## 2021-01-20 -20.182727\r## 2021-01-21 10.687223\r## 2021-01-22 -19.066326\r## 2021-01-25 -1.202367\r## 2021-01-27 -17.557455\r## 2021-01-28 197.795282\r## 2021-01-29 251.885779\r*** MACD ***\nMoving Average Convergence Divergence (MACD) is a trend-following momentum indicator that shows the relationship between two moving averages of a security\u0026rsquo;s price. The MACD is calculated by subtracting the 26-period Exponential Moving Average (EMA) from the 12-period EMA.\n## MACD_IND\r## Date ## 2021-01-22 -0.202013\r## 2021-01-25 -0.178572\r## 2021-01-27 -0.155298\r## 2021-01-28 -0.064902\r## 2021-01-29 0.123602\r*** Bollinger Bands ***\nBollinger Bands are a type of statistical chart characterizing the prices and volatility over time of a financial instrument or commodity.\n## 69.56 67.14 64.71\rIn case we choose another value than zero for our min_periods parameter, we will end up with a couple of NaN-Values . These are not a number values and they are useless. Therefore, we would want to delete the entries that have such values.\nWe do this by using the dropna function. If we would have had any entries with NaN values in any column, they would now have been deleted\n3: Predicting the movement of stock\nTo predict the movement of the stock we use 5 lag returns as the dependent variables. The first leg is return yesterday, leg2 is return day before yesterday and so on. The dependent variable is whether the prices went up or down on that day. Other variables include the technical indicators which along with 5 lag returns are used to predict the movement of stock using logistic regression.\n3.1: Creating lag returns\n3.2: Creating returns dataframe\n3.2: create the lagged percentage returns columns\n## Today Lag1 Lag2 Lag3 Lag4 Lag5 \\\r## Date ## 2021-01-15 -2.881529 0.992259 0.189722 0.541728 1.838441 -7.940141 ## 2021-01-18 1.027106 -2.881529 0.992259 0.189722 0.541728 1.838441 ## 2021-01-19 0.931050 1.027106 -2.881529 0.992259 0.189722 0.541728 ## 2021-01-20 1.434436 0.931050 1.027106 -2.881529 0.992259 0.189722 ## 2021-01-21 0.428576 1.434436 0.931050 1.027106 -2.881529 0.992259 ## 2021-01-22 -0.953117 0.428576 1.434436 0.931050 1.027106 -2.881529 ## 2021-01-25 -0.193659 -0.953117 0.428576 1.434436 0.931050 1.027106 ## 2021-01-27 0.043620 -0.193659 -0.953117 0.428576 1.434436 0.931050 ## 2021-01-28 1.647823 0.043620 -0.193659 -0.953117 0.428576 1.434436 ## 2021-01-29 3.026269 1.647823 0.043620 -0.193659 -0.953117 0.428576 ## ## Lag6 ## Date ## 2021-01-15 0.801823 ## 2021-01-18 -7.940141 ## 2021-01-19 1.838441 ## 2021-01-20 0.541728 ## 2021-01-21 0.189722 ## 2021-01-22 0.992259 ## 2021-01-25 -2.881529 ## 2021-01-27 1.027106 ## 2021-01-28 0.931050 ## 2021-01-29 1.434436\r3.3: \u0026ldquo;Direction\u0026rdquo; column (+1 or -1) indicating an up/down day\n***3.4: Create the dependent and independent variables ***\n## C:/Users/slaxm/AppData/Local/r-miniconda/envs/r-reticulate/python.exe:1: SettingWithCopyWarning: ## A value is trying to be set on a copy of a slice from a DataFrame.\r## Try using .loc[row_indexer,col_indexer] = value instead\r## ## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\r## Lag1 Lag2 Lag3 Lag4 Lag5 ... CCI \\\r## Date ... ## 2021-01-22 0.428576 1.434436 0.931050 1.027106 -2.881529 ... -19.066326 ## 2021-01-25 -0.953117 0.428576 1.434436 0.931050 1.027106 ... -1.202367 ## 2021-01-27 -0.193659 -0.953117 0.428576 1.434436 0.931050 ... -17.557455 ## 2021-01-28 0.043620 -0.193659 -0.953117 0.428576 1.434436 ... 197.795282 ## 2021-01-29 1.647823 0.043620 -0.193659 -0.953117 0.428576 ... 251.885779 ## ## ROC DX Open-Close Open-Open ## Date ## 2021-01-22 3.518369 3.329629 -0.118 0.035 ## 2021-01-25 1.452748 3.329629 0.188 -0.335 ## 2021-01-27 0.950126 10.544917 -0.463 -0.780 ## 2021-01-28 2.419294 19.475516 -0.495 -0.003 ## 2021-01-29 4.482045 23.759789 0.342 1.933 ## ## [5 rows x 31 columns]\r3.5: Create training and test sets\n***3.6: Create model ***\n3.7: train the model on the training set\n## LogisticRegression()\r3.8: make an array of predictions on the test set\n3.9: output the hit-rate and the confusion matrix for the model\n## ## Train Accuracy: 98.72%\r## Test Accuracy: 97.64%\r## [[54 1]\r## [ 2 70]]\r***3.10: Predict movement of stock for tomorrow. ***\n## y_test y_pred\r## Date ## 2021-01-22 -1 -1\r## 2021-01-25 -1 -1\r## 2021-01-27 1 1\r## 2021-01-28 1 1\r## 2021-01-29 1 1\r## [1]\r",
    "ref": "/blog/applying-logistic-regression/"
  },{
    "title": "Logistic Regression in ML",
    "date": "",
    "description": "Applying Logistic Regression to datasets",
    "body": "\r\r\rLogistic Regression\rIntroduction\rLogistic regression is a machine learning classification algorithm. The algorithm assigns observations to a set of classes. The logistic regration is used to predict a categorical variabble.\nComparison to linear regression\rIf we are given dataset which contains study time and exam scrores. Then\n\r\rLinear Regression helps to predic the exam scroes which is a continuous\rvariable.\rLogistic Regression can predict if the student\rpassed or failed which is a discrete categorical variable\r\r\r\rTypes of logistic regression\r\r\rBinary (Pass/Fail)\rMulti (Cats, Dogs, Sheep)\rOrdinal (Low, Medium, High)\r\r\r\r\rBinary logistic regression\rIf we are having dataset of student exam results and the objective is to predict student will pass or faile based on hours slept and hours spent studying.\n\r\rStudied\rSlept\rPassed\r\r4.85\r9.63\r1\r\r8.62\r3.23\r0\r\r5.43\r8.23\r1\r\r9.21\r6.34\r0\r\r\r\rGraph of the data\nimage\n\rSigmoid activation\rGenerally probability is assigned to predicted values. For this we use sigmoid function which maps predictions to probabilities.\nMath\n\\[S(z) = \\frac{1} {1 + e^{-z}}\\]\n\rnote\n\r\\(s(z)\\) = output between 0 and 1 (probability estimate)\r\\(z\\) = input to the function (your algorithm’s prediction e.g. mx +\r\r\r\\(e\\) = base of natural log\r\r\rGraph\nimage\n\rCode\nsigmoid \u0026lt;- function(z) {\r#SIGMOID Compute sigmoid functoon\r# J \u0026lt;- SIGMOID(z) computes the sigmoid of z.\r# You need to return the following variables correctly\rz \u0026lt;- as.matrix(z)\rg \u0026lt;- matrix(0,dim(z)[1],dim(z)[2])\r# ----------------------- YOUR CODE HERE -----------------------\r# Instructions: Compute the sigmoid of each value of z (z can be a matrix,\r# vector or scalar).\rg \u0026lt;- 1 / (1 + exp(-1 * z))\rg\r# ----------------------------------------------------\r}\r\rDecision boundary\rThe prediction function returns probability value between 0\rand 1. Map this to a discrete class (true/false) based on some threshold value.\n\\[p \\geq 0.5, class=1 \\\\ p \u0026lt; 0.5, class=0\\]\nFor example, if our threshold is .5 and our prediction function\rreturned .7, we will classify this observation as positive. If our\rprediction is .1 we would classify the observation as negative.\nimage\n\r\rMaking predictions\rTo make predictions we need to find the probability of our observations.\nMath\n\\[z = W_0 + W_1 Studied \\]\nWe can transform the output using the sigmoid function to return a probability value between 0 and 1.\n\\[P(class=1) = \\frac{1} {1 + e^{-z}}\\]\nIf the model returns .3 it believes there is only a 30% chance of\rpassing and this would be classified as fail.\nCode\npredict \u0026lt;- function(theta, X) {\rm \u0026lt;- dim(X)[1] # Number of training examples\rp \u0026lt;- rep(0,m)\rp[sigmoid(X %*% theta) \u0026gt;= 0.5] \u0026lt;- 1\rp\r# ----------------------------------------------------\r}\rA group of 20 students spend between 0 and 6 hours studying for an exam. How does the number of hours spent studying affect the probability that the student will pass the exam?\nHours \u0026lt;- c(0.50, 0.75, 1.00, 1.25, 1.50, 1.75, 1.75, 2.00, 2.25,\r2.50, 2.75, 3.00, 3.25, 3.50, 4.00, 4.25, 4.50, 4.75,\r5.00, 5.50)\rPass \u0026lt;- c(0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1)\rHrsStudying \u0026lt;- data.frame(Hours, Pass)\rThe table shows the number of hours each student spent studying, and whether they passed (1) or failed (0).\nHrsStudying_Table \u0026lt;- t(HrsStudying); HrsStudying_Table\r## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\r## Hours 0.5 0.75 1 1.25 1.5 1.75 1.75 2 2.25 2.5 2.75 3 3.25\r## Pass 0.0 0.00 0 0.00 0.0 0.00 1.00 0 1.00 0.0 1.00 0 1.00\r## [,14] [,15] [,16] [,17] [,18] [,19] [,20]\r## Hours 3.5 4 4.25 4.5 4.75 5 5.5\r## Pass 0.0 1 1.00 1.0 1.00 1 1.0\rThe graph shows the probability of passing the exam versus the number of hours studying, with the logistic regression curve fitted to the data.\nlibrary(ggplot2)\rggplot(HrsStudying, aes(Hours, Pass)) +\rgeom_point(aes()) +\rgeom_smooth(method=\u0026#39;glm\u0026#39;, family=\u0026quot;binomial\u0026quot;, se=FALSE) +\rlabs (x=\u0026quot;Hours Studying\u0026quot;, y=\u0026quot;Probability of Passing Exam\u0026quot;,\rtitle=\u0026quot;Probability of Passing Exam vs Hours Studying\u0026quot;)\r## Warning: Ignoring unknown parameters: family\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rThe logistic regression analysis gives the following output.\nmodel \u0026lt;- glm(Pass ~.,family=binomial(link=\u0026#39;logit\u0026#39;),data=HrsStudying)\rmodel$coefficients\r## (Intercept) Hours ## -4.077713 1.504645\rCoefficient Std.Error z-value P-value (Wald)\nIntercept -4.0777 1.7610 -2.316 0.0206\nHours 1.5046 0.6287 2.393 0.0167\nThe output indicates that hours studying is significantly associated with the probability of passing the exam (p=0.0167, Wald test). The output also provides the coefficients for Intercept = -4.0777 and Hours = 1.5046. These coefficients are entered in the logistic regression equation to estimate the probability of passing the exam:\n\\[P(class=1) = \\frac{1} {1 + e^{-(-4.0777+1.5046* Hours)}}\\]\nFor example, for a student who studies 3 hours, entering the value Hours = 3 in the equation gives the estimated probability of passing the exam of p = 0.60\nStudentHours \u0026lt;- 3\rProbabilityOfPassingExam \u0026lt;- 1/(1+exp(-(-4.0777+1.5046*StudentHours)))\rProbabilityOfPassingExam\r## [1] 0.6073293\rThis table shows the probability of passing the exam for several values of hours studying.\nExamPassTable \u0026lt;- data.frame(column1=c(1, 2, 3, 4, 5),\rcolumn2=c(1/(1+exp(-(-4.0777+1.5046*1))),\r1/(1+exp(-(-4.0777+1.5046*2))),\r1/(1+exp(-(-4.0777+1.5046*3))),\r1/(1+exp(-(-4.0777+1.5046*4))),\r1/(1+exp(-(-4.0777+1.5046*5)))))\rnames(ExamPassTable) \u0026lt;- c(\u0026quot;Hours of study\u0026quot;, \u0026quot;Probability of passing exam\u0026quot;)\rExamPassTable\r## Hours of study Probability of passing exam\r## 1 1 0.07088985\r## 2 2 0.25568845\r## 3 3 0.60732935\r## 4 4 0.87442903\r## 5 5 0.96909067\r\rCost function\rInstead of Mean Squared Error, we use a cost function called Cross-entropy loss can be\rdivided into two separate cost functions: one for \\(y=1\\) and one for\r\\(y=0\\).\nimage\n\rThe benefits of taking the logarithm reveal themselves when you look at\rthe cost function graphs for y=1 and y=0. These smooth monotonic\rfunctions [^2] (always increasing or always decreasing) make it easy to\rcalculate the gradient and minimize cost. Image from Andrew Ng’s slides\ron logistic regression [^3].\nimage\n\rAbove functions compressed into one\nimage\n\rMultiplying by \\(y\\) and \\((1-y)\\) in the above equation is a sneaky trick\rthat let’s us use the same equation to solve for both y=1 and y=0 cases.\rIf y=0, the first side cancels out. If y=1, the second side cancels out.\rIn both cases we only perform the operation we need to perform.\nVectorized cost function\nimage\n\rCode\ncostFunction \u0026lt;- function(X, y) {\r#COSTFUNCTION Compute cost for logistic regression\r# J \u0026lt;- COSTFUNCTION(theta, X, y) computes the cost of using theta as the\r# parameter for logistic regression.\rfunction(theta) {\r# Initialize some useful values\rm \u0026lt;- length(y) # number of training examples\r# You need to return the following variable correctly\rJ \u0026lt;- 0\rh \u0026lt;- sigmoid(X %*% theta)\rJ \u0026lt;- (t(-y) %*% log(h) - t(1 - y) %*% log(1 - h)) / m\rJ\r# ----------------------------------------------------\r}\r}\r\rGradient descent\rRemember that the general form of gradient descent is:\n\\[\\begin{align*}\u0026amp; Repeat \\; \\lbrace \\newline \u0026amp; \\; \\theta_j := \\theta_j - \\alpha \\dfrac{\\partial}{\\partial \\theta_j}J(\\theta) \\newline \u0026amp; \\rbrace\\end{align*}\\]\nWe can do the derivative using calculus to get:\n\\[\\begin{align*} \u0026amp; Repeat \\; \\lbrace \\newline \u0026amp; \\; \\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\newline \u0026amp; \\rbrace \\end{align*}\\]\nA vectorized implementation is:\n\\[\\begin{align*} \\newline \u0026amp; \\; \\theta := \\theta - \\frac{\\alpha}{m} {X^T (g(X\\theta}) - y^ \\rightarrow )\\end{align*}\\]\ngrad \u0026lt;- function(X, y) {\r#COSTFUNCTION Compute gradient for logistic regression\r# J \u0026lt;- COSTFUNCTION(theta, X, y) computes the gradient of the cost\r# w.r.t. to the parameters.\rfunction(theta) {\r# You need to return the following variable correctly\rgrad \u0026lt;- matrix(0,dim(as.matrix(theta)))\rm \u0026lt;- length(y)\rh \u0026lt;- sigmoid(X %*% theta)\r# calculate grads\rgrad \u0026lt;- (t(X) %*% (h - y)) / m\rgrad\r# ----------------------------------------------------\r}\r}\rPseudocode\nRepeat {\r1. Calculate gradient average\r2. Multiply by learning rate\r3. Subtract from weights\r}\rCost history\nimage\n\rAccuracy\nAccuracy measures how correct our predictions\rwere. In this case we simply compare predicted labels to true labels and\rdivide by the total.\nDecision boundary\nAnother helpful technique is to plot the decision boundary on top of our\rpredictions to see how our labels compare to the actual labels. This\rinvolves plotting our predicted probabilities and coloring them with\rtheir true labels.\nimage\n\r\r\rMulticlass logistic regression\rInstead of \\(y = {0,1}\\) we will expand our definition so that\r\\(y = {0,1...n}\\). Basically we re-run binary classification multiple\rtimes, once for each class.\nProcedure\r\rDivide the problem into n+1 binary classification problems (+1\rbecause the index starts at 0?).\rFor each class…\rPredict the probability the observations are in that single class.\rprediction = \u0026lt;math\u0026gt;max(probability of the classes)\r\r\rFor each sub-problem, we select one class (YES) and lump all the others\rinto a second class (NO). Then we take the class with the highest\rpredicted value.\nSince y = {0,1…n}, we divide our problem into n+1 (+1 because the index starts at 0) binary classification problems; in each one, we predict the probability that ‘y’ is a member of one of our classes.\n\\[\\begin{align*}\u0026amp; y \\in \\lbrace0, 1 ... n\\rbrace \\newline\u0026amp; h_\\theta^{(0)}(x) = P(y = 0 | x ; \\theta) \\newline\u0026amp; h_\\theta^{(1)}(x) = P(y = 1 | x ; \\theta) \\newline\u0026amp; \\cdots \\newline\u0026amp; h_\\theta^{(n)}(x) = P(y = n | x ; \\theta) \\newline\u0026amp; \\mathrm{prediction} = \\max_i( h_\\theta ^{(i)}(x) )\\newline\\end{align*}\\]\nSlide show\nknitr::include_url(\u0026#39;/slides/LogisticRegression.html\u0026#39;)\r\r\r\r\r",
    "ref": "/blog/logistic-regression/"
  },{
    "title": "Classification",
    "date": "",
    "description": "Classifcation models a function to predict a specific outcome out of mulitple possible outcomes.",
    "body": "\r\r\rClassification\rWith regression we now predicted specific output-values for certain given input-values. Sometimes, however, we are not trying to predict outputs but to categorize or classify our elements. For this, we use classification algorithms\nIn the figure above, we see one specific kind of classification algorithm, namely the K-Nearest-Neighbor classifier. Here we already have a decent amount of classified elements. We then add a new one (represented by the stars) and try to predict its class by looking at its nearest neighbors.\n\rCLASSIFICATION ALGORITHMS\rThere are various different classification algorithms and they are often used for predicting medical data or other real life use-cases. For example, by providing a large amount of tumor samples, we can classify if a tumor is benign or malignant with a pretty high certainty.\n\rK-NEAREST-NEIGHBORS\rAs already mentioned, by using the K-Nearest-Neighbors classifier, we assign the class of the new object, based on its nearest neighbors. The K specifies the amount of neighbors to look at. For example, we could say that we only want to look at the one neighbor who is nearest but we could also say that we want to factor in 100 neighbors.\rNotice that K shouldn’t be a multiple of the number of classes since it might cause conflicts when we have an equal amount of elements from one class as from the other.\n\rNAIVE-BAYES\rThe Naive Bayes algorithm might be a bit confusing when you encounter it the first time. However, we are only going to discuss the basics and focus more on the implementation in Python later on.\n\r\rOutlook\rTemperture\rHumidity\rWindy\rPlay\r\r\r\rSunny\rHot\rHigh\rFALSE\rNo\r\rSunny\rHot\rHigh\rTRUE\rNo\r\rRainy\rMild\rHigh\rFALSE\rNo\r\rRainy\rHot\rHigh\rTRUE\rNo\r\rOvercast\rHot\rNormal\rTRUE\rYes\r\rSunny\rHot\rNormal\rTRUE\rYes\r\rSunny\rMild\rHigh\rTRUE\rYes\r\rOvercast\rCold\rNormal\rTRUE\rNo\r\r\r\rImagine that we have a table like the one above. We have four input values (which we would have to make numerical of course) and one label or output. The two classes are Yes and No and they indicate if we are going to play outside or not.\nWhat Naive Bayes now does is to write down all the probabilities for the individual scenarios. So we would start by writing the general probability of playing and not playing. In this case, we only play three out of eight times and thus our probability of playing will be 3/8 and the probability of not playing will be 5/8.\nAlso, out of the five times we had a high humidity we only played once, whereas out of the three times it was normal, we played twice. So our probability for playing when we have a high humidity is 1/5 and for playing when we have a medium humidity is 2/3. We go on like that and note all the probabilities we have in our table. To then get the classification for a new entry, we multiply the probabilities together and end up with a prediction.\n\rLOGISTIC REGRESSION\rAnother popular classification algorithm is called logistic regression . Even though the name says regression , this is actually a classification algorithm. It looks at probabilities and determines how likely it is that a certain event happens (or a certain class is the right one), given the input data. This is done by plotting something similar to a logistic growth curve and splitting the data into two.\n\rDECISION TREES\rWith decision tree classifiers, we construct a decision tree out of our training data and use it to predict the classes of new elements.\nSince we are not using a line (and thus our model is not linear), we are also preventing mistakes caused by outliers.\nThis classification algorithm requires very little data preparation and it is also very easy to understand and visualize. On the other hand, it is very easy to be overfitting the model. Here, the model is very closely matched to the training data and thus has worse chances to make a correct prediction on new data.\n\rRANDOM FOREST\rRndom forest classifier is based on decision trees. What it does is creating a forest of multiple decision trees. To classify a new object, all the various trees determine a class and the most frequent result gets chosen. This makes the result more accurate and it also prevents overfitting. It is also more suited to handle data sets with higher dimensions. On the other hand, since the generation of the forest is random , you have very little control over your model.\n\rLOADING DATA\rNow let us get into the code. In this example, we will get our data directly from the sklearn module. For the program we need the following imports:\nimport numpy as np\rimport pandas as pd\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.neighbors import KNeighborsClassifier\rfrom sklearn.datasets import load_breast_cancer \rAt the last import, we import a dataset containing data on breast cancer. Also notice that we are only importing the KNeighborsClassifier for now.\ndata = load_breast_cancer()\rprint (data.feature_names)\r## [\u0026#39;mean radius\u0026#39; \u0026#39;mean texture\u0026#39; \u0026#39;mean perimeter\u0026#39; \u0026#39;mean area\u0026#39;\r## \u0026#39;mean smoothness\u0026#39; \u0026#39;mean compactness\u0026#39; \u0026#39;mean concavity\u0026#39;\r## \u0026#39;mean concave points\u0026#39; \u0026#39;mean symmetry\u0026#39; \u0026#39;mean fractal dimension\u0026#39;\r## \u0026#39;radius error\u0026#39; \u0026#39;texture error\u0026#39; \u0026#39;perimeter error\u0026#39; \u0026#39;area error\u0026#39;\r## \u0026#39;smoothness error\u0026#39; \u0026#39;compactness error\u0026#39; \u0026#39;concavity error\u0026#39;\r## \u0026#39;concave points error\u0026#39; \u0026#39;symmetry error\u0026#39; \u0026#39;fractal dimension error\u0026#39;\r## \u0026#39;worst radius\u0026#39; \u0026#39;worst texture\u0026#39; \u0026#39;worst perimeter\u0026#39; \u0026#39;worst area\u0026#39;\r## \u0026#39;worst smoothness\u0026#39; \u0026#39;worst compactness\u0026#39; \u0026#39;worst concavity\u0026#39;\r## \u0026#39;worst concave points\u0026#39; \u0026#39;worst symmetry\u0026#39; \u0026#39;worst fractal dimension\u0026#39;]\rprint (data.target_names)\r## [\u0026#39;malignant\u0026#39; \u0026#39;benign\u0026#39;]\rtargets, we have two options in this dataset: malignant and benign .\n\rPREPARING DATA\rAgain, we convert our data back into NumPy arrays and split them into training and test data.\nX = np.array(data.data)\rY = np.array(data.target)\rX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.1 )\rThe data attribute refers to our features and the target attribute points to the classes or labels. We again choose a test size of ten percent.\n\rTRAINING AND TESTING\rWe start by first defining our K-Nearest-Neighbors classifier and then training it.\nknn = KNeighborsClassifier( n_neighbors = 5 )\rknn.fit(X_train, Y_train)\r## KNeighborsClassifier()\rThe n_neighbors parameter specifies how many neighbor points we want to consider. In this case, we take five. Then we test our model again for its accuracy.\naccuracy = knn.score(X_test, Y_test)\rprint (accuracy)\r## 0.9473684210526315\rWe get a pretty decent accuracy for such a complex task.\n0.9649122807017544\nTHE BEST ALGORITHM\rNow let’s put all the classification algorithms that we’ve discussed up until now to use and see which one performs best.\nfrom sklearn.neighbors import KNeighborsClassifier\rfrom sklearn.naive_bayes import GaussianNB\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.tree import DecisionTreeClassifier\rfrom sklearn.ensemble import RandomForestClassifier \rclf1 = KNeighborsClassifier( n_neighbors = 5 )\rclf2 = GaussianNB()\rclf3 = LogisticRegression()\rclf4 = DecisionTreeClassifier()\rclf5 = RandomForestClassifier()\rclf1.fit(X_train, Y_train)\r## KNeighborsClassifier()\rclf2.fit(X_train, Y_train)\r## GaussianNB()\rclf3.fit(X_train, Y_train)\r## LogisticRegression()\r## ## C:\\Users\\slaxm\\AppData\\Local\\R-MINI~1\\envs\\R-RETI~1\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\r## STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\r## ## Increase the number of iterations (max_iter) or scale the data as shown in:\r## https://scikit-learn.org/stable/modules/preprocessing.html\r## Please also refer to the documentation for alternative solver options:\r## https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\r## extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\rclf4.fit(X_train, Y_train)\r## DecisionTreeClassifier()\rclf5.fit(X_train, Y_train)\r## RandomForestClassifier()\rprint (clf1.score(X_test, Y_test))\r## 0.9473684210526315\rprint (clf2.score(X_test, Y_test))\r## 0.8947368421052632\rprint (clf3.score(X_test, Y_test))\r## 0.9298245614035088\rprint (clf4.score(X_test, Y_test))\r## 0.9298245614035088\rprint (clf5.score(X_test, Y_test))\r## 0.9473684210526315\rWhen you run this program a couple of times, you will notice that we can’t really say which algorithm is the best. Every time we run this script, we will see different results, at least for this specific data set.\n\rPREDICTING LABELS\rAgain, we can again make predictions for new, unknown data. The chance of success in the classification is even very high. We just need to pass an array of input values and use the predict function .\nX_new = np.array([[...]])\rY_new = clf.predict(X_new)\r\r\r",
    "ref": "/blog/classification/"
  },{
    "title": "Student Grade Prediction",
    "date": "",
    "description": "Predicting student grade using Linear Regression",
    "body": "\r\r\rLinear Regression\rThe easiest and most basic machine learning algorithm is linear regression . It will be the first one that we are going to look at and it is a supervised learning algorithm. That means that we need both – inputs and outputs – to train the model.\nIf we are applying this model to data of schools and we try to find a relation between missing hours, learning time and the resulting grade, we will probably get a less accurate result than by including 30 parameters. Logically, however, we then no longer have a straight line or flat surface but a hyperplane. This is the equivalent to a straight line, in higher dimensions.\n\rLoading Data\rto get started with our code, we first need data that we want to work with. Here we use a dataset from UCI.\nLink: https://archive.ics.uci.edu/ml/datasets/student+performance\nThis is a dataset which contains a lot of information about student performance. We will use it as sample data for our models.\nWe download the ZIP-file from the Data Folder and extract the file student-mat.csv from there into the folder in which we code our script.\nNow we can start with our code. First of all, we will import the necessary libraries.\nimport numpy as np\rimport pandas as pd\rimport matplotlib.pyplot as plt\rfrom sklearn.linear_model import LinearRegression\rfrom sklearn.model_selection import train_test_split\rBesides the imports of the first three libraries that we already know, we have two more imports that are new to us. First, we import the LinearRegression module . This is the module that we will use for creating, training and using our regression model. Additionally, we import the train_test_split module, which we will use to prepare and split our data.\rOur first action is to load the data from the CSV file into a Pandas DataFrame. We do this with the function read_csv.\ndata = pd.read_csv( \u0026#39;../data/student/student-mat.csv\u0026#39; , sep = \u0026#39;;\u0026#39; )\rprint(data.tail())\r## school sex age address famsize Pstatus ... Walc health absences G1 G2 G3\r## 390 MS M 20 U LE3 A ... 5 4 11 9 9 9\r## 391 MS M 17 U LE3 T ... 4 2 3 14 16 16\r## 392 MS M 21 R GT3 T ... 3 3 3 10 8 7\r## 393 MS M 18 R LE3 T ... 4 5 0 11 12 10\r## 394 MS M 19 U LE3 T ... 3 5 5 8 9 9\r## ## [5 rows x 33 columns]\rIt is important that we change our separator to semicolon, since the default separator for CSV files is a comma and our file is separated by semicolons.\nIn the next step, we think about which features (i.e. columns) are relevant for us, and what exactly we want to predict. A description of all features can be found on the previously mentioned website. In this example, we will limit ourselves to the following columns:\nAge, Sex, Studytime, Absences, G1, G2, G3 (label)\ndata = data[[ \u0026#39;age\u0026#39; , \u0026#39;sex\u0026#39; , \u0026#39;studytime\u0026#39; ,\u0026#39;absences\u0026#39; , \u0026#39;G1\u0026#39; , \u0026#39;G2\u0026#39; , \u0026#39;G3\u0026#39; ]]\rThe columns G1, G2 and G3 are the three grades that the students get. Our goal is to predict the\rthird and final grade by looking at the other values like first grade, age, sex and so on.\nSummarized that means that we only select these columns from our DataFrame, out of the 33 possible.\nG3 is our label and the rest are our features. Each feature is an axis in the coordinate system and each point is a record, that is, one row in the table.\rBut we have a little problem here. The sex feature is not numeric, but stored as F (for female) or M (for male) . But for us to work with it and register it in the coordinate system, we have to convert it into numbers.\ndata[ \u0026#39;sex\u0026#39; ] = data[ \u0026#39;sex\u0026#39; ].map({ \u0026#39;F\u0026#39; : 0 , \u0026#39;M\u0026#39; : 1 })\rprediction = \u0026#39;G3\u0026#39; \rWe do this by using the map function. Here, we map a dictionary to our feature. Each F becomes a zero and every M becomes a one. Now we can work with it.\rFinally, we define the column of the desired label as a variable to make it easier to work with.\n\rPREPARING DATA\rOur data is now fully loaded and selected. However, in order to use it as training and testing data for our model, we have to reformat them. The sklearn models do not accept Pandas data frames, but only NumPy arrays. That’s why we turn our features into an x-array and our label into a y-array.\nX = np.array(data.drop([prediction], 1 ))\rY = np.array(data[prediction])\rThe method np.array converts the selected columns into an array. The drop function returns the data frame without the specified column. Our X array now contains all of our columns, except for the final grade. The final grade is in the Y array.\rIn order to train and test our model, we have to split our available data. The first part is used to get the hyperplane to fit our data as well as possible. The second part then checks the accuracy of the prediction, with previously unknown data.\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1 )\rWith the function train_test_split , we divide our X and Y arrays into four arrays. The order must be exactly as shown here. The test_size parameter specifies what percentage of records to use for testing. In this case, it is 10%. This is also a good and recommended value. We do this to test how accurate it is with data that our model has never seen before.\n\rTRAINING AND TESTING\rNow we can start training and testing our model. For that, we first define our model.\nmodel = LinearRegression()\rmodel.fit(X_train, Y_train)\r## LinearRegression()\rBy using the constructor of the LinearRegression class, we create our model. We then use the fit function and pass our training data. Now our model is already trained. It has now adjusted its hyperplane so that it fits all of our values.\rIn order to test how well our model performs, we can use the score method and pass our testing data.\naccuracy = model.score(X_test, Y_test)\rprint (accuracy)\rSince the splitting of training and test data is always random, we will have slightly different results on each run. An average result could look like this:\r0.9130676521162756\rActually, 91 percent is a pretty high and good accuracy. Now that we know that our model is somewhat reliable, we can enter new data and predict the final grade.\nX_new = np.array([[ 18 , 1 , 3 , 40 , 15 , 16 ]])\rY_new = model.predict(X_new)\rprint (Y_new)\r## [17.21890846]\rHere we define a new NumPy array with values for our features in the right order. Then we use the predict method, to calculate the likely final grade for our inputs.\n[17.12142363]\nIn this case, the final grade would probably be 17.\n\r",
    "ref": "/blog/student-grade-prediction/"
  },{
    "title": "Linear Regression",
    "date": "",
    "description": "A linear equation that models a function such that if we give any `x` to it, it will predict a value `y` , where both `x and y` are input and output variables respectively",
    "body": "\r\r\rDefinition\rA linear equation that models a function such that if we give any x to it, it will predict a value y , where both x and y are input and output variables respectively. These are numerical and continous values.\rIt is the most simple and well known algorithm used in machine learning.\n\rFlowchart\r\nThe above Flowchart represents that we choose our training set, feed it to an algorithm, it will learn the patterns and will output a function called Hypothesis function 'H(x)'. We then give any x value to that function and it will output an estimated y value for it.\nFor historical reasons, this function H(x) is called hypothesis function.\n\rCost Function\rThe best fit line to our data will be where we have least distance between the predicted 'y' value and trained 'y' value.\n\rFormula for Cost Function\r\rWhere :\r- h(xi) 👉 hypothesis function\r- yi 👉 actual values of y\r- 1/m 👉 gives Mean of Squared Errors\r- 1/2 👉 Mean is halved as a convenience for the computation of the Gradient Descent.\n\rcomputeCost \u0026lt;- function (X, y, theta){\r# number of training examples\rm \u0026lt;- length(y);\r# need to return\rJ \u0026lt;- 0;\rpredictions \u0026lt;- X %*% theta;\rsqerrors = (predictions - y)^2;\rJ = 1/(2*m)* sum(sqerrors);\rJ\r}\rThis formula inputs the sum of the distances between predicted values and actual values of training set, does sqaure it and take the average and multiply it by 0.5\rThis cost function is also called as Squared Error Function or Mean Squared Error.\rWhy do we take squares of the error’s?\nThe MSE function is commonly used and is a reasonable choice and works well for most Regression problems.\rLet’s subsititute MSE function to function J :\r\n\rGradient Descent\rSo now we have our hypothesis function and we have a way of measuring how well it fits into the data. Now we need to estimate the parameters in the hypothesis function. That’s where Gradient Descent comes in.\nGradient Descent is used to minimize the cost function J, minimizing J is same as minimizing MSE to get best possible fit line to our data.\n\\[\\displaystyle \\min_{\\theta_0,\\theta_1}\\frac{1}{2m}\\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)^2\\]\n\rFormula for Gradient Descent\r\rWhere :\r- := Is the Assignment Operator\r- α is Alpha, it’s the number which is called learning rate. If its too high it may fail to converge and if too low then descending will be slow.\r- ‘θj’ Taking Gradient Descent of a feature or a column of a dataset.\r- ∂/(∂θj) J(θ0,θ1) Taking partial derivative of MSE cost function.\n\r\n gradientDescent \u0026lt;- function(X, y, theta, alpha, num_iters){\rm \u0026lt;- length(y); J_history = rep(0, num_iters);\rfor (iter in 1:num_iters){\rpredictions \u0026lt;- X %*% theta;\rupdates = t(X) %*% (predictions - y);\rtheta = theta - alpha * (1/m) * updates;\rJ_history[iter] \u0026lt;- computeCost(X, y, theta);\r}\rlist(\u0026quot;theta\u0026quot; = theta, \u0026quot;J_history\u0026quot; = J_history) }\rNow Let’s apply Gradient Descend to minmize our MSE function.\rIn order to apply Gradient Descent, we need to figure out the partial derivative term.\nSo lets solve partial derivative of cost function J.\n\n\nNow let’s plug these two values to our Gradient Descent:\n\n\nApplications\r\rSales Forecasting\rDemand Supply Forecasting\rOperations cost optimization\rInsurance industry - claim prediction\rBanking\rHealthcare industry - cost prediction\rEcommerce industry - Recommandation System\r\r\rKey Points\r\rIf sample is small ( \u0026lt; 10000) then normal equation can be used to get the theta values\n\rAs the training set size increases it is better to use gradient descent algorithm instead of normal equation\n\rIf sample data contains large digits for x, y values then it is better to scale the values around mean before applying cost function and gradient descent\n\rIn R language lm(x~y) can be used directly for determining theta values which is more efficient than using gradient descent algorithm\n\rInitially it is better to calculate correlation coeficient to ensure that variables are related in some way\n\rNormalizing data is important to deal with when individual values are numerically large ( \u0026gt; 4 digits)\n\r\r\rSlide show\rknitr::include_url(\u0026#39;/slides/GradientDescentLR.html\u0026#39;)\r\r\rSlide show (Multiple Features)\rknitr::include_url(\u0026#39;/slides/MultipleFeaturesLR.html\u0026#39;)\r\r\r\r",
    "ref": "/blog/linear-regression/"
  },{
    "title": "Time-Series Forecasting",
    "date": "",
    "description": "Introduction to time-series forecasting",
    "body": "\r\r\rWhat is time series?\rA series of observations collected during some time intervals in know as time series. The observations which are collected are primarily dependent on the time at which it is collected. The sale of the items in a retail store like Walmart will be a time series. The periodicity of the the sale could be at daily level or weekly level. The number of people flying from Bangalore to Delhi on daily basis is a time series.\rTime-series forecasting uses historical time based data to forecast a variable for example\n\rAt Supermarket the demand for a grocery product each day\rCount of the downloads of an App in a country for the coming month\rProduct sales in units sold each day in a retail store\rYearly unemployment rate\rThe closing price of a stock\rExpected yearly yield of a crop\rPopulation growth in a country\rDaily demand for a bus/airline/train\r\rTime series and a normal series?\rTime component is important in time series. The time series is primarily dependent on the time. But a normal series say 1, 2, 3…100 has no time component to it. When a value that a series will take depends on the time it was recorded, it is called as time series.\n\rHow to define a time series object using R language\rThe function ts() helps to create time-series objects. As.ts() and is.ts() coerce an object to a time-series and test whether an object is a time series or not\rfor example:\n\rFrequency\rThe count of number of observations per unit of time is called as frequency. Frequency can further be distinguished as Frequency for yearly data is 1, Frequency for weekly data is 7, Frequency for quarterly data is 4 and so on…\n\rSome useful softwares\rforecast: For forecasting functions\n\rtseries: For unit root tests and GARC models\n\rMcomp: Time series data from forecasting competitions\n\rfma: For data\n\rexpsmooth: For data\n\rfpp: For data\n\rARIMA\n\rprophet\n\rSPSS\n\r\r\rprophet\rProphet represents a time-series as a combination of trend, seasonality and holidays. This decomposed time series model can be represented by the following equation:\nCapture10\n\rwhere:\n\rg(t) defines the trend function that models the non-periodic changes in the time series\n\rs(t) defines the periodic changes (e.g., weekly and yearly seasonality)\n\rh(t) means the effect of holidays\n\rεt: is the error notation representing information that was missed by the model\n\r\r\rTrend\rProphet provides two variants for the trend function, g(t) which are logistic growth modeling non-linear growth and constant rate of growth or Piece wise Linear Model.\n\rSeasonality\rs(t) is called as the seasonal component which allows to flexibly model the periodic changes due to weekly and yearly seasonability. For example, a 5-day work week can produce effects on a time series that repeat each week, while vacation schedules and school breaks can produce effects that repeat each year.\n\rHolidays and Events\rThere are events which can provide predictable outcome to business. These events are holidays or any other important ocasion such as black friday.Prophet provides list of such past and future events for modeling into time-series component.\nlibrary(Quandl)\rlibrary(quantmod)\rlibrary(prophet)\rQuandl.api_key(\u0026quot;YBM9uPgpnsaDYPkAn539\u0026quot;)\r#daily_gold = Quandl(\u0026quot;WGC/GOLD_DAILY_INR\u0026quot;,collapse=\u0026quot;daily\u0026quot;,start_date=\u0026quot;2000-01-01\u0026quot;,end_date = \u0026quot;2020-08-11\u0026quot;,type=\u0026quot;raw\u0026quot;)\rdaily_gold = Quandl(\u0026quot;LBMA/SILVER\u0026quot;,collapse=\u0026quot;daily\u0026quot;,start_date=\u0026quot;2020-01-01\u0026quot;,end_date = \u0026quot;2020-08-12\u0026quot;,type=\u0026quot;raw\u0026quot;)\rkeeps \u0026lt;- c(\u0026quot;Date\u0026quot;, \u0026quot;USD\u0026quot;)\rdaily_gold = daily_gold[keeps]\rnames(daily_gold) \u0026lt;- c(\u0026#39;ds\u0026#39;,\u0026#39;y\u0026#39;)\rdaily_gold[2] \u0026lt;- daily_gold[2] model = prophet(daily_gold)\rfuture \u0026lt;- make_future_dataframe(model, periods = 10)\rforecast \u0026lt;- predict(model, future)\rprint(forecast)\r## ds trend additive_terms additive_terms_lower\r## 1 2020-01-02 18.67220 0.1719716 0.1719716\r## 2 2020-01-03 18.63388 0.1920116 0.1920116\r## 3 2020-01-06 18.51890 0.1161517 0.1161517\r## 4 2020-01-07 18.48058 0.1067298 0.1067298\r## 5 2020-01-08 18.44225 0.2188640 0.2188640\r## 6 2020-01-09 18.40392 0.1719716 0.1719716\r## 7 2020-01-10 18.36560 0.1920116 0.1920116\r## 8 2020-01-13 18.25062 0.1161517 0.1161517\r## 9 2020-01-14 18.21230 0.1067298 0.1067298\r## 10 2020-01-15 18.17397 0.2188640 0.2188640\r## 11 2020-01-16 18.13565 0.1719716 0.1719716\r## 12 2020-01-17 18.09732 0.1920116 0.1920116\r## 13 2020-01-20 17.98235 0.1161517 0.1161517\r## 14 2020-01-21 17.94402 0.1067298 0.1067298\r## 15 2020-01-22 17.90570 0.2188640 0.2188640\r## 16 2020-01-23 17.86737 0.1719716 0.1719716\r## 17 2020-01-24 17.82905 0.1920116 0.1920116\r## 18 2020-01-27 17.71407 0.1161517 0.1161517\r## 19 2020-01-28 17.67574 0.1067298 0.1067298\r## 20 2020-01-29 17.63742 0.2188640 0.2188640\r## 21 2020-01-30 17.59909 0.1719716 0.1719716\r## 22 2020-01-31 17.56077 0.1920116 0.1920116\r## 23 2020-02-03 17.44579 0.1161517 0.1161517\r## 24 2020-02-04 17.40747 0.1067298 0.1067298\r## 25 2020-02-05 17.36914 0.2188640 0.2188640\r## 26 2020-02-06 17.33082 0.1719716 0.1719716\r## 27 2020-02-07 17.29176 0.1920116 0.1920116\r## 28 2020-02-10 17.17459 0.1161517 0.1161517\r## 29 2020-02-11 17.13553 0.1067298 0.1067298\r## 30 2020-02-12 17.09647 0.2188640 0.2188640\r## 31 2020-02-13 17.05742 0.1719716 0.1719716\r## 32 2020-02-14 17.00788 0.1920116 0.1920116\r## 33 2020-02-17 16.85928 0.1161517 0.1161517\r## 34 2020-02-18 16.80975 0.1067298 0.1067298\r## 35 2020-02-19 16.76021 0.2188640 0.2188640\r## 36 2020-02-20 16.70263 0.1719716 0.1719716\r## 37 2020-02-21 16.64505 0.1920116 0.1920116\r## 38 2020-02-24 16.47231 0.1161517 0.1161517\r## 39 2020-02-25 16.41472 0.1067298 0.1067298\r## 40 2020-02-26 16.35714 0.2188640 0.2188640\r## 41 2020-02-27 16.29913 0.1719716 0.1719716\r## 42 2020-02-28 16.24113 0.1920116 0.1920116\r## 43 2020-03-02 16.06710 0.1161517 0.1161517\r## 44 2020-03-03 16.00909 0.1067298 0.1067298\r## 45 2020-03-04 15.95108 0.2188640 0.2188640\r## 46 2020-03-05 15.89307 0.1719716 0.1719716\r## 47 2020-03-06 15.83507 0.1920116 0.1920116\r## 48 2020-03-09 15.66104 0.1161517 0.1161517\r## 49 2020-03-10 15.60303 0.1067298 0.1067298\r## 50 2020-03-11 15.54502 0.2188640 0.2188640\r## 51 2020-03-12 15.48701 0.1719716 0.1719716\r## 52 2020-03-13 15.42900 0.1920116 0.1920116\r## 53 2020-03-16 15.25498 0.1161517 0.1161517\r## 54 2020-03-17 15.19697 0.1067298 0.1067298\r## 55 2020-03-18 15.13896 0.2188640 0.2188640\r## 56 2020-03-19 15.08098 0.1719716 0.1719716\r## 57 2020-03-20 15.02300 0.1920116 0.1920116\r## 58 2020-03-23 14.84905 0.1161517 0.1161517\r## 59 2020-03-24 14.79107 0.1067298 0.1067298\r## 60 2020-03-25 14.73308 0.2188640 0.2188640\r## 61 2020-03-26 14.70644 0.1719716 0.1719716\r## 62 2020-03-27 14.67979 0.1920116 0.1920116\r## 63 2020-03-30 14.59985 0.1161517 0.1161517\r## 64 2020-03-31 14.57320 0.1067298 0.1067298\r## 65 2020-04-01 14.54656 0.2188640 0.2188640\r## 66 2020-04-02 14.55587 0.1719716 0.1719716\r## 67 2020-04-03 14.56517 0.1920116 0.1920116\r## 68 2020-04-06 14.59310 0.1161517 0.1161517\r## 69 2020-04-07 14.60241 0.1067298 0.1067298\r## 70 2020-04-08 14.61172 0.2188640 0.2188640\r## 71 2020-04-09 14.64302 0.1719716 0.1719716\r## 72 2020-04-14 14.79949 0.1067298 0.1067298\r## 73 2020-04-15 14.83079 0.2188640 0.2188640\r## 74 2020-04-16 14.86208 0.1719716 0.1719716\r## 75 2020-04-17 14.89338 0.1920116 0.1920116\r## 76 2020-04-20 15.00470 0.1161517 0.1161517\r## 77 2020-04-21 15.04181 0.1067298 0.1067298\r## 78 2020-04-22 15.07892 0.2188640 0.2188640\r## 79 2020-04-23 15.11603 0.1719716 0.1719716\r## 80 2020-04-24 15.15314 0.1920116 0.1920116\r## 81 2020-04-27 15.26628 0.1161517 0.1161517\r## 82 2020-04-28 15.30399 0.1067298 0.1067298\r## 83 2020-04-29 15.34170 0.2188640 0.2188640\r## 84 2020-04-30 15.37942 0.1719716 0.1719716\r## 85 2020-05-01 15.41713 0.1920116 0.1920116\r## 86 2020-05-04 15.53329 0.1161517 0.1161517\r## 87 2020-05-05 15.57200 0.1067298 0.1067298\r## 88 2020-05-06 15.61072 0.2188640 0.2188640\r## 89 2020-05-07 15.64944 0.1719716 0.1719716\r## 90 2020-05-11 15.80432 0.1161517 0.1161517\r## 91 2020-05-12 15.84304 0.1067298 0.1067298\r## 92 2020-05-13 15.88176 0.2188640 0.2188640\r## 93 2020-05-14 15.92048 0.1719716 0.1719716\r## 94 2020-05-15 15.95920 0.1920116 0.1920116\r## 95 2020-05-18 16.07536 0.1161517 0.1161517\r## 96 2020-05-19 16.11408 0.1067298 0.1067298\r## 97 2020-05-20 16.15280 0.2188640 0.2188640\r## 98 2020-05-21 16.19152 0.1719716 0.1719716\r## 99 2020-05-22 16.23024 0.1920116 0.1920116\r## 100 2020-05-26 16.38512 0.1067298 0.1067298\r## 101 2020-05-27 16.42384 0.2188640 0.2188640\r## 102 2020-05-28 16.46256 0.1719716 0.1719716\r## 103 2020-05-29 16.50128 0.1920116 0.1920116\r## 104 2020-06-01 16.61744 0.1161517 0.1161517\r## 105 2020-06-02 16.65616 0.1067298 0.1067298\r## 106 2020-06-03 16.69488 0.2188640 0.2188640\r## 107 2020-06-04 16.73360 0.1719716 0.1719716\r## 108 2020-06-05 16.77232 0.1920116 0.1920116\r## 109 2020-06-08 16.88848 0.1161517 0.1161517\r## 110 2020-06-09 16.92720 0.1067298 0.1067298\r## 111 2020-06-10 16.96592 0.2188640 0.2188640\r## 112 2020-06-11 17.00464 0.1719716 0.1719716\r## 113 2020-06-12 17.04335 0.1920116 0.1920116\r## 114 2020-06-15 17.15951 0.1161517 0.1161517\r## 115 2020-06-16 17.19823 0.1067298 0.1067298\r## 116 2020-06-17 17.23695 0.2188640 0.2188640\r## 117 2020-06-18 17.27567 0.1719716 0.1719716\r## 118 2020-06-19 17.31439 0.1920116 0.1920116\r## 119 2020-06-22 17.43055 0.1161517 0.1161517\r## 120 2020-06-23 17.51737 0.1067298 0.1067298\r## 121 2020-06-24 17.60418 0.2188640 0.2188640\r## 122 2020-06-25 17.69100 0.1719716 0.1719716\r## 123 2020-06-26 17.77781 0.1920116 0.1920116\r## 124 2020-06-29 18.03825 0.1161517 0.1161517\r## 125 2020-06-30 18.21636 0.1067298 0.1067298\r## 126 2020-07-01 18.39447 0.2188640 0.2188640\r## 127 2020-07-02 18.57258 0.1719716 0.1719716\r## 128 2020-07-03 18.75069 0.1920116 0.1920116\r## 129 2020-07-06 19.28501 0.1161517 0.1161517\r## 130 2020-07-07 19.46312 0.1067298 0.1067298\r## 131 2020-07-08 19.64123 0.2188640 0.2188640\r## 132 2020-07-09 19.81934 0.1719716 0.1719716\r## 133 2020-07-10 19.99745 0.1920116 0.1920116\r## 134 2020-07-13 20.53177 0.1161517 0.1161517\r## 135 2020-07-14 20.70988 0.1067298 0.1067298\r## 136 2020-07-15 20.88799 0.2188640 0.2188640\r## 137 2020-07-16 21.06610 0.1719716 0.1719716\r## 138 2020-07-17 21.24421 0.1920116 0.1920116\r## 139 2020-07-20 21.77853 0.1161517 0.1161517\r## 140 2020-07-21 21.95664 0.1067298 0.1067298\r## 141 2020-07-22 22.13475 0.2188640 0.2188640\r## 142 2020-07-23 22.31286 0.1719716 0.1719716\r## 143 2020-07-24 22.49097 0.1920116 0.1920116\r## 144 2020-07-27 23.02529 0.1161517 0.1161517\r## 145 2020-07-28 23.20340 0.1067298 0.1067298\r## 146 2020-07-29 23.38151 0.2188640 0.2188640\r## 147 2020-07-30 23.55962 0.1719716 0.1719716\r## 148 2020-07-31 23.73773 0.1920116 0.1920116\r## 149 2020-08-03 24.27206 0.1161517 0.1161517\r## 150 2020-08-04 24.45016 0.1067298 0.1067298\r## 151 2020-08-05 24.62827 0.2188640 0.2188640\r## 152 2020-08-06 24.80638 0.1719716 0.1719716\r## 153 2020-08-07 24.98449 0.1920116 0.1920116\r## 154 2020-08-10 25.51882 0.1161517 0.1161517\r## 155 2020-08-11 25.69692 0.1067298 0.1067298\r## 156 2020-08-12 25.87503 0.2188640 0.2188640\r## 157 2020-08-13 26.05314 0.1719716 0.1719716\r## 158 2020-08-14 26.23125 0.1920116 0.1920116\r## 159 2020-08-15 26.40936 -0.4028644 -0.4028644\r## 160 2020-08-16 26.58747 -0.4028644 -0.4028644\r## 161 2020-08-17 26.76558 0.1161517 0.1161517\r## 162 2020-08-18 26.94368 0.1067298 0.1067298\r## 163 2020-08-19 27.12179 0.2188640 0.2188640\r## 164 2020-08-20 27.29990 0.1719716 0.1719716\r## 165 2020-08-21 27.47801 0.1920116 0.1920116\r## 166 2020-08-22 27.65612 -0.4028644 -0.4028644\r## additive_terms_upper weekly weekly_lower weekly_upper\r## 1 0.1719716 0.1719716 0.1719716 0.1719716\r## 2 0.1920116 0.1920116 0.1920116 0.1920116\r## 3 0.1161517 0.1161517 0.1161517 0.1161517\r## 4 0.1067298 0.1067298 0.1067298 0.1067298\r## 5 0.2188640 0.2188640 0.2188640 0.2188640\r## 6 0.1719716 0.1719716 0.1719716 0.1719716\r## 7 0.1920116 0.1920116 0.1920116 0.1920116\r## 8 0.1161517 0.1161517 0.1161517 0.1161517\r## 9 0.1067298 0.1067298 0.1067298 0.1067298\r## 10 0.2188640 0.2188640 0.2188640 0.2188640\r## 11 0.1719716 0.1719716 0.1719716 0.1719716\r## 12 0.1920116 0.1920116 0.1920116 0.1920116\r## 13 0.1161517 0.1161517 0.1161517 0.1161517\r## 14 0.1067298 0.1067298 0.1067298 0.1067298\r## 15 0.2188640 0.2188640 0.2188640 0.2188640\r## 16 0.1719716 0.1719716 0.1719716 0.1719716\r## 17 0.1920116 0.1920116 0.1920116 0.1920116\r## 18 0.1161517 0.1161517 0.1161517 0.1161517\r## 19 0.1067298 0.1067298 0.1067298 0.1067298\r## 20 0.2188640 0.2188640 0.2188640 0.2188640\r## 21 0.1719716 0.1719716 0.1719716 0.1719716\r## 22 0.1920116 0.1920116 0.1920116 0.1920116\r## 23 0.1161517 0.1161517 0.1161517 0.1161517\r## 24 0.1067298 0.1067298 0.1067298 0.1067298\r## 25 0.2188640 0.2188640 0.2188640 0.2188640\r## 26 0.1719716 0.1719716 0.1719716 0.1719716\r## 27 0.1920116 0.1920116 0.1920116 0.1920116\r## 28 0.1161517 0.1161517 0.1161517 0.1161517\r## 29 0.1067298 0.1067298 0.1067298 0.1067298\r## 30 0.2188640 0.2188640 0.2188640 0.2188640\r## 31 0.1719716 0.1719716 0.1719716 0.1719716\r## 32 0.1920116 0.1920116 0.1920116 0.1920116\r## 33 0.1161517 0.1161517 0.1161517 0.1161517\r## 34 0.1067298 0.1067298 0.1067298 0.1067298\r## 35 0.2188640 0.2188640 0.2188640 0.2188640\r## 36 0.1719716 0.1719716 0.1719716 0.1719716\r## 37 0.1920116 0.1920116 0.1920116 0.1920116\r## 38 0.1161517 0.1161517 0.1161517 0.1161517\r## 39 0.1067298 0.1067298 0.1067298 0.1067298\r## 40 0.2188640 0.2188640 0.2188640 0.2188640\r## 41 0.1719716 0.1719716 0.1719716 0.1719716\r## 42 0.1920116 0.1920116 0.1920116 0.1920116\r## 43 0.1161517 0.1161517 0.1161517 0.1161517\r## 44 0.1067298 0.1067298 0.1067298 0.1067298\r## 45 0.2188640 0.2188640 0.2188640 0.2188640\r## 46 0.1719716 0.1719716 0.1719716 0.1719716\r## 47 0.1920116 0.1920116 0.1920116 0.1920116\r## 48 0.1161517 0.1161517 0.1161517 0.1161517\r## 49 0.1067298 0.1067298 0.1067298 0.1067298\r## 50 0.2188640 0.2188640 0.2188640 0.2188640\r## 51 0.1719716 0.1719716 0.1719716 0.1719716\r## 52 0.1920116 0.1920116 0.1920116 0.1920116\r## 53 0.1161517 0.1161517 0.1161517 0.1161517\r## 54 0.1067298 0.1067298 0.1067298 0.1067298\r## 55 0.2188640 0.2188640 0.2188640 0.2188640\r## 56 0.1719716 0.1719716 0.1719716 0.1719716\r## 57 0.1920116 0.1920116 0.1920116 0.1920116\r## 58 0.1161517 0.1161517 0.1161517 0.1161517\r## 59 0.1067298 0.1067298 0.1067298 0.1067298\r## 60 0.2188640 0.2188640 0.2188640 0.2188640\r## 61 0.1719716 0.1719716 0.1719716 0.1719716\r## 62 0.1920116 0.1920116 0.1920116 0.1920116\r## 63 0.1161517 0.1161517 0.1161517 0.1161517\r## 64 0.1067298 0.1067298 0.1067298 0.1067298\r## 65 0.2188640 0.2188640 0.2188640 0.2188640\r## 66 0.1719716 0.1719716 0.1719716 0.1719716\r## 67 0.1920116 0.1920116 0.1920116 0.1920116\r## 68 0.1161517 0.1161517 0.1161517 0.1161517\r## 69 0.1067298 0.1067298 0.1067298 0.1067298\r## 70 0.2188640 0.2188640 0.2188640 0.2188640\r## 71 0.1719716 0.1719716 0.1719716 0.1719716\r## 72 0.1067298 0.1067298 0.1067298 0.1067298\r## 73 0.2188640 0.2188640 0.2188640 0.2188640\r## 74 0.1719716 0.1719716 0.1719716 0.1719716\r## 75 0.1920116 0.1920116 0.1920116 0.1920116\r## 76 0.1161517 0.1161517 0.1161517 0.1161517\r## 77 0.1067298 0.1067298 0.1067298 0.1067298\r## 78 0.2188640 0.2188640 0.2188640 0.2188640\r## 79 0.1719716 0.1719716 0.1719716 0.1719716\r## 80 0.1920116 0.1920116 0.1920116 0.1920116\r## 81 0.1161517 0.1161517 0.1161517 0.1161517\r## 82 0.1067298 0.1067298 0.1067298 0.1067298\r## 83 0.2188640 0.2188640 0.2188640 0.2188640\r## 84 0.1719716 0.1719716 0.1719716 0.1719716\r## 85 0.1920116 0.1920116 0.1920116 0.1920116\r## 86 0.1161517 0.1161517 0.1161517 0.1161517\r## 87 0.1067298 0.1067298 0.1067298 0.1067298\r## 88 0.2188640 0.2188640 0.2188640 0.2188640\r## 89 0.1719716 0.1719716 0.1719716 0.1719716\r## 90 0.1161517 0.1161517 0.1161517 0.1161517\r## 91 0.1067298 0.1067298 0.1067298 0.1067298\r## 92 0.2188640 0.2188640 0.2188640 0.2188640\r## 93 0.1719716 0.1719716 0.1719716 0.1719716\r## 94 0.1920116 0.1920116 0.1920116 0.1920116\r## 95 0.1161517 0.1161517 0.1161517 0.1161517\r## 96 0.1067298 0.1067298 0.1067298 0.1067298\r## 97 0.2188640 0.2188640 0.2188640 0.2188640\r## 98 0.1719716 0.1719716 0.1719716 0.1719716\r## 99 0.1920116 0.1920116 0.1920116 0.1920116\r## 100 0.1067298 0.1067298 0.1067298 0.1067298\r## 101 0.2188640 0.2188640 0.2188640 0.2188640\r## 102 0.1719716 0.1719716 0.1719716 0.1719716\r## 103 0.1920116 0.1920116 0.1920116 0.1920116\r## 104 0.1161517 0.1161517 0.1161517 0.1161517\r## 105 0.1067298 0.1067298 0.1067298 0.1067298\r## 106 0.2188640 0.2188640 0.2188640 0.2188640\r## 107 0.1719716 0.1719716 0.1719716 0.1719716\r## 108 0.1920116 0.1920116 0.1920116 0.1920116\r## 109 0.1161517 0.1161517 0.1161517 0.1161517\r## 110 0.1067298 0.1067298 0.1067298 0.1067298\r## 111 0.2188640 0.2188640 0.2188640 0.2188640\r## 112 0.1719716 0.1719716 0.1719716 0.1719716\r## 113 0.1920116 0.1920116 0.1920116 0.1920116\r## 114 0.1161517 0.1161517 0.1161517 0.1161517\r## 115 0.1067298 0.1067298 0.1067298 0.1067298\r## 116 0.2188640 0.2188640 0.2188640 0.2188640\r## 117 0.1719716 0.1719716 0.1719716 0.1719716\r## 118 0.1920116 0.1920116 0.1920116 0.1920116\r## 119 0.1161517 0.1161517 0.1161517 0.1161517\r## 120 0.1067298 0.1067298 0.1067298 0.1067298\r## 121 0.2188640 0.2188640 0.2188640 0.2188640\r## 122 0.1719716 0.1719716 0.1719716 0.1719716\r## 123 0.1920116 0.1920116 0.1920116 0.1920116\r## 124 0.1161517 0.1161517 0.1161517 0.1161517\r## 125 0.1067298 0.1067298 0.1067298 0.1067298\r## 126 0.2188640 0.2188640 0.2188640 0.2188640\r## 127 0.1719716 0.1719716 0.1719716 0.1719716\r## 128 0.1920116 0.1920116 0.1920116 0.1920116\r## 129 0.1161517 0.1161517 0.1161517 0.1161517\r## 130 0.1067298 0.1067298 0.1067298 0.1067298\r## 131 0.2188640 0.2188640 0.2188640 0.2188640\r## 132 0.1719716 0.1719716 0.1719716 0.1719716\r## 133 0.1920116 0.1920116 0.1920116 0.1920116\r## 134 0.1161517 0.1161517 0.1161517 0.1161517\r## 135 0.1067298 0.1067298 0.1067298 0.1067298\r## 136 0.2188640 0.2188640 0.2188640 0.2188640\r## 137 0.1719716 0.1719716 0.1719716 0.1719716\r## 138 0.1920116 0.1920116 0.1920116 0.1920116\r## 139 0.1161517 0.1161517 0.1161517 0.1161517\r## 140 0.1067298 0.1067298 0.1067298 0.1067298\r## 141 0.2188640 0.2188640 0.2188640 0.2188640\r## 142 0.1719716 0.1719716 0.1719716 0.1719716\r## 143 0.1920116 0.1920116 0.1920116 0.1920116\r## 144 0.1161517 0.1161517 0.1161517 0.1161517\r## 145 0.1067298 0.1067298 0.1067298 0.1067298\r## 146 0.2188640 0.2188640 0.2188640 0.2188640\r## 147 0.1719716 0.1719716 0.1719716 0.1719716\r## 148 0.1920116 0.1920116 0.1920116 0.1920116\r## 149 0.1161517 0.1161517 0.1161517 0.1161517\r## 150 0.1067298 0.1067298 0.1067298 0.1067298\r## 151 0.2188640 0.2188640 0.2188640 0.2188640\r## 152 0.1719716 0.1719716 0.1719716 0.1719716\r## 153 0.1920116 0.1920116 0.1920116 0.1920116\r## 154 0.1161517 0.1161517 0.1161517 0.1161517\r## 155 0.1067298 0.1067298 0.1067298 0.1067298\r## 156 0.2188640 0.2188640 0.2188640 0.2188640\r## 157 0.1719716 0.1719716 0.1719716 0.1719716\r## 158 0.1920116 0.1920116 0.1920116 0.1920116\r## 159 -0.4028644 -0.4028644 -0.4028644 -0.4028644\r## 160 -0.4028644 -0.4028644 -0.4028644 -0.4028644\r## 161 0.1161517 0.1161517 0.1161517 0.1161517\r## 162 0.1067298 0.1067298 0.1067298 0.1067298\r## 163 0.2188640 0.2188640 0.2188640 0.2188640\r## 164 0.1719716 0.1719716 0.1719716 0.1719716\r## 165 0.1920116 0.1920116 0.1920116 0.1920116\r## 166 -0.4028644 -0.4028644 -0.4028644 -0.4028644\r## multiplicative_terms multiplicative_terms_lower multiplicative_terms_upper\r## 1 0 0 0\r## 2 0 0 0\r## 3 0 0 0\r## 4 0 0 0\r## 5 0 0 0\r## 6 0 0 0\r## 7 0 0 0\r## 8 0 0 0\r## 9 0 0 0\r## 10 0 0 0\r## 11 0 0 0\r## 12 0 0 0\r## 13 0 0 0\r## 14 0 0 0\r## 15 0 0 0\r## 16 0 0 0\r## 17 0 0 0\r## 18 0 0 0\r## 19 0 0 0\r## 20 0 0 0\r## 21 0 0 0\r## 22 0 0 0\r## 23 0 0 0\r## 24 0 0 0\r## 25 0 0 0\r## 26 0 0 0\r## 27 0 0 0\r## 28 0 0 0\r## 29 0 0 0\r## 30 0 0 0\r## 31 0 0 0\r## 32 0 0 0\r## 33 0 0 0\r## 34 0 0 0\r## 35 0 0 0\r## 36 0 0 0\r## 37 0 0 0\r## 38 0 0 0\r## 39 0 0 0\r## 40 0 0 0\r## 41 0 0 0\r## 42 0 0 0\r## 43 0 0 0\r## 44 0 0 0\r## 45 0 0 0\r## 46 0 0 0\r## 47 0 0 0\r## 48 0 0 0\r## 49 0 0 0\r## 50 0 0 0\r## 51 0 0 0\r## 52 0 0 0\r## 53 0 0 0\r## 54 0 0 0\r## 55 0 0 0\r## 56 0 0 0\r## 57 0 0 0\r## 58 0 0 0\r## 59 0 0 0\r## 60 0 0 0\r## 61 0 0 0\r## 62 0 0 0\r## 63 0 0 0\r## 64 0 0 0\r## 65 0 0 0\r## 66 0 0 0\r## 67 0 0 0\r## 68 0 0 0\r## 69 0 0 0\r## 70 0 0 0\r## 71 0 0 0\r## 72 0 0 0\r## 73 0 0 0\r## 74 0 0 0\r## 75 0 0 0\r## 76 0 0 0\r## 77 0 0 0\r## 78 0 0 0\r## 79 0 0 0\r## 80 0 0 0\r## 81 0 0 0\r## 82 0 0 0\r## 83 0 0 0\r## 84 0 0 0\r## 85 0 0 0\r## 86 0 0 0\r## 87 0 0 0\r## 88 0 0 0\r## 89 0 0 0\r## 90 0 0 0\r## 91 0 0 0\r## 92 0 0 0\r## 93 0 0 0\r## 94 0 0 0\r## 95 0 0 0\r## 96 0 0 0\r## 97 0 0 0\r## 98 0 0 0\r## 99 0 0 0\r## 100 0 0 0\r## 101 0 0 0\r## 102 0 0 0\r## 103 0 0 0\r## 104 0 0 0\r## 105 0 0 0\r## 106 0 0 0\r## 107 0 0 0\r## 108 0 0 0\r## 109 0 0 0\r## 110 0 0 0\r## 111 0 0 0\r## 112 0 0 0\r## 113 0 0 0\r## 114 0 0 0\r## 115 0 0 0\r## 116 0 0 0\r## 117 0 0 0\r## 118 0 0 0\r## 119 0 0 0\r## 120 0 0 0\r## 121 0 0 0\r## 122 0 0 0\r## 123 0 0 0\r## 124 0 0 0\r## 125 0 0 0\r## 126 0 0 0\r## 127 0 0 0\r## 128 0 0 0\r## 129 0 0 0\r## 130 0 0 0\r## 131 0 0 0\r## 132 0 0 0\r## 133 0 0 0\r## 134 0 0 0\r## 135 0 0 0\r## 136 0 0 0\r## 137 0 0 0\r## 138 0 0 0\r## 139 0 0 0\r## 140 0 0 0\r## 141 0 0 0\r## 142 0 0 0\r## 143 0 0 0\r## 144 0 0 0\r## 145 0 0 0\r## 146 0 0 0\r## 147 0 0 0\r## 148 0 0 0\r## 149 0 0 0\r## 150 0 0 0\r## 151 0 0 0\r## 152 0 0 0\r## 153 0 0 0\r## 154 0 0 0\r## 155 0 0 0\r## 156 0 0 0\r## 157 0 0 0\r## 158 0 0 0\r## 159 0 0 0\r## 160 0 0 0\r## 161 0 0 0\r## 162 0 0 0\r## 163 0 0 0\r## 164 0 0 0\r## 165 0 0 0\r## 166 0 0 0\r## yhat_lower yhat_upper trend_lower trend_upper yhat\r## 1 17.41644 20.24155 18.67220 18.67220 18.84417\r## 2 17.31615 20.38588 18.63388 18.63388 18.82589\r## 3 17.29826 20.00924 18.51890 18.51890 18.63505\r## 4 17.11989 20.00281 18.48058 18.48058 18.58730\r## 5 17.21647 20.15874 18.44225 18.44225 18.66111\r## 6 17.20815 19.93784 18.40392 18.40392 18.57590\r## 7 17.20266 20.03088 18.36560 18.36560 18.55761\r## 8 16.87155 19.73565 18.25062 18.25062 18.36678\r## 9 16.99001 19.77607 18.21230 18.21230 18.31903\r## 10 16.98716 19.78964 18.17397 18.17397 18.39284\r## 11 16.82710 19.68362 18.13565 18.13565 18.30762\r## 12 16.89890 19.66100 18.09732 18.09732 18.28933\r## 13 16.75477 19.46560 17.98235 17.98235 18.09850\r## 14 16.65619 19.54965 17.94402 17.94402 18.05075\r## 15 16.68462 19.64238 17.90570 17.90570 18.12456\r## 16 16.59340 19.46455 17.86737 17.86737 18.03934\r## 17 16.54104 19.36742 17.82905 17.82905 18.02106\r## 18 16.34624 19.34911 17.71407 17.71407 17.83022\r## 19 16.27767 19.13838 17.67574 17.67574 17.78247\r## 20 16.51191 19.27700 17.63742 17.63742 17.85628\r## 21 16.42489 19.26834 17.59909 17.59909 17.77107\r## 22 16.23500 19.03627 17.56077 17.56077 17.75278\r## 23 16.14301 18.99679 17.44579 17.44579 17.56194\r## 24 16.01077 18.94865 17.40747 17.40747 17.51420\r## 25 16.26619 18.96448 17.36914 17.36914 17.58801\r## 26 16.09021 18.98643 17.33082 17.33082 17.50279\r## 27 16.05622 18.92530 17.29176 17.29176 17.48377\r## 28 15.83618 18.66045 17.17459 17.17459 17.29074\r## 29 15.80866 18.59028 17.13553 17.13553 17.24226\r## 30 15.90010 18.79902 17.09647 17.09647 17.31534\r## 31 15.75960 18.66246 17.05742 17.05742 17.22939\r## 32 15.86051 18.67817 17.00788 17.00788 17.19989\r## 33 15.65109 18.32269 16.85928 16.85928 16.97543\r## 34 15.58930 18.36183 16.80975 16.80975 16.91648\r## 35 15.65584 18.33159 16.76021 16.76021 16.97908\r## 36 15.44566 18.32967 16.70263 16.70263 16.87460\r## 37 15.38300 18.17769 16.64505 16.64505 16.83706\r## 38 15.15129 18.00002 16.47231 16.47231 16.58846\r## 39 15.11594 17.91640 16.41472 16.41472 16.52145\r## 40 15.16482 18.00890 16.35714 16.35714 16.57601\r## 41 15.12112 17.94214 16.29913 16.29913 16.47111\r## 42 14.96962 17.82329 16.24113 16.24113 16.43314\r## 43 14.79150 17.63239 16.06710 16.06710 16.18325\r## 44 14.66540 17.43861 16.00909 16.00909 16.11582\r## 45 14.63775 17.61623 15.95108 15.95108 16.16995\r## 46 14.66914 17.54617 15.89307 15.89307 16.06505\r## 47 14.55850 17.42838 15.83507 15.83507 16.02708\r## 48 14.27290 17.08489 15.66104 15.66104 15.77719\r## 49 14.29930 17.14506 15.60303 15.60303 15.70976\r## 50 14.28868 17.12078 15.54502 15.54502 15.76389\r## 51 14.17581 16.95898 15.48701 15.48701 15.65898\r## 52 14.17642 17.00095 15.42900 15.42900 15.62102\r## 53 14.05473 16.76189 15.25498 15.25498 15.37113\r## 54 13.94354 16.69294 15.19697 15.19697 15.30370\r## 55 13.94978 16.79619 15.13896 15.13896 15.35782\r## 56 13.91303 16.72400 15.08098 15.08098 15.25295\r## 57 13.82790 16.61997 15.02300 15.02300 15.21501\r## 58 13.57377 16.36860 14.84905 14.84905 14.96520\r## 59 13.46119 16.22103 14.79107 14.79107 14.89780\r## 60 13.49616 16.49598 14.73308 14.73308 14.95195\r## 61 13.38355 16.31412 14.70644 14.70644 14.87841\r## 62 13.49687 16.39160 14.67979 14.67979 14.87180\r## 63 13.35927 16.09236 14.59985 14.59985 14.71600\r## 64 13.32614 15.98741 14.57320 14.57320 14.67993\r## 65 13.24439 16.15105 14.54656 14.54656 14.76542\r## 66 13.28874 16.14364 14.55587 14.55587 14.72784\r## 67 13.48926 16.07240 14.56517 14.56517 14.75719\r## 68 13.38459 16.07173 14.59310 14.59310 14.70925\r## 69 13.27214 16.21675 14.60241 14.60241 14.70914\r## 70 13.42729 16.24055 14.61172 14.61172 14.83058\r## 71 13.33183 16.15217 14.64302 14.64302 14.81499\r## 72 13.49601 16.26082 14.79949 14.79949 14.90622\r## 73 13.54674 16.47619 14.83079 14.83079 15.04965\r## 74 13.65312 16.32397 14.86208 14.86208 15.03405\r## 75 13.55393 16.38738 14.89338 14.89338 15.08539\r## 76 13.68774 16.57683 15.00470 15.00470 15.12086\r## 77 13.65399 16.61498 15.04181 15.04181 15.14854\r## 78 13.91210 16.70268 15.07892 15.07892 15.29779\r## 79 13.85641 16.76272 15.11603 15.11603 15.28800\r## 80 13.94981 16.76451 15.15314 15.15314 15.34515\r## 81 14.05608 16.81168 15.26628 15.26628 15.38243\r## 82 13.91856 16.76846 15.30399 15.30399 15.41072\r## 83 14.16328 16.98224 15.34170 15.34170 15.56057\r## 84 14.23989 17.01519 15.37942 15.37942 15.55139\r## 85 14.16336 17.07244 15.41713 15.41713 15.60914\r## 86 14.18717 17.15089 15.53329 15.53329 15.64944\r## 87 14.29606 17.14108 15.57200 15.57200 15.67873\r## 88 14.40648 17.25719 15.61072 15.61072 15.82959\r## 89 14.50778 17.23154 15.64944 15.64944 15.82141\r## 90 14.52384 17.34338 15.80432 15.80432 15.92047\r## 91 14.59869 17.35125 15.84304 15.84304 15.94977\r## 92 14.55714 17.42189 15.88176 15.88176 16.10062\r## 93 14.78982 17.53806 15.92048 15.92048 16.09245\r## 94 14.85689 17.54490 15.95920 15.95920 16.15121\r## 95 14.73846 17.56227 16.07536 16.07536 16.19151\r## 96 14.79613 17.59867 16.11408 16.11408 16.22081\r## 97 15.01962 17.80810 16.15280 16.15280 16.37166\r## 98 14.97940 17.92737 16.19152 16.19152 16.36349\r## 99 15.04128 17.89787 16.23024 16.23024 16.42225\r## 100 15.10481 17.85542 16.38512 16.38512 16.49185\r## 101 15.31395 18.08176 16.42384 16.42384 16.64270\r## 102 15.14936 18.03070 16.46256 16.46256 16.63453\r## 103 15.28882 18.04689 16.50128 16.50128 16.69329\r## 104 15.23571 18.11914 16.61744 16.61744 16.73359\r## 105 15.39208 18.13537 16.65616 16.65616 16.76289\r## 106 15.53433 18.32039 16.69488 16.69488 16.91374\r## 107 15.46694 18.32831 16.73360 16.73360 16.90557\r## 108 15.58558 18.33504 16.77232 16.77232 16.96433\r## 109 15.63800 18.35929 16.88848 16.88848 17.00463\r## 110 15.72142 18.52004 16.92720 16.92720 17.03393\r## 111 15.84100 18.67727 16.96592 16.96592 17.18478\r## 112 15.68801 18.58300 17.00464 17.00464 17.17661\r## 113 15.87844 18.60107 17.04335 17.04335 17.23537\r## 114 15.87947 18.62799 17.15951 17.15951 17.27567\r## 115 15.90826 18.58102 17.19823 17.19823 17.30496\r## 116 16.03728 18.78726 17.23695 17.23695 17.45582\r## 117 16.01225 18.91367 17.27567 17.27567 17.44765\r## 118 16.14257 18.89917 17.31439 17.31439 17.50641\r## 119 16.10120 18.92398 17.43055 17.43055 17.54671\r## 120 16.23115 18.93439 17.51737 17.51737 17.62410\r## 121 16.36420 19.25076 17.60418 17.60418 17.82305\r## 122 16.38915 19.29523 17.69100 17.69100 17.86297\r## 123 16.57566 19.25733 17.77781 17.77781 17.96982\r## 124 16.65747 19.54801 18.03825 18.03825 18.15440\r## 125 16.81758 19.65139 18.21636 18.21636 18.32309\r## 126 17.15294 20.05535 18.39447 18.39447 18.61333\r## 127 17.23543 20.17887 18.57258 18.57258 18.74455\r## 128 17.51878 20.17124 18.75069 18.75069 18.94270\r## 129 17.88000 20.73016 19.28501 19.28501 19.40116\r## 130 18.13340 21.04623 19.46312 19.46312 19.56985\r## 131 18.41661 21.30806 19.64123 19.64123 19.86009\r## 132 18.61368 21.44537 19.81934 19.81934 19.99131\r## 133 18.81820 21.60545 19.99745 19.99745 20.18946\r## 134 19.31757 22.11467 20.53177 20.53177 20.64793\r## 135 19.29643 22.26512 20.70988 20.70988 20.81661\r## 136 19.72791 22.53511 20.88799 20.88799 21.10685\r## 137 19.84778 22.66348 21.06610 21.06610 21.23807\r## 138 20.14493 22.85215 21.24421 21.24421 21.43622\r## 139 20.50852 23.31586 21.77853 21.77853 21.89469\r## 140 20.67280 23.53838 21.95664 21.95664 22.06337\r## 141 20.89819 23.77421 22.13475 22.13475 22.35362\r## 142 21.09018 23.83789 22.31286 22.31286 22.48483\r## 143 21.20460 24.02347 22.49097 22.49097 22.68298\r## 144 21.73394 24.55587 23.02529 23.02529 23.14145\r## 145 21.85028 24.71197 23.20340 23.20340 23.31013\r## 146 22.23761 24.93391 23.38151 23.38151 23.60038\r## 147 22.34870 25.11997 23.55962 23.55962 23.73159\r## 148 22.48474 25.30101 23.73773 23.73773 23.92974\r## 149 22.92847 25.82033 24.27206 24.27206 24.38821\r## 150 23.12327 25.95254 24.45016 24.45016 24.55689\r## 151 23.46466 26.22074 24.62827 24.62827 24.84714\r## 152 23.43214 26.40297 24.80638 24.80638 24.97835\r## 153 23.75541 26.52802 24.98449 24.98449 25.17650\r## 154 24.18229 27.13152 25.51882 25.51882 25.63497\r## 155 24.37196 27.22953 25.69692 25.69692 25.80365\r## 156 24.68784 27.50650 25.87503 25.87503 26.09390\r## 157 24.89143 27.69131 26.05314 26.05314 26.22511\r## 158 25.04311 27.82584 26.23104 26.23125 26.42326\r## 159 24.54628 27.35972 26.40272 26.41345 26.00649\r## 160 24.87597 27.67926 26.57363 26.59792 26.18460\r## 161 25.45178 28.25800 26.74334 26.78339 26.88173\r## 162 25.59895 28.52797 26.91039 26.97097 27.05041\r## 163 25.90262 28.81007 27.07811 27.16267 27.34066\r## 164 26.11224 28.96532 27.24273 27.35188 27.47187\r## 165 26.23676 29.03529 27.40302 27.54279 27.67002\r## 166 25.89155 28.66725 27.56252 27.73345 27.25326\rplot(model,forecast)\rprophet_plot_components(model,forecast)\rImplementing forcasting using TimeSeries object in R\rlibrary(tseries)\rlibrary(zoo)\rlibrary(forecast)\rlibrary(normwhn.test)\rdat = structure(\rc(1100, 1150, 1200, 1150, 1150, 1200, 1300, 1100, 1200,1250, 1200, 1250, 1300,1400,1550,1400,1450,1450,1450,1450,1500,1550,1600,1650,\r1650,1650,1650,1650,1750,1850,1750,1800,1850,1950,1950,2000,\r1950,2000,2050,2100,2150,2200,2250,2450,2650,2700,2850,2850,\r2750,2750,2750,2750,2800,2800,2850,2850,3000,2950,3000,2950,\r2900,2850,2750,2750,2450,2400,2450,2500,2750,2550,2600,2300,\r2470,2570,2570,2550,2500,2470,2550,2550,2470,2470,2370,2400,\r2870,2648,2656,2600,2656,2608,2460,2644,2564,2508,2524,2508,\r3006,3027,3129,3142,3119,2920,2902,2875,2880,2726,2543,2836,\r2887,2978,2901,2946,2890,2947,2811,2899,3017,2989,2967,2902,\r2994,3046,3074,3100,3141,3002,2934,2901,2908,3003,3097,3083,\r3205,3253,3221,3192,3170,3240,3345,3480,3750,3725,3800,4000,\r3800,4000,4200,4500,4800,5000,5100,5500,5500,5800,6000,6200), .Tsp = c(2008, 2020.91666666667, 12), class = \u0026quot;ts\u0026quot;)\rarimaModel \u0026lt;- arima(dat, order=c(0, 1, 1), list(order=c(0, 1, 0), period = 12))\rsummary(arimaModel)\r## ## Call:\r## arima(x = dat, order = c(0, 1, 1), seasonal = list(order = c(0, 1, 0), period = 12))\r## ## Coefficients:\r## ma1\r## -0.1067\r## s.e. 0.0720\r## ## sigma^2 estimated as 22135: log likelihood = -918.26, aic = 1840.53\r## ## Training set error measures:\r## ME RMSE MAE MPE MAPE MASE ACF1\r## Training set 14.32495 142.4436 105.2636 0.3245391 3.827641 1.189896 -0.02882344\rpreds \u0026lt;- predict(arimaModel, n.ahead = 12)\rprint(preds)\r## $pred\r## Jan Feb Mar Apr May Jun Jul Aug\r## 2021 5998.214 6198.214 6398.214 6698.214 6998.214 7198.214 7298.214 7698.214\r## Sep Oct Nov Dec\r## 2021 7698.214 7998.214 8198.214 8398.214\r## ## $se\r## Jan Feb Mar Apr May Jun Jul Aug\r## 2021 148.7772 199.4956 239.7131 274.0916 304.6146 332.3461 357.9354 381.8136\r## Sep Oct Nov Dec\r## 2021 404.2839 425.5694 445.8398 465.2279\rplot(dat, col = \u0026#39;green\u0026#39;, main = \u0026#39;Acutal vs ARIMA Model\u0026#39;,\rylab = \u0026#39;Gold Rates\u0026#39;, lwd = 3)\rlines(arimaModel$fitted, col = \u0026#39;black\u0026#39;, lwd = 3, lty = 2)\rlegend(\u0026#39;topleft\u0026#39;, legend = c(\u0026#39;Actual\u0026#39;, \u0026#39;ARIMA Fit\u0026#39;),\rcol = c(\u0026#39;green\u0026#39;,\u0026#39;black\u0026#39;), lwd = c(3,3), lty = c(1,2))\rts.plot(dat, preds$pred, main = \u0026#39;Actual vs ARIMA Predictions\u0026#39;,\rcol = c(\u0026#39;green\u0026#39;,\u0026#39;black\u0026#39;), lty = c(1,2), lwd = c(3,3))\rlegend(\u0026#39;topleft\u0026#39;, legend = c(\u0026#39;Actual Data\u0026#39;, \u0026#39;ARIMA Predictions\u0026#39;),\rcol = c(\u0026#39;green\u0026#39;,\u0026#39;black\u0026#39;), lwd = c(3,3), lty = c(1,2))\r\rSummary\rThe main aim of time-series modeling is to carefully collect and regorously study the past observations of the time-series to develop an appropriate model which describes the inherent structure of the series. The model is then used to generate the future values for the series.\rTime series forecasting is thus can be termed as act of predicting the future by understanding the past.\n\r\r\r",
    "ref": "/blog/time-series/"
  },{
    "title": "Text analytics",
    "date": "",
    "description": "The analysis of text data gives useful insigths. This post uses news group data set to investigate text data",
    "body": "\r\r\rProcessing large amounts text data is an important area in natural language processing. The analysis of text data with machine learning tools can give us important insights. Given a text data such as a book, posts or tweets, one may ask questions such as list of common words.\nIn this post we are going to analyse 20 news groups dataset. The Newsgroups dataset comprises around 18000 newsgroups posts on 20 topics. The dataset can by obtained by using fetch_20newsgroups in sklearn.datasets as fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\n1: First step is to get the dataset and look into it to get understanding about how it is organized…\nfrom sklearn.datasets import fetch_20newsgroups\rnewsgroups_full = fetch_20newsgroups(subset=\u0026#39;all\u0026#39;, remove=(\u0026#39;headers\u0026#39;, \u0026#39;footers\u0026#39;, \u0026#39;quotes\u0026#39;), shuffle=True, random_state=42)\rprint(newsgroups_full.keys())\r## dict_keys([\u0026#39;data\u0026#39;, \u0026#39;filenames\u0026#39;, \u0026#39;target_names\u0026#39;, \u0026#39;target\u0026#39;, \u0026#39;DESCR\u0026#39;])\rThe newsgroups_full dataset has properties and function such as keys() which important keys for fetching the details of different types.\rFor example target_names specifies various names of the newsgroups, target is 20 different unique index corresponding to target_names\rthe key data is used to get actual data stored in different files having some filenames. Lets see how go use different keys\n# The target names are the names of the news groups\rprint(newsgroups_full.target_names)\r## [\u0026#39;alt.atheism\u0026#39;, \u0026#39;comp.graphics\u0026#39;, \u0026#39;comp.os.ms-windows.misc\u0026#39;, \u0026#39;comp.sys.ibm.pc.hardware\u0026#39;, \u0026#39;comp.sys.mac.hardware\u0026#39;, \u0026#39;comp.windows.x\u0026#39;, \u0026#39;misc.forsale\u0026#39;, \u0026#39;rec.autos\u0026#39;, \u0026#39;rec.motorcycles\u0026#39;, \u0026#39;rec.sport.baseball\u0026#39;, \u0026#39;rec.sport.hockey\u0026#39;, \u0026#39;sci.crypt\u0026#39;, \u0026#39;sci.electronics\u0026#39;, \u0026#39;sci.med\u0026#39;, \u0026#39;sci.space\u0026#39;, \u0026#39;soc.religion.christian\u0026#39;, \u0026#39;talk.politics.guns\u0026#39;, \u0026#39;talk.politics.mideast\u0026#39;, \u0026#39;talk.politics.misc\u0026#39;, \u0026#39;talk.religion.misc\u0026#39;]\r# The data is actual data stred as list\rprint(newsgroups_full.target_names[newsgroups_full.target[1]])\r## comp.sys.ibm.pc.hardware\rprint(newsgroups_full.data[1])\r## My brother is in the market for a high-performance video card that supports\r## VESA local bus with 1-2MB RAM. Does anyone have suggestions/ideas on:\r## ## - Diamond Stealth Pro Local Bus\r## ## - Orchid Farenheit 1280\r## ## - ATI Graphics Ultra Pro\r## ## - Any other high-performance VLB card\r## ## ## Please post or email. Thank you!\r## ## - Matt\rAs we can se the above two statements give us the data about post belonging to comp.sys.ibm.pc.hardware which contains:\n\r# Putting the words in the dictionary\rnewsgroups_full_dnry = dict()\rfor ind in range(len(newsgroups_full.data)):\rgrp_name = newsgroups_full.target_names[newsgroups_full.target[ind]]\rif grp_name in newsgroups_full_dnry:\rnewsgroups_full_dnry[grp_name] += 1\relse:\rnewsgroups_full_dnry[grp_name] = 1\rprint(\u0026quot;Total number of articles in dataset \u0026quot; + str(len(newsgroups_full.data))) \r## Total number of articles in dataset 18846\rprint(\u0026quot;Number of articles category wise: \u0026quot;)\r## Number of articles category wise:\rprint(newsgroups_full_dnry)\r## {\u0026#39;rec.sport.hockey\u0026#39;: 999, \u0026#39;comp.sys.ibm.pc.hardware\u0026#39;: 982, \u0026#39;talk.politics.mideast\u0026#39;: 940, \u0026#39;comp.sys.mac.hardware\u0026#39;: 963, \u0026#39;sci.electronics\u0026#39;: 984, \u0026#39;talk.religion.misc\u0026#39;: 628, \u0026#39;sci.crypt\u0026#39;: 991, \u0026#39;sci.med\u0026#39;: 990, \u0026#39;alt.atheism\u0026#39;: 799, \u0026#39;rec.motorcycles\u0026#39;: 996, \u0026#39;rec.autos\u0026#39;: 990, \u0026#39;comp.windows.x\u0026#39;: 988, \u0026#39;comp.graphics\u0026#39;: 973, \u0026#39;sci.space\u0026#39;: 987, \u0026#39;talk.politics.guns\u0026#39;: 910, \u0026#39;misc.forsale\u0026#39;: 975, \u0026#39;rec.sport.baseball\u0026#39;: 994, \u0026#39;talk.politics.misc\u0026#39;: 775, \u0026#39;comp.os.ms-windows.misc\u0026#39;: 985, \u0026#39;soc.religion.christian\u0026#39;: 997}\rPie chart of distribution of the articles\nimport matplotlib.pyplot as plt\rlabels = newsgroups_full.target_names\rslices = []\rfor key in newsgroups_full_dnry:\rslices.append(newsgroups_full_dnry[key])\rfig , ax = plt.subplots()\rax.pie(slices, labels = labels , autopct = \u0026#39;%1.1f%%\u0026#39;, shadow = True, startangle = 90)\rax.axis(\u0026quot;equal\u0026quot;)\rax.set_title(\u0026quot;News groups messages distribution\u0026quot;)\rThe distribution of messages posted in different newsgroups is almost similar. The sports groups have most number of messages\nViewing the data as tabular form. We can put the data in the dataframe and see the top ten records\nimport pandas as pd\rdata_labels_map = dict(enumerate(newsgroups_full.target_names))\rmessage, target_labels, target_names = (newsgroups_full.data, newsgroups_full.target, [data_labels_map[label] for label in newsgroups_full.target])\rnewsgroups_full_df = pd.DataFrame({\u0026#39;text\u0026#39;: message, \u0026#39;source\u0026#39;: target_labels, \u0026#39;source_name\u0026#39;: target_names})\rprint(newsgroups_full_df.shape)\r## (18846, 3)\rnewsgroups_full_df.head(10)\r## text ... source_name\r## 0 \\n\\nI am sure some bashers of Pens fans are pr... ... rec.sport.hockey\r## 1 My brother is in the market for a high-perform... ... comp.sys.ibm.pc.hardware\r## 2 \\n\\n\\n\\n\\tFinally you said what you dream abou... ... talk.politics.mideast\r## 3 \\nThink!\\n\\nIt\u0026#39;s the SCSI card doing the DMA t... ... comp.sys.ibm.pc.hardware\r## 4 1) I have an old Jasmine drive which I cann... ... comp.sys.mac.hardware\r## 5 \\n\\nBack in high school I worked as a lab assi... ... sci.electronics\r## 6 \\n\\nAE is in Dallas...try 214/241-6060 or 214/... ... comp.sys.mac.hardware\r## 7 \\n[stuff deleted]\\n\\nOk, here\u0026#39;s the solution t... ... rec.sport.hockey\r## 8 \\n\\n\\nYeah, it\u0026#39;s the second one. And I believ... ... rec.sport.hockey\r## 9 \\nIf a Christian means someone who believes in... ... talk.religion.misc\r## ## [10 rows x 3 columns]\r2: Next step is cleaning the text…\nTo clean the large amounts of text we use nltk tools such as WordNetLemmatizer, PorterStemmer, stopwords, names.\rLets import them first\nimport nltk\rfrom nltk.corpus import names\rfrom nltk.stem import WordNetLemmatizer\rfrom nltk.stem import PorterStemmer\rfrom nltk.corpus import stopwords\rfrom nltk.tokenize import word_tokenize\rimport re\rstopWords = set(stopwords.words(\u0026#39;english\u0026#39;))\rvalidwords = set(nltk.corpus.words.words())\rre is regular expression library in python. We need to first define few functions such as text_tokenizer. The main aim is to clean the posts first by removing the alpha-numeric, numeric and non-alphabatic characters then by applying stemming and lemmmatizing techiniques so that we are left with only the words which are meaningful for the analysis. Lets write the functions for the same\nporter_stemmer = PorterStemmer()\rlemmatizer = WordNetLemmatizer()\rdef text_tokenizer(str_input):\rwords = re.sub(r\u0026quot;[^A-Za-z\\-]\u0026quot;, \u0026quot; \u0026quot;, str_input).lower().split()\rwords = [porter_stemmer.stem(word) for word in words if len(word) \u0026gt; 2 ]\rwords = [lemmatizer.lemmatize(word) for word in words if len(word) \u0026gt; 2 and word in validwords and word not in stopWords]\rreturn \u0026#39; \u0026#39;.join(words)\r2.1: Next is to apply text_tokenizer function to get a new column having clean text…\nnewsgroups_full_df[\u0026#39;clean_text\u0026#39;] = newsgroups_full_df.text.apply(lambda x: text_tokenizer(x))\rnewsgroups_full_df.sort_values(by=[\u0026#39;source\u0026#39;],inplace=True)\rnewsgroups_full_df.head(5)\r## text ... clean_text\r## 8501 \\nI could give much the same testimonial about... ... could give much scout back gay thank well put ...\r## 14285 \\nFine... THE ILLIAD IS THE WORD OF GOD(tm) (... ... fine word god matter prove wrong west\r## 17533 Hello Gang,\\n\\nThere have been some notes rece... ... hello gang note recent ask obtain fish questio...\r## 1527 \\n Sorry, gotta disagree with you on this one... ... one bill prefer half bake bob vice said queen ...\r## 14271 The latest news seems to be that Koresh will g... ... latest news seem give finish write sequel\r## ## [5 rows x 4 columns]\r2.3:Creating a dictionary of newsgroup cleaned text\nwordlst = list()\rnewsgroup_dic = dict()\rlabel = \u0026#39;\u0026#39;\rfor i in range(0,20):\rnewsgroups_full_df_1 = newsgroups_full_df.loc[newsgroups_full_df[\u0026#39;source\u0026#39;] == i]\rfor row in newsgroups_full_df_1[[\u0026#39;source_name\u0026#39;, \u0026#39;clean_text\u0026#39;]].iterrows():\rr = row[1]\rlabel = r.source_name\rwordlst.append(\u0026#39;\u0026#39;.join(map(str,r.clean_text)))\rwordstr = \u0026#39; \u0026#39;.join(map(str, wordlst))\rnewsgroup_dic[label] = wordstr\rlabel = \u0026#39;\u0026#39;\rwordstr = \u0026#39;\u0026#39;\rwordlst.clear() \rNext steps will create the features out of the dictionary of the newsgroups words just created in the previous steps. In natural language processing feature extraction is an important step. In this case the words themselves becomes the features. To extract the features python provides an important library called CountVectorizer. We need to transform our cleaned_text using sklearn.feature_extraction.text and CountVectorizer library. Lets apply it to our newsgroup data.\n3: Feature extraction…\nThe feature vector can be created with sklearn CountVectorizer. When creating the feature vectors we can decide the number of features, as well as set limits for the minimum and maximum number of documents a word can appear.\nNote that the transformed data is stored in a sparse matrix (which is much more efficient for large data sets).\n# First lets import it\rfrom sklearn.feature_extraction.text import CountVectorizer\rcount_vectorizer = CountVectorizer(stop_words = \u0026#39;english\u0026#39;)\rThe function get_word_freq_dict_sorted returns a sorted dictionary of words counts. It taks a dataframe as its argument.\ndef get_word_freq_dict_sorted(ng_X_df):\rwordfreq = ng_X_df.sum(axis=0)\rfeatures = ng_X_df.columns.tolist()\rcounts = wordfreq.tolist()\rwordfreq_df = pd.DataFrame()\rwordfreq_df[\u0026#39;word\u0026#39;] = features\rwordfreq_df[\u0026#39;count\u0026#39;] = counts\rwordfreq_dict = dict(wordfreq_df.values.tolist())\rwordfreqdict_sorted = dict(sorted(wordfreq_dict.items(), key=lambda x: x[1],reverse=True))\rreturn wordfreqdict_sorted\rNow iterate over the newsgroup dictionary obtained from the newsgroups dataframe and create another dictionary where keys are the newsgroups and values are another dictionary of word counts in that newsgroup.\nng_dict_of_words = dict()\rfor key in newsgroup_dic:\rng_X = count_vectorizer.fit_transform([newsgroup_dic[key]])\rng_X_df = pd.DataFrame(ng_X.toarray(), columns=count_vectorizer.get_feature_names())\rng_dict_of_words[key] = get_word_freq_dict_sorted(ng_X_df)\r\r4: Exploring words in the news groups..\nQUESTION: What are the top words in newsgroup comp.sys.ibm.pc.hardware by their count ?\nANSWER: Iterating over the dictionary corresponding to comp.sys.ibm.pc.hardware we get the top ten words as {space orbit launch use like time mission year earth moon}. Like wise we get the most common words in each newsgroup by their count.\nword_dic = ng_dict_of_words[\u0026#39;comp.sys.ibm.pc.hardware\u0026#39;] word_df = pd.DataFrame.from_dict(word_dic, orient=\u0026#39;index\u0026#39;)\rprint(word_df.T.iloc[0:1,0:10])\r## drive use card ani control disk work problem know ide\r## 0 990 792 537 476 441 384 369 356 333 309\rVarious other approaches to explore words in news groups include graphical methods, which help us visualize the distribution of words across news groups. We can use matplotlib.pyplot to draw differnt graphs.\nNext we will explore various algorithms for text classification.\n5 Text Classification…\nText classification is done using various machine learning algorithms. The most popular ones are\n\rMultinomialNB\rLogisticRegression\rSVC\r\rThe goal of the text classification is to predict which newsgroup a post belongs to based on the post text.\nBOW and TF-IDF are two different techniques for text classification\nBag of Words (BoW) is an algorithm that counts frequency of a word in newsgroups. Those word counts allow us to compare different newsgroups and gauge their similarities for applications like search, topic modeling etc.\nIn TF-IDF, words are given weight. TF-IDF measures relevance, not frequency. That is, wordcounts are replaced with TF-IDF scores across the whole dataset.\nTo use text classification algorithm we need to randomly separates data into training and testing dataset and fit the classifier with selected training data. A classifer defines model for text classification. The score gives us the accuracy for testing data.\nDifferent classifiers can give us different results for accuracy. Accuracy depends on the specific problem, number of categories and differences between them, etc.\n6 Evaluation…\nEvaluation of the model can be done using the confusion matrix which can be ploted using the heatmap plot. A basic heatmap is shown below\nnewgroupsheatmap.png\n\rThe confusion matrix depicts the wrongly classified records. For example 4 articles from comp.graphics are wrongly classified as comp.windows.x.\n***7 Slide show\nknitr::include_url(\u0026#39;/slides/NewsGroupsAnalysis.html\u0026#39;)\r\rSummary: Text classifcation has usefull applications in detection of spam pages, personal email sorting, tagging products or document filtering, automatic classification of the text based on its contents, sentiment analysis etc. There are different methods and models availble in sklearn and nltp libraries in python which can be utilized for text classification and natural language processing applications.\n",
    "ref": "/blog/text-analytics/"
  },{
    "title": "EDA with Iris dataset",
    "date": "",
    "description": "Exploring datasets is an important topic in data science. To achieve this task EDA i.e Exploratory data analysis helps",
    "body": "\r\r\rLoading the dataset\rExploring datasets is an important topic in data science. To achieve this task EDA i.e Exploratory data analysis helps by means of summary statistics and other infographics. In this post we will take iris dataset and apply EDA techiniqes to better gain an insight into the dataset.\nfrom sklearn.datasets import load_iris\riris_dataset = load_iris()\rprint(type(iris_dataset))\r## \u0026lt;class \u0026#39;sklearn.utils.Bunch\u0026#39;\u0026gt;\rprint(iris_dataset.keys())\r## dict_keys([\u0026#39;data\u0026#39;, \u0026#39;target\u0026#39;, \u0026#39;frame\u0026#39;, \u0026#39;target_names\u0026#39;, \u0026#39;DESCR\u0026#39;, \u0026#39;feature_names\u0026#39;, \u0026#39;filename\u0026#39;])\rThe dataset is of Bunch datatypes having keys 'data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'.\nBefore we go ahead we need to convert Bunch to pandas DataFrame.\n\rcreating dataframe from raw data\r\rimport pandas as pd\riris_df = pd.DataFrame(iris_dataset.data, columns = iris_dataset.feature_names)\rprint(iris_df.head())\r## sepal length (cm) sepal width (cm) petal length (cm) petal width (cm)\r## 0 5.1 3.5 1.4 0.2\r## 1 4.9 3.0 1.4 0.2\r## 2 4.7 3.2 1.3 0.2\r## 3 4.6 3.1 1.5 0.2\r## 4 5.0 3.6 1.4 0.2\rThe columns shows the length, width but does not show the group to which this length or width belongs. The group to which these values belong is stored in target_names which can stored in seperate column in our dataframe as.\n\rgroup_names = pd.Series([iris_dataset.target_names[ind] for ind in iris_dataset.target], dtype = \u0026#39;category\u0026#39;)\riris_df[\u0026#39;group\u0026#39;] = group_names\rprint(iris_df.head())\r## sepal length (cm) sepal width (cm) ... petal width (cm) group\r## 0 5.1 3.5 ... 0.2 setosa\r## 1 4.9 3.0 ... 0.2 setosa\r## 2 4.7 3.2 ... 0.2 setosa\r## 3 4.6 3.1 ... 0.2 setosa\r## 4 5.0 3.6 ... 0.2 setosa\r## ## [5 rows x 5 columns]\r\rGetting summary statistical measures\rTo start with EDA, mean and median should be calculated for the numeric variables. To get the summary statistics we use describe function.\niris_sumry = iris_df.describe().transpose()\riris_sumry[\u0026#39;std\u0026#39;] = iris_df.std()\riris_sumry.head()\r## count mean std min 25% 50% 75% max\r## sepal length (cm) 150.0 5.843333 0.828066 4.3 5.1 5.80 6.4 7.9\r## sepal width (cm) 150.0 3.057333 0.435866 2.0 2.8 3.00 3.3 4.4\r## petal length (cm) 150.0 3.758000 1.765298 1.0 1.6 4.35 5.1 6.9\r## petal width (cm) 150.0 1.199333 0.762238 0.1 0.3 1.30 1.8 2.5\rThere is no feature which is having a mean as zero. The average sepal width and petal length are not much different. The median of petal length is much different from the mean.\nFor sepal length 25% quartile is 5.1 i.e 25% of the dataset values for this feature are less than 5.1\n\rChecking skewness and kurtosis\rTo check whether the petal length is normaly distributed or not we find the skewness and kurtosis and perform the test as…\nfrom scipy.stats import skew, skewtest\rpetal_length = iris_df[\u0026#39;petal length (cm)\u0026#39;]\rsk = skew(petal_length)\rz_score, p_value = skewtest(petal_length)\rprint(\u0026#39;Skewness %0.3f z-score %0.3f p-value %0.3f\u0026#39; % (sk, z_score, p_value))\r## Skewness -0.272 z-score -1.400 p-value 0.162\r\rfrom scipy.stats import kurtosis, kurtosistest\rpetal_length = iris_df[\u0026#39;petal length (cm)\u0026#39;]\rku = kurtosis(petal_length)\rz_score, p_value = kurtosistest(petal_length)\rprint(\u0026#39;Kurtosis %0.3f z-score %0.3f p-value %0.3f\u0026#39; % (ku, z_score, p_value))\r## Kurtosis -1.396 z-score -14.823 p-value 0.000\rFrom the values of skewness and kurtosis we see that the petal length of plants is skewed to the left.\n\rCreating categorical dataframe\rTo create a categorical dataframe from the quantitative data we can make use of the binning as. Binning transforms the numeric data into categorical data. In EDA this helps in reducing outliers.\n\rperct = [0,.25,.5,.75,1]\riris_bin = pd.concat(\r[pd.qcut(iris_df.iloc[:,0], perct, precision=1),\rpd.qcut(iris_df.iloc[:,1], perct, precision=1),\rpd.qcut(iris_df.iloc[:,2], perct, precision=1),\rpd.qcut(iris_df.iloc[:,3], perct, precision=1)],\rjoin=\u0026#39;outer\u0026#39;, axis = 1)\r\rFrequencies and Contingency tables\rThe resulting frequencies of each class of species in iris dataset can be obtained as …\n\rprint(iris_df[\u0026#39;group\u0026#39;].value_counts())\r## virginica 50\r## versicolor 50\r## setosa 50\r## Name: group, dtype: int64\rThe resultant frequencies for the binned dataframe\n\rprint(iris_bin[\u0026#39;petal length (cm)\u0026#39;].value_counts())\r## (0.9, 1.6] 44\r## (4.4, 5.1] 41\r## (5.1, 6.9] 34\r## (1.6, 4.4] 31\r## Name: petal length (cm), dtype: int64\rWe can describe the binned dataframe using the describe function\niris_bin.describe().transpose()\r## count unique top freq\r## sepal length (cm) 150 4 (4.2, 5.1] 41\r## sepal width (cm) 150 4 (1.9, 2.8] 47\r## petal length (cm) 150 4 (0.9, 1.6] 44\r## petal width (cm) 150 4 (0.0, 0.3] 41\rContingency tables based on groups and binning can be obtained as…\nprint(pd.crosstab(iris_df[\u0026#39;group\u0026#39;], iris_bin[\u0026#39;petal length (cm)\u0026#39;]))\r## petal length (cm) (0.9, 1.6] (1.6, 4.4] (4.4, 5.1] (5.1, 6.9]\r## group ## setosa 44 6 0 0\r## versicolor 0 25 25 0\r## virginica 0 0 16 34\rCross tabulation can further used to apply chi-square test to determine which feature has the effect on the species of the plant. Further chi-square test can help us understand the relationship between target outcome (plant group) and other independant variables (length and width). For example one can setup a chi-squre test to check if the petal length is statistically different from each other i.e values are significantly different across class of species.\n\rApplying t-test to check statistical signifcance\rfrom scipy.stats import ttest_ind\rgrp0 = iris_df[\u0026#39;group\u0026#39;] == \u0026#39;setosa\u0026#39;\rgrp1 = iris_df[\u0026#39;group\u0026#39;] == \u0026#39;versicolor\u0026#39;\rgrp2 = iris_df[\u0026#39;group\u0026#39;] == \u0026#39;virginica\u0026#39;\rpetal_length = iris_df[\u0026#39;petal length (cm)\u0026#39;]\rprint(\u0026#39;var1 %0.3f var2 %03f\u0026#39; % (petal_length[grp1].var(),\rpetal_length[grp2].var()))\r## var1 0.221 var2 0.304588\rsepal_width = iris_df[\u0026#39;sepal width (cm)\u0026#39;]\rt, p_value = ttest_ind(sepal_width[grp1], sepal_width[grp2], axis=0, equal_var=False)\rprint(\u0026#39;t statistic %0.3f p-value %0.3f\u0026#39; % (t, p_value))\r## t statistic -3.206 p-value 0.002\rThe p-value shows that group means are significantly different.\nFurther we can check it among more than 2 groups using ANOVA\n\r\r\rfrom scipy.stats import f_oneway\rsepal_width = iris_df[\u0026#39;sepal width (cm)\u0026#39;]\rf, p_value = f_oneway(sepal_width[grp0],\rsepal_width[grp1],\rsepal_width[grp2])\rprint(\u0026#39;One-way ANOVA F-value %0.3f p-value %0.3f\u0026#39; % (f,p_value))\r## One-way ANOVA F-value 49.160 p-value 0.000\rApplying chi-square to cagegorical variables\r\rfrom scipy.stats import chi2_contingency\rtable = pd.crosstab(iris_df[\u0026#39;group\u0026#39;],\riris_bin[\u0026#39;petal length (cm)\u0026#39;])\rchi2, p, dof, expected = chi2_contingency(table.values)\rprint(\u0026#39;Chi-square %0.2f p-value %0.3f\u0026#39; % (chi2, p))\r## Chi-square 212.43 p-value 0.000\rThe p-value and chi-square value indicates that petal length variable can be effectively used for distinguishing between iris groups.\n\rVisualising data\rCreating box plot\rimport seaborn as sns\rimport matplotlib.pyplot as plt\rsns.boxplot(x=\u0026quot;group\u0026quot;,y=\u0026quot;petal length (cm)\u0026quot;,data=iris_df)\rplt.show()\rThe box plot shows that the 3 groups, setosa, versicolor, and virginica, have different petal lengths.\n\r\rConclusion\rIn this artical we hv seen how to apply to do exploratary data analysis with iris dataset. We also learned the tools that help us understand the relationship between outcome variable and independent variables. We learned various techniqes in EDA that can be used before building the machine learning models.\n\r\r",
    "ref": "/blog/eda-with-iris-dataset/"
  },{
    "title": "Understanding frequency tables",
    "date": "",
    "description": "Frequency tables are a great tool to help explore datasets and get an idea about relationships between variables",
    "body": "\r\r\rIntroduction\rTo discover the relationship between variables is the main task of data analysis. Tools like frequency tables helps to explore the data and get an idea of the relationships between variables. A frequency table is just a data table that shows the counts of one or more categorical variables.\nTo explore frequency tables, we will take the titanic dataset\n\rimport numpy as np\rimport pandas as pd\rimport os\rtit_train = pd.read_csv(\u0026quot;../data/titanic/train.csv\u0026quot;)\rcabin_as_text = tit_train[\u0026#39;Cabin\u0026#39;].astype(str)\rnew_cabin = np.array([cabin[0] for cabin in cabin_as_text])\rtit_train[\u0026quot;Cabin\u0026quot;] = pd.Categorical(new_cabin)\r\rOne-Way Tables\rIn pandas frequency tables are known as crosstabs. Using the pd.crosstab() function we can get the frequency tables. This function takes one or more array type objects as indexes or columns and then constructs a new dataframe of the variable counts based on the supplied arrays.\n\rcross_tab_survived = pd.crosstab(index=tit_train[\u0026quot;Survived\u0026quot;], columns=\u0026quot;count\u0026quot;) print(cross_tab_survived)\r## col_0 count\r## Survived ## 0 549\r## 1 342\rWe can make a more crosstabs to explore other variables\ncross_tab_plcass = pd.crosstab(index=tit_train[\u0026quot;Pclass\u0026quot;], columns=\u0026quot;count\u0026quot;) print(cross_tab_plcass)\r## col_0 count\r## Pclass ## 1 216\r## 2 184\r## 3 491\rcross_tab_sex = pd.crosstab(index=tit_train[\u0026quot;Sex\u0026quot;], columns=\u0026quot;count\u0026quot;) print(cross_tab_sex)\r## col_0 count\r## Sex ## female 314\r## male 577\rcross_tab_cab = pd.crosstab(index=tit_train[\u0026quot;Cabin\u0026quot;], columns=\u0026quot;count\u0026quot;) print(cross_tab_cab)\r## col_0 count\r## Cabin ## A 15\r## B 47\r## C 59\r## D 33\r## E 32\r## F 13\r## G 4\r## T 1\r## n 687\rThe one-way tables give us useful insights. We can see the distribution of records across the categories. For example we find that males are more than females by a significant margin and there were more third class travelers than first and second class combined.\nSince the crosstab function produces DataFrames, the DataFrame operations we’ve learned work on crosstabs:\nprint (cross_tab_cab.sum(), \u0026quot;\\n\u0026quot;) # Sum the counts\r## col_0\r## count 891\r## dtype: int64\rprint (cross_tab_cab.shape, \u0026quot;\\n\u0026quot;) # Check number of rows and cols\r## (9, 1)\rOne of the most useful feature of frequency tables is that they allow to extract the proportion of the data that belongs to each category. With a one-way table, we can do this by dividing each table value by the total number of records in the table.\ncross_tab_cab / cross_tab_cab.sum()\r## col_0 count\r## Cabin ## A 0.016835\r## B 0.052750\r## C 0.066218\r## D 0.037037\r## E 0.035915\r## F 0.014590\r## G 0.004489\r## T 0.001122\r## n 0.771044\rcross_tab_cab\r## col_0 count\r## Cabin ## A 15\r## B 47\r## C 59\r## D 33\r## E 32\r## F 13\r## G 4\r## T 1\r## n 687\r\r\rTwo-Way Tables\rTwo-way frequency tables, also called contingency tables, are tables of counts with two dimensions where each dimension is a different variable. Two-way tables can give us insight into the relationship between two variables. To create a two way table, pass two variables to the pd.crosstab() function instead of one.\n\rTable of survival vs. sex\r\rsurvive_sex = pd.crosstab(index=tit_train[\u0026quot;Survived\u0026quot;], columns=tit_train[\u0026quot;Sex\u0026quot;])\rsurvive_sex.index= [\u0026quot;died\u0026quot;,\u0026quot;survived\u0026quot;]\rsurvive_sex\r## Sex female male\r## died 81 468\r## survived 233 109\r\rTable of survival vs passenger class\r\rsurvive_class = pd.crosstab(index=tit_train[\u0026quot;Survived\u0026quot;], columns=tit_train[\u0026quot;Pclass\u0026quot;])\rsurvive_class.columns = [\u0026quot;class1\u0026quot;,\u0026quot;class2\u0026quot;,\u0026quot;class3\u0026quot;]\rsurvive_class.index= [\u0026quot;died\u0026quot;,\u0026quot;survived\u0026quot;]\rsurvive_class\r## class1 class2 class3\r## died 80 97 372\r## survived 136 87 119\r\rTable of survival vs passenger class\r\r\rsurvived_class = pd.crosstab(index=tit_train[\u0026quot;Survived\u0026quot;], columns=tit_train[\u0026quot;Pclass\u0026quot;],\rmargins=True) # Include row and column totals\rsurvived_class.columns = [\u0026quot;class1\u0026quot;,\u0026quot;class2\u0026quot;,\u0026quot;class3\u0026quot;,\u0026quot;rowtotal\u0026quot;]\rsurvived_class.index= [\u0026quot;died\u0026quot;,\u0026quot;survived\u0026quot;,\u0026quot;coltotal\u0026quot;]\rsurvived_class\r## class1 class2 class3 rowtotal\r## died 80 97 372 549\r## survived 136 87 119 342\r## coltotal 216 184 491 891\rTo get the proportion of counts along each column (in this case, the survival rate within each passenger class) divide by the column totals:\nsurvived_class/survived_class.loc[\u0026quot;coltotal\u0026quot;]\r## class1 class2 class3 rowtotal\r## died 0.37037 0.527174 0.757637 0.616162\r## survived 0.62963 0.472826 0.242363 0.383838\r## coltotal 1.00000 1.000000 1.000000 1.000000\rTo get the proportion of counts along each row divide by the row totals. Use the df.div() to the get division to on a column by column basis:\nsurvived_class.div(survived_class[\u0026quot;rowtotal\u0026quot;],\raxis=0)\r## class1 class2 class3 rowtotal\r## died 0.145719 0.176685 0.677596 1.0\r## survived 0.397661 0.254386 0.347953 1.0\r## coltotal 0.242424 0.206510 0.551066 1.0\rAlternatively, you can transpose the table with df.T to swap rows and columns and perform row by row division as normal:\nsurvived_class.T/survived_class[\u0026quot;rowtotal\u0026quot;]\r## died survived coltotal\r## class1 0.145719 0.397661 0.242424\r## class2 0.176685 0.254386 0.206510\r## class3 0.677596 0.347953 0.551066\r## rowtotal 1.000000 1.000000 1.000000\rHigher Dimensional Tables\rThe crosstab() function lets you create tables out of more than two categories. Higher dimensional tables can be a little confusing to look at, but they can also yield finer-grained insight into interactions between multiple variables. Let’s create a 3-way table inspecting survival, sex and passenger class:\nsurv_sex_class = pd.crosstab(index=tit_train[\u0026quot;Survived\u0026quot;], columns=[tit_train[\u0026quot;Pclass\u0026quot;],\rtit_train[\u0026quot;Sex\u0026quot;]],\rmargins=True) # Include row and column totals\rsurv_sex_class\r## Pclass 1 2 3 All\r## Sex female male female male female male ## Survived ## 0 3 77 6 91 72 300 549\r## 1 91 45 70 17 72 47 342\r## All 94 122 76 108 144 347 891\rNotice that by passing a second variable to the columns argument, the resulting table has columns categorized by both Pclass and Sex.\nsurv_sex_class[2] \r## Sex female male\r## Survived ## 0 6 91\r## 1 70 17\r## All 76 108\rsurv_sex_class[2][\u0026quot;female\u0026quot;] \r## Survived\r## 0 6\r## 1 70\r## All 76\r## Name: female, dtype: int64\rDue to the convenient hierarchical structure of the table, we still use one division to get the proportion of survival across each column:\nsurv_sex_class/surv_sex_class.loc[\u0026quot;All\u0026quot;] # Divide by column totals\r## Pclass 1 2 3 All\r## Sex female male female male female male ## Survived ## 0 0.031915 0.631148 0.078947 0.842593 0.5 0.864553 0.616162\r## 1 0.968085 0.368852 0.921053 0.157407 0.5 0.135447 0.383838\r## All 1.000000 1.000000 1.000000 1.000000 1.0 1.000000 1.000000\rHere we see that over 90% of women in 1st class and 2nd class survived, but only 50% of women in 3rd class survived. Men in 1st class also survived with a greater rate than men in lower classes. Passenger class seems to have a significant impact on survival, so it would likely be useful to include as a feature in a predictive model.\n\r\rSummary\rFrequency tables are effective tools for understanding relationships between features of a dataset. It is easy to inspect the data in the frequency tables. Sometimes creating plots from the frequency tables helps in detecting the patterns in the data.\n\r",
    "ref": "/blog/exploring-frequency-tables/"
  },{
    "title": "Exploring and preparing data",
    "date": "",
    "description": "Exploring and preparing data with titanic dataset",
    "body": "\r\r\r.badCode {\rbackground-color: black;\r}\r.badCode {\rbackground-color: black;\r}\r\rIntroduction\rThe first a part of any data analysis or predictive modeling task is an initial exploration of the datasets. Albeit we collected the datasets ourself and we have already got an inventory of questions in mind that we simply want to answer, it’s important to explore the datasets before doing any serious analysis, since oddities within the datasets can cause bugs and muddle your results. Before exploring deeper questions, we got to answer many simpler ones about the shape and quality of datasets . That said, it’s important to travel into initial data exploration with an enormous picture question in mind.\nThis post aims to boost a number of the questions we ought to consider once we check out a replacement data set for the primary time and show the way to perform various Python operations associated with those questions.\nIn this post, we’ll explore the Titanic disaster training set available from Kaggle.co. The dataset consists of 889 passengers who rode aboard the Titanic.\n\rGetting the dataset\rTo get the dataset into pandas dataframe simply call the function read_csv.\nimport pandas as pd\rimport numpy as np\rtit_train = pd.read_csv(\u0026quot;../data/titanic/train.csv\u0026quot;) # Read the data\r\rChecking the dimensions of the dataset with df.shape and the variable data types of df.dtypes.\ntit_train.shape\r## (891, 12)\rtit_train.dtypes\r## PassengerId int64\r## Survived int64\r## Pclass int64\r## Name object\r## Sex object\r## Age float64\r## SibSp int64\r## Parch int64\r## Ticket object\r## Fare float64\r## Cabin object\r## Embarked object\r## dtype: object\rThe output displays that we re working with a set of 891 records and 12 columns. Most of the column variables are encoded as numeric data types (ints and floats) but a some of them are encoded as “object”.\nCheck the head of the data to get a better sense of what the variables look like:\ntit_train.head(5)\r## PassengerId Survived Pclass ... Fare Cabin Embarked\r## 0 1 0 3 ... 7.2500 NaN S\r## 1 2 1 1 ... 71.2833 C85 C\r## 2 3 1 3 ... 7.9250 NaN S\r## 3 4 1 1 ... 53.1000 C123 S\r## 4 5 0 3 ... 8.0500 NaN S\r## ## [5 rows x 12 columns]\rWe have a combination of numeric columns and columns with text data.\nIn dataset analysis, variables or features that split records into a fixed number of unique categories, such as Sex, are called as categorical variables.\nPandas can be used to interpret categorical variables as such when we load dataset, but we can convert a variable to categorical if necessary\nAfter getting a sense of the datasets structure, it is a good practice to look at a statistical summary of the features with df.describe():\ntit_train.describe().transpose()\r## count mean std ... 50% 75% max\r## PassengerId 891.0 446.000000 257.353842 ... 446.0000 668.5 891.0000\r## Survived 891.0 0.383838 0.486592 ... 0.0000 1.0 1.0000\r## Pclass 891.0 2.308642 0.836071 ... 3.0000 3.0 3.0000\r## Age 714.0 29.699118 14.526497 ... 28.0000 38.0 80.0000\r## SibSp 891.0 0.523008 1.102743 ... 0.0000 1.0 8.0000\r## Parch 891.0 0.381594 0.806057 ... 0.0000 0.0 6.0000\r## Fare 891.0 32.204208 49.693429 ... 14.4542 31.0 512.3292\r## ## [7 rows x 8 columns]\rwe notice that the non-numeric columns are omitted from the statistical summary provided by df.describe().\nWe can find the summary of the categorical variables by passing only those columns to describe():\n\rcat_vars = tit_train.dtypes[tit_train.dtypes == \u0026quot;object\u0026quot;].index\rprint(cat_vars)\r\r## Index([\u0026#39;Name\u0026#39;, \u0026#39;Sex\u0026#39;, \u0026#39;Ticket\u0026#39;, \u0026#39;Cabin\u0026#39;, \u0026#39;Embarked\u0026#39;], dtype=\u0026#39;object\u0026#39;)\rtit_train[cat_vars].describe().transpose()\r## count unique top freq\r## Name 891 891 Meek, Mrs. Thomas (Annie Louise Rowley) 1\r## Sex 891 2 male 577\r## Ticket 891 681 CA. 2343 7\r## Cabin 204 147 C23 C25 C27 4\r## Embarked 889 3 S 644\rThe summary of the categorical features shows the count of non-NaN records, the number of unique categories, the most frequent occurring value and the number of occurrences of the most frequent value.\nAlthough describe() gives a concise overview of each variable, it does not necessarily give us enough information to determine what each variable means.\nCertain features like “Age” and “Fare” are easy to understand, while others like “SibSp” and “Parch” are not. The details of these are provided by kaggle on the data download page.\n# VARIABLE DESCRIPTIONS:\r# survival Survival\r# (0 = No; 1 = Yes)\r# pclass Passenger Class\r# (1 = 1st; 2 = 2nd; 3 = 3rd)\r# name Name\r# sex Sex\r# age Age\r# sibsp Number of Siblings/Spouses Aboard\r# parch Number of Parents/Children Aboard\r# ticket Ticket Number\r# fare Passenger Fare\r# cabin Cabin\r# embarked Port of Embarkation\r# (C = Cherbourg; Q = Queenstown; S = Southampton)\rAfter looking at the data we ask yourself a few questions:\n\r\rQUESTIONS\r\r\r\rDo we require all of the variables ?\r\rShould we transform any variables ?\r\rCheck if there are any NA values, outliers etc ?\r\rShould we create new variables?\r\r\r\rDo we require all of the Variables?\rRemoval of unnecessary variables is a first step when dealing with any data set, since removing variables reduces complexity and can make computation on the data faster.\nWhether we should get rid of a variable or not will depend on size of the data set and the goal of the analysis. With a dataset like the titanic data, there’s no requirement to remove variables from a computing perspective.\rBut it can be helpful to drop variables that will only distract from your goal.\nLet’s go through each variable and consider whether we should keep it or not in the context of survival prediction.\n“PassengerId” is just a number assigned to each passenger. We can remove it\ndel tit_train[\u0026#39;PassengerId\u0026#39;]\rVariable “Survived” shows whether each passenger lived or died. Since survival prediction is our goal, we definitely need to keep it.\nFeatures describing passengers numerically or grouping them into a few broad categories could be useful for survival prediction. Therefore variables Pclass, Sex, Age, SibSp, Parch, Fare and Embarked can be kept.\nfurther, “Name” appears to be a character string of the name of each passenger and it will also help in identifying passenger so we can keep it\nNext, let’s see at “Ticket”\ntit_train[\u0026#39;Ticket\u0026#39;][0:10]\r## 0 A/5 21171\r## 1 PC 17599\r## 2 STON/O2. 3101282\r## 3 113803\r## 4 373450\r## 5 330877\r## 6 17463\r## 7 349909\r## 8 347742\r## 9 237736\r## Name: Ticket, dtype: object\rtit_train[\u0026#39;Ticket\u0026#39;].describe()\r## count 891\r## unique 681\r## top CA. 2343\r## freq 7\r## Name: Ticket, dtype: object\rTicket has 681 unique values: almost as many as there are passengers. Categorical variables with this many levels are generally not very useful for prediction. Let’s remove it\ndel tit_train[\u0026#39;Ticket\u0026#39;]\rLastly let’s see the “Cabin” variable\ntit_train[\u0026#39;Cabin\u0026#39;][0:10]\r## 0 NaN\r## 1 C85\r## 2 NaN\r## 3 C123\r## 4 NaN\r## 5 NaN\r## 6 E46\r## 7 NaN\r## 8 NaN\r## 9 NaN\r## Name: Cabin, dtype: object\rtit_train[\u0026#39;Cabin\u0026#39;].describe()\r## count 204\r## unique 147\r## top C23 C25 C27\r## freq 4\r## Name: Cabin, dtype: object\rCabin also has 147 unique values, which shows it may not be useful for prediction. On the other hand, the names of the different levels for the cabin variable seem to have a some structure, each starts with a capital letter followed by a number. We can use that structure to reduce the number of levels to make categories large enough that they might be useful for prediction later on. So Lets Keep Cabin for now.\n\rShould we transform Any Variables?\rPclass is an integer variable that indicates a passenger’s class, with 1 being first class, 2 as second class and 3 as third class. We can transform this by transforming Pclass into an ordered categorical variable\n\rpclass_new = pd.Categorical(tit_train[\u0026#39;Pclass\u0026#39;], ordered=True)\rpclass_new = pclass_new.rename_categories([\u0026quot;class1\u0026quot;,\u0026quot;class2\u0026quot;,\u0026quot;class3\u0026quot;])\rpclass_new.describe()\r## counts freqs\r## categories ## class1 216 0.242424\r## class2 184 0.206510\r## class3 491 0.551066\rtit_train[\u0026#39;Pclass\u0026#39;] = pclass_new\rNow see the Cabin variable. It appears that each Cabin is in a general section of the ship indicated by the capital letter at the start of each factor level\ntit_train[\u0026#39;Cabin\u0026#39;].unique()\r## array([nan, \u0026#39;C85\u0026#39;, \u0026#39;C123\u0026#39;, \u0026#39;E46\u0026#39;, \u0026#39;G6\u0026#39;, \u0026#39;C103\u0026#39;, \u0026#39;D56\u0026#39;, \u0026#39;A6\u0026#39;,\r## \u0026#39;C23 C25 C27\u0026#39;, \u0026#39;B78\u0026#39;, \u0026#39;D33\u0026#39;, \u0026#39;B30\u0026#39;, \u0026#39;C52\u0026#39;, \u0026#39;B28\u0026#39;, \u0026#39;C83\u0026#39;, \u0026#39;F33\u0026#39;,\r## \u0026#39;F G73\u0026#39;, \u0026#39;E31\u0026#39;, \u0026#39;A5\u0026#39;, \u0026#39;D10 D12\u0026#39;, \u0026#39;D26\u0026#39;, \u0026#39;C110\u0026#39;, \u0026#39;B58 B60\u0026#39;, \u0026#39;E101\u0026#39;,\r## \u0026#39;F E69\u0026#39;, \u0026#39;D47\u0026#39;, \u0026#39;B86\u0026#39;, \u0026#39;F2\u0026#39;, \u0026#39;C2\u0026#39;, \u0026#39;E33\u0026#39;, \u0026#39;B19\u0026#39;, \u0026#39;A7\u0026#39;, \u0026#39;C49\u0026#39;, \u0026#39;F4\u0026#39;,\r## \u0026#39;A32\u0026#39;, \u0026#39;B4\u0026#39;, \u0026#39;B80\u0026#39;, \u0026#39;A31\u0026#39;, \u0026#39;D36\u0026#39;, \u0026#39;D15\u0026#39;, \u0026#39;C93\u0026#39;, \u0026#39;C78\u0026#39;, \u0026#39;D35\u0026#39;,\r## \u0026#39;C87\u0026#39;, \u0026#39;B77\u0026#39;, \u0026#39;E67\u0026#39;, \u0026#39;B94\u0026#39;, \u0026#39;C125\u0026#39;, \u0026#39;C99\u0026#39;, \u0026#39;C118\u0026#39;, \u0026#39;D7\u0026#39;, \u0026#39;A19\u0026#39;,\r## \u0026#39;B49\u0026#39;, \u0026#39;D\u0026#39;, \u0026#39;C22 C26\u0026#39;, \u0026#39;C106\u0026#39;, \u0026#39;C65\u0026#39;, \u0026#39;E36\u0026#39;, \u0026#39;C54\u0026#39;,\r## \u0026#39;B57 B59 B63 B66\u0026#39;, \u0026#39;C7\u0026#39;, \u0026#39;E34\u0026#39;, \u0026#39;C32\u0026#39;, \u0026#39;B18\u0026#39;, \u0026#39;C124\u0026#39;, \u0026#39;C91\u0026#39;, \u0026#39;E40\u0026#39;,\r## \u0026#39;T\u0026#39;, \u0026#39;C128\u0026#39;, \u0026#39;D37\u0026#39;, \u0026#39;B35\u0026#39;, \u0026#39;E50\u0026#39;, \u0026#39;C82\u0026#39;, \u0026#39;B96 B98\u0026#39;, \u0026#39;E10\u0026#39;, \u0026#39;E44\u0026#39;,\r## \u0026#39;A34\u0026#39;, \u0026#39;C104\u0026#39;, \u0026#39;C111\u0026#39;, \u0026#39;C92\u0026#39;, \u0026#39;E38\u0026#39;, \u0026#39;D21\u0026#39;, \u0026#39;E12\u0026#39;, \u0026#39;E63\u0026#39;, \u0026#39;A14\u0026#39;,\r## \u0026#39;B37\u0026#39;, \u0026#39;C30\u0026#39;, \u0026#39;D20\u0026#39;, \u0026#39;B79\u0026#39;, \u0026#39;E25\u0026#39;, \u0026#39;D46\u0026#39;, \u0026#39;B73\u0026#39;, \u0026#39;C95\u0026#39;, \u0026#39;B38\u0026#39;,\r## \u0026#39;B39\u0026#39;, \u0026#39;B22\u0026#39;, \u0026#39;C86\u0026#39;, \u0026#39;C70\u0026#39;, \u0026#39;A16\u0026#39;, \u0026#39;C101\u0026#39;, \u0026#39;C68\u0026#39;, \u0026#39;A10\u0026#39;, \u0026#39;E68\u0026#39;,\r## \u0026#39;B41\u0026#39;, \u0026#39;A20\u0026#39;, \u0026#39;D19\u0026#39;, \u0026#39;D50\u0026#39;, \u0026#39;D9\u0026#39;, \u0026#39;A23\u0026#39;, \u0026#39;B50\u0026#39;, \u0026#39;A26\u0026#39;, \u0026#39;D48\u0026#39;,\r## \u0026#39;E58\u0026#39;, \u0026#39;C126\u0026#39;, \u0026#39;B71\u0026#39;, \u0026#39;B51 B53 B55\u0026#39;, \u0026#39;D49\u0026#39;, \u0026#39;B5\u0026#39;, \u0026#39;B20\u0026#39;, \u0026#39;F G63\u0026#39;,\r## \u0026#39;C62 C64\u0026#39;, \u0026#39;E24\u0026#39;, \u0026#39;C90\u0026#39;, \u0026#39;C45\u0026#39;, \u0026#39;E8\u0026#39;, \u0026#39;B101\u0026#39;, \u0026#39;D45\u0026#39;, \u0026#39;C46\u0026#39;, \u0026#39;D30\u0026#39;,\r## \u0026#39;E121\u0026#39;, \u0026#39;D11\u0026#39;, \u0026#39;E77\u0026#39;, \u0026#39;F38\u0026#39;, \u0026#39;B3\u0026#39;, \u0026#39;D6\u0026#39;, \u0026#39;B82 B84\u0026#39;, \u0026#39;D17\u0026#39;, \u0026#39;A36\u0026#39;,\r## \u0026#39;B102\u0026#39;, \u0026#39;B69\u0026#39;, \u0026#39;E49\u0026#39;, \u0026#39;C47\u0026#39;, \u0026#39;D28\u0026#39;, \u0026#39;E17\u0026#39;, \u0026#39;A24\u0026#39;, \u0026#39;C50\u0026#39;, \u0026#39;B42\u0026#39;,\r## \u0026#39;C148\u0026#39;], dtype=object)\rIf we grouped the cabin just by this letter, we could lesser the number of levels while getting some useful information.\n\rchr_cabin = tit_train[\u0026quot;Cabin\u0026quot;].astype(str)\rn_Cabin = np.array([cabin[0] for cabin in chr_cabin]) n_Cabin = pd.Categorical(n_Cabin)\rn_Cabin.describe()\r## counts freqs\r## categories ## A 15 0.016835\r## B 47 0.052750\r## C 59 0.066218\r## D 33 0.037037\r## E 32 0.035915\r## F 13 0.014590\r## G 4 0.004489\r## T 1 0.001122\r## n 687 0.771044\rThe output of describe() shows we can group Cabin into broader categories, but we also discovered something interesting: 688 of the records have Cabin are “n” which is shortened from “nan”. In other words, more than 2/3 of the passengers do not have a cabin.\nA missing cabin variable could be an indication that a passenger died.\nWe can keep the new variable cabin\ntit_train[\u0026quot;Cabin\u0026quot;] = n_Cabin\r\rChecking to if there are null Values, Outliers or Other garbage Values?\rTo check the missing values we us pd.isnull() function for example\n\rmock_vector = pd.Series([1,None,3,None,7,8])\rmock_vector.isnull()\r## 0 False\r## 1 True\r## 2 False\r## 3 True\r## 4 False\r## 5 False\r## dtype: bool\rIf the missing values are numeric then they can be simple deleted. If missing values are categorical then they can be treated as additional category with value as NA.\nTo check if there is missing age values in titanic dataset\ntit_train[\u0026quot;Age\u0026quot;].describe()\r## count 714.000000\r## mean 29.699118\r## std 14.526497\r## min 0.420000\r## 25% 20.125000\r## 50% 28.000000\r## 75% 38.000000\r## max 80.000000\r## Name: Age, dtype: float64\rWe see that the count of age (712) is less than the total row count of dataset(889).\rTo check indexes of the missing ages we use np.where()\nmissvalues = np.where(tit_train[\u0026quot;Age\u0026quot;].isnull() == True)\rmissvalues\r## (array([ 5, 17, 19, 26, 28, 29, 31, 32, 36, 42, 45, 46, 47,\r## 48, 55, 64, 65, 76, 77, 82, 87, 95, 101, 107, 109, 121,\r## 126, 128, 140, 154, 158, 159, 166, 168, 176, 180, 181, 185, 186,\r## 196, 198, 201, 214, 223, 229, 235, 240, 241, 250, 256, 260, 264,\r## 270, 274, 277, 284, 295, 298, 300, 301, 303, 304, 306, 324, 330,\r## 334, 335, 347, 351, 354, 358, 359, 364, 367, 368, 375, 384, 388,\r## 409, 410, 411, 413, 415, 420, 425, 428, 431, 444, 451, 454, 457,\r## 459, 464, 466, 468, 470, 475, 481, 485, 490, 495, 497, 502, 507,\r## 511, 517, 522, 524, 527, 531, 533, 538, 547, 552, 557, 560, 563,\r## 564, 568, 573, 578, 584, 589, 593, 596, 598, 601, 602, 611, 612,\r## 613, 629, 633, 639, 643, 648, 650, 653, 656, 667, 669, 674, 680,\r## 692, 697, 709, 711, 718, 727, 732, 738, 739, 740, 760, 766, 768,\r## 773, 776, 778, 783, 790, 792, 793, 815, 825, 826, 828, 832, 837,\r## 839, 846, 849, 859, 863, 868, 878, 888], dtype=int64),)\rlen(missvalues)\r## 1\rBefore we do anything with missing values its good to check the distribution of the missing values to know the central tendency of age.\ntit_train.hist(column=\u0026#39;Age\u0026#39;, figsize=(9,6), bins=20) \rThe histogram shows that couple of passengers are near age 80.\nTo check the fare variable we create the box plot\ntit_train[\u0026quot;Fare\u0026quot;].plot(kind=\u0026quot;box\u0026quot;,\rfigsize=(9,9))\r50% of the data in the box plot represents the median. There are outliers in the data. There are passengers who paid double the amount than any other passenger. We can check this using np.where() function\nind = np.where(tit_train[\u0026quot;Fare\u0026quot;] == max(tit_train[\u0026quot;Fare\u0026quot;]) )\rtit_train.loc[ind]\r## Survived Pclass ... Cabin Embarked\r## 258 1 class1 ... n C\r## 679 1 class1 ... B C\r## 737 1 class1 ... B C\r## ## [3 rows x 10 columns]\rBefore modeling datasets using ML models it is better to address missing values, outliers, mislabeled data, bad data because they can corrupt the analysis and lead to wrong results.\n\rShould we Create New Variables?\rThe decision to create new variables should be taken while preparing the data. The new variable could represent aggregate of existing variables, for example in titanic dataset we can create a new variable called family which stores the number of members in that family.\ntit_train[\u0026quot;Family\u0026quot;] = tit_train[\u0026quot;SibSp\u0026quot;] + tit_train[\u0026quot;Parch\u0026quot;]\rwe can check who has most family members on the board\nmostfamily = np.where(tit_train[\u0026quot;Family\u0026quot;] == max(tit_train[\u0026quot;Family\u0026quot;]))\rtit_train.loc[mostfamily]\r## Survived Pclass Name ... Cabin Embarked Family\r## 159 0 class3 Sage, Master. Thomas Henry ... n S 10\r## 180 0 class3 Sage, Miss. Constance Gladys ... n S 10\r## 201 0 class3 Sage, Mr. Frederick ... n S 10\r## 324 0 class3 Sage, Mr. George John Jr ... n S 10\r## 792 0 class3 Sage, Miss. Stella Anna ... n S 10\r## 846 0 class3 Sage, Mr. Douglas Bullen ... n S 10\r## 863 0 class3 Sage, Miss. Dorothy Edith \u0026quot;Dolly\u0026quot; ... n S 10\r## ## [7 rows x 11 columns]\r\r\rSummary\rThere are question that should be answered while investing any dataset. Once the basic questions are answered one can move further to find relationship between variables/features and build the machine learning models.\n\r",
    "ref": "/blog/exploring-and-preparing-data/"
  },{
    "title": "Handling numeric data",
    "date": "",
    "description": "Handling numeric data using mtcars dataset. Numeric data is easiar to deal with in Data analysis projects",
    "body": "\r\r\rIntroduction\rIn data analysis projects Numeric data present is very different from the text data. The numeric data is relatively clean than the text data that is why it is easiear to deal with them. In this post, we’ll learn few common operations used to prepare numeric data for use in analysis and predictive models using mtcars dataset.\nGetting the dataset\rTo get the dataset into pandas dataframe simply call the function read_csv.\nimport pandas as pd\rimport numpy as np\rmt_car = pd.read_csv(\u0026quot;../data/mtcars/mt_car.csv\u0026quot;) # Read the data\r\rCenter and Scale\rTo center and scale the dataset we substract the mean value from each data point. Subtracting the mean centers the data around zero and sets the new mean to zero. Lets try do it with mtcars dataset.\n\rprint (mt_car.head() )\r## m_pg n_cyl disp_ment n_hp dra w_t q_sec v_s a_m n_gear n_carb\r## 0 21.0 6.0 160.0 110.0 3.90 2.62 16.46 0.0 1.0 4.0 4.0\r## 1 21.0 6.0 160.0 110.0 3.90 2.88 17.02 0.0 1.0 4.0 4.0\r## 2 22.8 4.0 108.0 93.0 3.85 2.32 18.61 1.0 1.0 4.0 1.0\r## 3 21.4 6.0 258.0 110.0 3.08 3.22 19.44 1.0 0.0 3.0 1.0\r## 4 18.7 8.0 360.0 175.0 3.15 3.44 17.02 0.0 0.0 3.0 2.0\rcol_means = mt_car.sum()/mt_car.shape[0] # Get column means\rcol_means\r## m_pg 20.090625\r## n_cyl 6.187500\r## disp_ment 230.721875\r## n_hp 146.687500\r## dra 3.596563\r## w_t 3.218437\r## q_sec 17.848750\r## v_s 0.437500\r## a_m 0.406250\r## n_gear 3.687500\r## n_carb 2.812500\r## dtype: float64\rNow we need to subtract the means of the column from each row in element-wise way to zero center the data. Pandas can peform math operations involving dataframews and columns on element-wise row-by-row basis by default so it can be simply subtracted from column means series from the dataset to center it.\n\rcenter_ed = mt_car - col_means\rprint(center_ed.describe())\r## m_pg n_cyl disp_ment ... a_m n_gear n_carb\r## count 3.200000e+01 32.000000 3.200000e+01 ... 32.000000 32.000000 32.0000\r## mean 3.996803e-15 0.000000 -4.618528e-14 ... 0.000000 0.000000 0.0000\r## std 6.026948e+00 1.785922 1.239387e+02 ... 0.498991 0.737804 1.6152\r## min -9.690625e+00 -2.187500 -1.596219e+02 ... -0.406250 -0.687500 -1.8125\r## 25% -4.665625e+00 -2.187500 -1.098969e+02 ... -0.406250 -0.687500 -0.8125\r## 50% -8.906250e-01 -0.187500 -3.442188e+01 ... -0.406250 0.312500 -0.8125\r## 75% 2.709375e+00 1.812500 9.527812e+01 ... 0.593750 0.312500 1.1875\r## max 1.380938e+01 1.812500 2.412781e+02 ... 0.593750 1.312500 5.1875\r## ## [8 rows x 11 columns]\rAfter centering the data we see that negative values are below average while positive values are above average. Next we can put it on common scale using the standard deviation as.\n\rcol_deviations = mt_car.std(axis=0) # Get column standard deviations\rcenter_n_scale = center_ed/col_deviations print(center_n_scale.describe())\r## m_pg n_cyl ... n_gear n_carb\r## count 3.200000e+01 3.200000e+01 ... 3.200000e+01 3.200000e+01\r## mean 6.661338e-16 -2.775558e-17 ... -2.775558e-17 2.775558e-17\r## std 1.000000e+00 1.000000e+00 ... 1.000000e+00 1.000000e+00\r## min -1.607883e+00 -1.224858e+00 ... -9.318192e-01 -1.122152e+00\r## 25% -7.741273e-01 -1.224858e+00 ... -9.318192e-01 -5.030337e-01\r## 50% -1.477738e-01 -1.049878e-01 ... 4.235542e-01 -5.030337e-01\r## 75% 4.495434e-01 1.014882e+00 ... 4.235542e-01 7.352031e-01\r## max 2.291272e+00 1.014882e+00 ... 1.778928e+00 3.211677e+00\r## ## [8 rows x 11 columns]\rWe see that after dividing by the standard deviation, every variable now has a standard deviation of 1. At this point, all the columns have roughly the same mean and scale of spread about the mean.\nManually centering and scaling is a good exercise, but it is often possible to perform common data preprocessing using functions available in the Python libraries. The Python library scikit-learn, a package for predictive modeling and data analysis, has pre-processing tools including a scale() function for centering and scaling data:\n\rfrom sklearn import preprocessing\rscale_data = preprocessing.scale(mt_car) scale_car = pd.DataFrame(scale_data, index = mt_car.index,\rcolumns=mt_car.columns)\rprint(scale_car.describe() )\r## m_pg n_cyl ... n_gear n_carb\r## count 3.200000e+01 3.200000e+01 ... 3.200000e+01 3.200000e+01\r## mean -4.996004e-16 2.775558e-17 ... -2.775558e-17 -2.775558e-17\r## std 1.016001e+00 1.016001e+00 ... 1.016001e+00 1.016001e+00\r## min -1.633610e+00 -1.244457e+00 ... -9.467293e-01 -1.140108e+00\r## 25% -7.865141e-01 -1.244457e+00 ... -9.467293e-01 -5.110827e-01\r## 50% -1.501383e-01 -1.066677e-01 ... 4.303315e-01 -5.110827e-01\r## 75% 4.567366e-01 1.031121e+00 ... 4.303315e-01 7.469671e-01\r## max 2.327934e+00 1.031121e+00 ... 1.807392e+00 3.263067e+00\r## ## [8 rows x 11 columns]\rpreprocessing.scale() returns ndarrays which needs to be converted back to dataframe.\n\rHandling Skewed-data\rTo understand whether the data is skewed or not we need to plot it. The overall shape and how the data is spread out can have a significant impact on the analysis and modeling\nnorm_dist = np.random.normal(size=10000) norm_dist= pd.DataFrame(norm_dist)\rnorm_dist.hist(figsize=(8,8), bins=30)\rNotice how the normally distributed data looks roughly symmetric with a bell-shaped curve. Now let’s generate some skewed data\nskew = np.random.exponential(scale=2, size= 10000) skew = pd.DataFrame(skew)\rskew.hist(figsize=(8,8),bins=50)\r\rCorrelation\rThe model that we use in predictive modeling have features and each feature is related to other features in some way or the other. Using corr() we can find how these features are related with each other\n\r\r",
    "ref": "/blog/handling-numeric-data/"
  },{
    "title": "Data Science",
    "date": "",
    "description": "A Data science process is a life cycle process for delivering a data science project that helps a data consumer to gain insights from the data.",
    "body": "\r\r\rData Science Process?\rA Data science process is a life cycle process for delivering a data science project that helps a data consumer to gain insights from the data.\nIt involves the following steps. They are as follows:\nDescribing the data problem by asking the right questions\rCollecting the data\rTransforming the raw data in to the format required applying Data munging techniques\rExploring Data by using EDA techniques\rAdvanced Exploratory data analysis.\rBuilding model and perform model assessment\rPresentation and Automation\r\r1 Describing the data problem by asking the right questions\rWhen the user presents their question, for example “What are my expected findings after the project is finished?”, or “What kind of information can I extract through the data science process?,” different results will be given. Therefore, asking the right\rquestion and setting a research goal is essential in the first place, for the question itself determines the objective and target of the data science project.\n## Examples of specific questions ?\r# - For future farming technique for a specified plant which treatment condition gives the largest plant yields on average\r# - Which web-site design is most viewed by customers ?\r# - What will the temperature be next Thurseday ?\r# - What will my fourth quarter sales in region C be ?\r# - Which viewers like the same types of movies?\r# - Which plant is in this image ?\r# - What will be the Share price next month ?\r\r2 Collecting the data\rNext step is to collect the relevent data from the data source. The collected data is known as raw data. The data can be extracted by file IO, JDBC/ODBC or using web crawler. The data is collected from the conerned stakeholders over a period of time and may be unstructured/raw format.\n\r3 Data munging or Data Preparation\rThe next step is data munging (or wrangling), a step to help map raw data into a more convenient format for further processing. During this step, there are many processes, such as data parsing, sorting, splitting, merging, filtering, missing value completion, and other processes to convert and organize the data, and enable it to fit into a consumable structure. Later, the mapped data can be further used for data aggregation, analysis, or visualization.\rThis step involves Data Cleansing, Data tranformation and combining data. For example during the cleansing process it is necessary to check for null values, during transformation it is necessary to combine and reduce number of variables, combing data includes creating data views and merging or joining data sets. Once data is clean we look for statistical properties such as distributions,correlations, and outliers.\nCommon data errors found during this phase are\n\rMistakes during data entry\rRedundant white space: Present of white spaces\rImpossible values: values which are not possible in the given context\rMissing values: features or data not labeled or having null values\rOutliers: Data points that are significantly spread out by the majority of the observations\r\rFor example to detect outliers statistical methods such as IQR and boxplot is used.\rDescribe the data as unimodal, bimodal, multimodal; symmetric, right-skewed, left-skewed to find out the outliers.\n\r4 Exploring Data by using EDA techniques\rAfter the data munging step, users can do further analysis toward data processing. The most basic analysis is to perform exploratory data analysis. Exploratory data analysis involves analyzing a datamatrix by summarizing its characteristics. Performing basic statistical, aggregation, and visual methods are also crucial tasks to help the user understand data characteristics, which are beneficial for the user to capture the majority, trends, and outliers easily through plots. Basic exploratory analysis includes creating simple graphs, combined graphs and summarising the findings based on the graphs.\nExperimental Data Analysis is a process of looking at a data set to see what are the appropriate statistical inferences that can possibly be learned. For univariate data, we can ask if the data is approximately normal, longer tailed, or shorter tailed? Does it have symmetry, or is it skewed? Is it unimodal, bimodal or multi-modal. The main tool is the proper use of computer graphics.\n\rbarplots for categorical data\n\rhistogram, dot plots, stem and leaf plots to see the shape of numerical distributions\n\rboxplots to see summaries of a numerical distribution, useful in comparing distributions and identifying long and short-tailed distributions.\n\rnormal probability plots To see if data is approximately normal\n\r\r\r5 Advanced exploratory data analysis\rUntil now, the descriptive statistic gives a general description of data features. However, one would like to generate an inference rule for the user to prepare and predict data features based on input parameters. Therefore, the application of machine learning enables the user to generate an inferential model, where the user can input a training dataset to generate a predictive model. After this, the prediction model can be utilized to predict the output value or label based on given parameters.\nExample: Applying anova to plantgrowth dataset\nrequire(stats); require(graphics)\rboxplot(weight ~ group, data = PlantGrowth, main = \u0026quot;PlantGrowth data\u0026quot;,\rylab = \u0026quot;Dried weight of plants\u0026quot;, col = \u0026quot;lightgray\u0026quot;,\rnotch = TRUE, varwidth = TRUE)\ranova(lm(weight ~ group, data = PlantGrowth))\r## Analysis of Variance Table\r## ## Response: weight\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## group 2 3.7663 1.8832 4.8461 0.01591 *\r## Residuals 27 10.4921 0.3886 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r\r6 Model building and assessment\rTo assess whether the generating model performs the best in the data estimation of a given problem, one must perform a model selection. The selection method here involves many steps, including data preprocessing, tuning parameters, and even switching the machine learning algorithm. However, one thing that is important to keep in mind is that the simplest model frequently achieves the best results in predictive or exploratory power whereas complex models often result in over fitting.\nExample: Simple prediction using confidence and prediction intervals\n## Predictions\rx \u0026lt;- rnorm(15)\ry \u0026lt;- x + rnorm(15)\rpredict(lm(y ~ x))\r## 1 2 3 4 5 6 ## 0.58968265 -1.11969110 0.49729237 0.11378869 -0.92128194 -0.60318281 ## 7 8 9 10 11 12 ## 0.31457319 0.74929559 -1.43755772 -0.97757204 1.10661265 -0.19310548 ## 13 14 15 ## -0.01952317 0.33691691 0.22535117\rnew \u0026lt;- data.frame(x = seq(-3, 3, 0.5))\rpredict(lm(y ~ x), new, se.fit = TRUE)\r## $fit\r## 1 2 3 4 5 6 7 ## -2.0812575 -1.7512478 -1.4212381 -1.0912284 -0.7612187 -0.4312090 -0.1011993 ## 8 9 10 11 12 13 ## 0.2288104 0.5588201 0.8888298 1.2188396 1.5488493 1.8788590 ## ## $se.fit\r## 1 2 3 4 5 6 7 8 ## 0.7204471 0.6165498 0.5160563 0.4214090 0.3375618 0.2745929 0.2488886 0.2712577 ## 9 10 11 12 13 ## 0.3321247 0.4148782 0.5089517 0.6091230 0.7128258 ## ## $df\r## [1] 13\r## ## $residual.scale\r## [1] 0.9638129\rpred.w.plim \u0026lt;- predict(lm(y ~ x), new, interval = \u0026quot;prediction\u0026quot;)\rpred.w.clim \u0026lt;- predict(lm(y ~ x), new, interval = \u0026quot;confidence\u0026quot;)\rmatplot(new$x, cbind(pred.w.clim, pred.w.plim[,-1]),\rlty = c(1,2,2,3,3), type = \u0026quot;l\u0026quot;, ylab = \u0026quot;predicted y\u0026quot;)\r\r7 Presentation and Automation\rFinally,The last step of the data science model is presenting your results and automating the analysis, if needed. One goal of a project is to change a process and/or make better decisions. We may still need to convince the business that our findings will indeed change the business process as expected. This is where we can shine in as influencer role. The importance of this step is more apparent in\rprojects on a strategic and tactical level. Certain projects require to perform the business process over and over again, so automating the project will save time.\n\r\r",
    "ref": "/blog/data-science-overview/"
  },{
    "title": "Machine Learning Overview",
    "date": "",
    "description": "Machine learning is the science, in which we focus on teaching machines or computers to perform certain tasks without being given specific instructions",
    "body": "\r\r\rMachine Learning ?\rMachine learning is fundamentally is just the science, in which we focus on teaching machines or computers to perform certain tasks without being given specific instructions. We want our machines to learn how to do something themselves without explaining it to them.\nIn order to do this, we oftentimes look at how the human brain works and try to design virtual brains that work in a similar manner.\nMachine learning and artificial intelligence are different. Artificial intelligence is a broad field and every system that can learn and solve problems might be considered an AI. Machine learning is one specific approach to this broad field.\nSupervised Learning\rIn machine learning we have different approaches or types. The two main approaches are supervised learning and unsupervised learning . So let’s first talk about supervised learning.\nHere, we give our model a set of inputs and also the corresponding outputs, which are the desired results. In this way, the model learns to match certain inputs to certain outputs and it adjusts its structure. It learns to make connections between what was put in and what the desired output is. It understands the correlation. When trained well enough, we can use the model to make predictions for inputs that we don’t know the results for.\nClassic supervised learning algorithms are regressions, classifications and support vector machines.\n\rUnsupervised Learning\rWith unsupervised learning on the other hand, we don’t give our model the desired results while training. Not because we don’t want to but because we don’t know them. This approach is more like a kind of pattern recognition. We give our model a set of input data and it then has to look for patterns in it. Once the model is trained, we can put in new data and our model will need to make decisions. Since the model doesn’t get any information about classes or results, it has to work with similarities and patterns in the data and categorize or cluster it by itself.\nClassic unsupervised learning algorithms are clustering, anomaly detection and some applications of neural networks.\n\rReinforcement Learning\rThen there is a third type of machine learning called reinforcement learning . Here we create some model with a random structure. Then we just observe what it does and reinforce or encourage it, when we like what it does. Otherwise, we can also give some negative feedback. The more our model does what we want it to do, the more we reinforce it and the more “rewards” it gets. This might happen in form of a number or a grade, which represents the so-called fitness of the model.\nIn this way, our model learns what is right and what is wrong. You can imagine it a little bit like natural selection and survival of the fittest. We can create 100 random models and kill the 50 models that perform worst. Then the remaining 50 reproduce and the same process repeats. These kinds of algorithms are called genetic algorithms.\rClassic reinforcement learning algorithms are genetic or evolutional algorithms.\n\rDeep Learning\rAnother term that is always confused with machine learning is deep learning . Deep learning however is just one area of machine learning, namely the one, which works with neural networks. Neural networks are a very comprehensive and complex topic.\n\rFields of Application\rActually, it would be easier to list all the areas in which machine learning doesn’t get applied rather than the fields of application. Despite that, we will take a quick look at some of the major areas, in which machine learning gets applied.\n· Research\n· Autonomous Cars\n· Spacecraft\n· Economics and Finance\n· Medical and Healthcare\n· Physics, Biology, Chemistry\n· Engineering\n· Mathematics\n· Robotics\n· Education\n· Forensics\n· Police and Military\n· Marketing\n· Search Engines\n· GPS and Pathfinding Systems\n\rMachine Learning Frameworks\rThis five of the most popular frameworks are:\n\rTorch\n\rTheano\n\rCaffe\n\rKeras\n\rTensorFlow\n\r\rTorch\rTorch originally released in 2002 by Ronan Collobert for the purpose of numeric computing. The computations of Torch involves multidimensional arrays called tensors. Tensors can be processed with regular vector or matrix operations. Torch acquired routines for building, training, and evaluating neural networks.Corporations like IBM and Facebook had great deal of interest in Torch. The other frameworks Theano, Caffe, Keras, and TensorFlow — can be interfaced through Python, which has emerged as the language of choice in the machine learning domain.\n\rTheano\rTheano was developed in 2010 by machile learning group at the University of Montreal. It was released as a library for numeric computation.\rLike NumPy, Theano also provides a wide range of Python modules for operating on multi-dimensional arrays. but theano stores operations in a data structure called a graph, which it compiles into high performance code. Theano supports symbolic differentiation, which makes it possible to find derivatives of functions automatically.\n\rCaffe\rThis framework as developed at UC Berkeley. It is a framework for developing image recognition applications. Caffe is written in C++, and like Theano, it supports GPU acceleration.\n\rKeras\rKeras is modular and simple machine learning framework. Keras acts as an interface to other machine learning frameworks.Keras’s simplicity stems from its small API and intuitive set of functions.\n\rTensorFlow\rThe Google Brain team released TensorFlow 1.0 in 2015, the current version is 1.4. It’s provided under the Apache 2.0 open source license, which means anyone is free to use it modify it and distribute modifications. Python is the primary interface in TensorFlow, but like Caffe, its core functionality is written in C++ for performance. TensorFlow applications can be executed on the Google Cloud Platform (GCP).\n\r\rMachine Learning Algorithms\rLinear Regression\rThe easiest and most basic machine learning algorithm is linear regression .It is a supervised learning algorithm. It is an approach to model the relation between a response i.e. dependent variable ‘Y’ and one or more independent or explanatory variables ‘X1,X2..’ .\n\rClassification\rIn linear regression we now predicted specific output-values for certain given input-values. Sometimes, however, we are not trying to predict outputs but to categorize or classify our elements. For this, we use classification algorithms.\n\rK-Nearest-Neighbors\rIn K-Nearest-Neighbors classifier, we assign the class of the new object, based on its nearest neighbors. The K specifies the amount of neighbors to look at. For example, we could say that we only want to look at the one neighbor who is nearest but we could also say that we want to factor in 100 neighbors.\n\rNaive-Bayes\rNaive-Bayes is a classification algorithm. It helps in finding the probability of one or more outcomes. It is used in decision making such that the outcome with higher probability is more likey to occur than outcome with lower probabilities.\n\rLogistic Regression\rAnother popular classification algorithm is called logistic regression. It looks at probabilities and determines how likely it is that a certain event happens (or a certain class is the right one), given the input data. This is done by plotting a logistic growth curve and splitting the data into two.\n\rDecision Trees\rWith decision tree classifiers, we construct a decision tree out of our training data and use it to predict the classes of new elements.\n\rRandom Forest\rThis classification is based on decision trees. What it does is creating a forest of multiple decision trees. To classify a new object, all the various trees determine a class and the most frequent result gets chosen. This makes the result more accurate and it also prevents overfitting\n\rSupport Vector Machines\rIn machine learning classification, SVM finds an optimal hyperplane that best segregates observations from different classes. A hyperplane is a plane of n -1 dimension that separates the n dimensional feature space of the observations into two spaces. For example, the hyperplane in a two-dimensional feature space is a line, and a surface in a three-dimensional feature space. The optimal hyperplane is picked so that the distance from its nearest points in each space to itself is maximized. And these nearest points are the so-called support vectors.\n\r\r\r",
    "ref": "/blog/machine-learning-overview/"
  },{
    "title": "Getting Started with Hadoop",
    "date": "",
    "description": "Big-data helps to come to conclusion based on the data points arrived from the Big-data analysis, for instance Big-data can be in the form of Petabytes or hexabytes.",
    "body": "\r\r\r“Hadoop” is a Big Data Management software of the 21st century which is powered by the distributed processing and data storage and the opportunity that such distributed processing offers in terms of cost effectiveness and efficiency.\nHowever, it is imperative to understand what it means to create a Hadoop Ecosystem. Such an effort requires an entire ecosystem of support and a platform for implementation of that has to be developed and matured over a period of time. While from developers viewpoint, the face of Hadoop may be a set of jar files deployed over a set of machines providing services for navigating and working with distributed files thus enabling communication between client and Hadoop cluster for data analysis, the thinking, vision and systems that produce this end product are implemented over several years. The seeds of the Hadoop were first planted in 2002 when Doug cutting and Mike Cafarella took steps in creating a better open-source search engine. This project was as Nutch which can search over 700 million web sites and over a trillion web pages. In essence this is the first piece vision of the Hadoop Ecosystem – Search Engine.\nThe first piece is Core Engine which has its roots in Nutch project, which was later evolved based on the Google’s File System Paper in October 2003 and MapReduce paper in December 2004. These papers later helped in automating steps which were done manually in Nutch project hence in early 2006, when Doug Cutting joined Yahoo and set up a 300-node research cluster there, the storage and processing parts of the Nutch were automated based on the Google’s File System and MapReduce framework to form Hadoop as an open-source Apache Software Foundation project and the Nutch web crawler remained its own separate project.\nMapReduce was developed by Google in 2004. It represents a programming framework for processing, generating and indexing large sets of data on the web. Google developed MapReduce as a general-purpose execution engine that handles the complexities of network communication, parallel programming and fault-tolerance for any kind of analytic application (hand-coded or analytics tool based).\nMapReduce is a framework for processing highly distributable problems across huge datasets. It is an algorithm based framework for computing distributed problems using divide and conquer approach cluster of nodes. MapReduce jobs are divided into two parts. The “Map” function divides a query into multiple parts and processes data at the node level. The “Reduce” function aggregates the results of the “Map” function to determine the “answer” to the query. It consists of Master node which maps input into smaller sub problems/distributes work to clusters, where these worker nodes process smaller problems, return answers back to master node and the master node reduces set of answers back to master node.\nIt divides the basic problem into a set of smaller manageable tasks and assigns them to a large number of computers (nodes). An ideal MapReduce task is too large for any one node to process, but can be accomplished by multiple nodes efficiently.\nMapReduce is named for the two steps at the heart of the framework.\n Map step – The master node takes the input, divides it into smaller sub-problems, and distributes them to worker nodes. Each worker node processes its smaller problem, and passes the result back to its master node. There can be multiple levels of workers.\n Reduce step – The master node collects the results from all of the sub-problems, combines the results into groups based on the key and then assigns them to worker nodes called reducers. Each reducer processes those values and sends the result back to the master node.\nMapReduce can be a huge help in analyzing and processing large chunks of data: buying pattern analysis, customer usage and interest patterns in e-commerce, processing the large amount of data generated in the fields of science and medicine, and processing and analyzing security data, credit scores and other large data-sets in the financial industry.\nAlthough in 2006-era, the Hadoop was not able to handle production search workloads at web scale and only worked on 5 to 20 nodes at that point. Horizontal scalability was an issue because Hadoop was originally required to handle petabytes and exabytes of data distributed over multiple nodes in parallel.\nIt took couple of years for Yahoo to move its web index into Hadoop. This transformation to Hadoop was completed by end of 2008. In 2008 Yahoo has created “Production search index” which was based on 10,000 core Hadoop clusters. Hadoop is used both in Production and research environments and many organisations are using Hadoop. Few examples are Facebook, AOL, Yahoo, IBM.\nIDC predicts that Hadoop software market will be worth 813 million in 2016. Hadoop has created many start-ups and spurred hundreds of millions in venture capital investment since 2008. Hadoop software is driving the big data market today and it will hit more than 23 billion by 2016.\nThe second piece of Hadoop is the creation of Big-Data Hadoop platforms that would enhance efficiency and can help in analysing large datasets and provide meaningful information to help organizations in making business decisions. Two core platforms of the Big Data Analytics involving Hadoop as core component are Cloudera and Hottonworks both providing Hadoop Ecosystem capabilities which enables analysis of more data at lower cost, Horizontal scalability, fault tolerance, improvement in programmability and data access, Co-ordination, workflow, management and deployment. Everyone selling any type of database, business intelligence software or anything else related to data at least connects to Hadoop in some capacity.\nCloudera was the first commercial Hadoop company launched in march 2008, which helped enterprises adopt Hadoop which otherwise would have taken longer time for them to adopt.\nThe third piece of Hadoop is the industry specific vertical applications that can ride on the aforesaid Core engine and platforms. This is where the opportunity lies for example in eCommerce “Analysing customer behaviour in real time”. There is huge scope for Big Data Hadoop applications in healthcare, education, eCommerce industries. The applications in BI, DW and analytics can be developed on a technology stack involving Hadoop HDFS and MapReduce (with HBase \u0026amp; Hive).\nThe fourth piece of Hadoop is the creation and administration of data centres which can help in Big Data processing and over which all the three pieces mentioned above can be deployed. Such data centres should protect the privacy of data and provide adequate security measures.\n",
    "ref": "/blog/hadoop/"
  },{
    "title": "Big Data Analytics",
    "date": "",
    "description": "Big-data helps to come to conclusion based on the data points arrived from the Big-data analysis, for instance Big-data can be in the form of Petabytes or hexabytes.",
    "body": "\r\r\rBig-data and Analytics:\rDo we know how we can reach at a decision based on mobile data or computer data or Aadhar number? How to find that the new accounts that are opened in the Pradhan Mantri’s Jan Dhan Yojana does not have enough money ?\nHow does company know that their brand is making consumers happy or not? OK, this works based on the Big-data Analytics. Lets us understand what this is, When we call it Big Data Analytics then this directly links us i.e. consumer. Many companies are selling their products online. To find the righteousness of the sells Big-data helps.\nLet us assume that someone is claiming that they have a certain number of hits of the consumer on their web-site, but those number of hits cannot be true unless the consumer have done those number of purchases because someone cannot be claimed as consumer unless he/she purchases something, now this indicates towards unstructured data. When data is structured and some conclusion is drawn from it then it will be known as Big Data Analysis.\nLets us understand it with the help of an example:-\nThere is a manufacturer of jewellery in Surat in India. He has started his website recently. Now he wants to increase the sales on the online platform only. He comes to Delhi and gets a very beautiful website. He has also made arrangements that his website shows up on the top of the google search results, now this kind or arrangement is known as search engine optimization. He is very excited from this. He was very excited that all over the world his website was visited by around 5000 people daily but within a month this excitement went down because out of those only 20 percent used to purchase. He was not able to understand the fact and was trying to increase the sales. Then one of his friends told him about Big data analytics firm in Bangalore. He was not able to understand that what that company will do. His friend told him that Big Data analysis will help him know the trends of the web site visited by the consumers. This will help him get the future strategy of the online jewelry business.\nThe Meaning of Big Data\rBig-data helps to come to conclusion based on the data points arrived from the Big-data analysis, for instance Big-data can be in the form of Petabytes or hexabytes. This may contain the data or information about lakhs, crores or millions of people. This data may come from sources such as customers connect center, social media, mobile data etc. These kinds of numbers are known as structured data but these are not complete. To get to the roots of these is difficult.\nBig-data \u0026amp; Analytics helps companies to arrive at a decision. This will help not only to get new information about the business but this helps in getting the big insights or trends which cannot be identified based on just numbers.\n\rHow it started\rIn September 1956 IBM has presented Random access memory accounting machine. This was the first disk storage product of the world. This has two kinds of machines. One was 305 with a capacity of 5 Megabyte and weight of 1 ton. Second was 650. Actually the use of these was done by telecommunications company to find whether the customers are happy or not. It can be identified with the help of an analysis.\nIn a report by Nasscom in partnership with Blue ocean market intelligence the market of big data is 1 Arab dollar i.e. around 62 Arab rupees.this will be more than double in 2017.\nBig data helps to get different kinds of ideas. With the help of this we can bring the change in management. Regardless of the organization Big-data analysis helps to get insights. With the help of these insights company can form the future strategy.\nFrom the point of view of the consumer it is important because it can increase the number of consumers.\n\rUse\rBig Data helps in decision making. Along with that it helps in getting the direction in which to put the efforts. In financial sector manufacturer of consumer products and telecommunications companies can identify “what the customer wants”. Companies working in the banks and financial sector can find which kinds of loan are being taken more, how the loan is getting repaid. How much debt is getting lost and what are the reasons of that.\nIn the era of supply chain and new brands the interest of the consumer is getting identified with the help of Big-data. Crores of people give their opinion on social media on something and when it is about some brand then with the help of Big-data it is considered to find further insights. Similarly Big Data helps in getting an opinion based on the text, audio and video. What is making consumers happy can be identified with the help of big data analytics.\nPeople use Big-data driven analytics to drive value in diverse fields such as\n\rDriving revenues in e-commerce\rDriving recommendations to consumers in various web applications such as social media, online retail, ads, etc.\rHealthcare applications including diagnosis of diseases\rSocial sciences – predicting election outcomes and econometric models\r\rIn India the market for Big-data analytics is increasing at the rate of 45% and there are opportunities in every sector.\n\r\r",
    "ref": "/blog/big-data-analytics/"
  },{
    "title": "Pandas (Python)",
    "date": "",
    "description": "Pandas is a software module for the Python programming language for the purpose of data manipulation and analysis. It provides data structures and operations to  manipulate numerical tables and time series data. It is build on top of numpy. It is applied for fast analysis and data cleaning and preparation.In this post we will learn various wasy to work with Pandas DataFrames.",
    "body": "\r\r\rPandas\rPandas is probably the most powerful library. It provides high-performance tools for data manipulation and analysis. Furthermore, it is very effective at converting data formats and querying data out of databases. The two main data structures of Pandas are the series and the data frame. To work with Pandas, we need to import the module.\nimport pandas as pd\rPandas Series\rA series in Pandas is a one-dimensional array which is labeled. You can imagine it to be the equivalent of an ordinary Python dictionary.\nseries = pd.Series([ 10 , 20 , 30 , 40 ],\r[ \u0026#39;A\u0026#39; , \u0026#39;B\u0026#39; , \u0026#39;C\u0026#39; , \u0026#39;D\u0026#39; ])\rIn order to create a series, we use the constructor of the Series class. The first parameter that we pass is a list full of values (in this case numbers). The second parameter is the list of the indices or keys (in this case strings). When we now print our series, we can see what the structure looks like.\nprint(series)\r## A 10\r## B 20\r## C 30\r## D 40\r## dtype: int64\rThe first column represents the indices, whereas the second column represents the actual values.\nACCESSING VALUES\rThe accessing of values works in the same way that it works with dictionaries. We need to address the respective index or key to get our desired value.\nprint (series[ \u0026#39;C\u0026#39; ])\r## 30\rprint (series[ 1 ])\r## 20\rAs you can see, we can choose how we want to access our elements. We can either address the key or the position that the respective element is at.\nCONVERTING DICTIONARIES\nSince series and dictionaries are quite similar, we can easily convert our Python dictionaries into Pandas series.\nmyDict = { \u0026#39;A\u0026#39; : 10 , \u0026#39;B\u0026#39; : 20 , \u0026#39;C\u0026#39; : 30 }\rseries = pd.Series(myDict)\rNow the keys are our indices and the values remain values. But what we can also do is, to change the order of the indices.\nmyDict = { \u0026#39;A\u0026#39; : 10 , \u0026#39;B\u0026#39; : 20 , \u0026#39;C\u0026#39; : 30 }\rseries = pd.Series(myDict, index =[ \u0026#39;C\u0026#39; , \u0026#39;A\u0026#39; , \u0026#39;B\u0026#39; ])\rOur series now looks like this:\nprint(series)\r## C 30\r## A 10\r## B 20\r## dtype: int64\r\r\rPANDAS DataFrame\rDataFrame is the main thing on which we’ll be mostly working on. Most manipulation or operation on the data will be applied by means of DataFrame.\nCreating DataFrame using dictionary data\rThis is a simple process in which we just need to pass the json data to the DataFrame method.\ncars = {\u0026#39;Brand\u0026#39;:[\u0026#39;Honda\u0026#39;,\u0026#39;Toyota\u0026#39;,\u0026#39;Ford\u0026#39;,\u0026#39;Audi\u0026#39;],\u0026#39;Price\u0026#39;:[22000,21000,27000,35000]}\rdf = pd.DataFrame(cars)\rdf\r## Brand Price\r## 0 Honda 22000\r## 1 Toyota 21000\r## 2 Ford 27000\r## 3 Audi 35000\rdata = { \u0026#39;Name\u0026#39; : [ \u0026#39;Anna\u0026#39; , \u0026#39;Bob\u0026#39; , \u0026#39;Charles\u0026#39; ], \u0026#39;Age\u0026#39; : [ 24 , 32 , 35 ], \u0026#39;Height\u0026#39; : [ 176 , 187 , 175 ]}\rdf = pd.DataFrame(data)\rTo create a Pandas data frame, we use the constructor of the class. In this case, we first create a dictionary with some data about three persons. We feed that data into our data frame. It then looks like this:\ndf\r## Name Age Height\r## 0 Anna 24 176\r## 1 Bob 32 187\r## 2 Charles 35 175\rAs you can see, without any manual work, we already have a structured data frame and table.\nTo now access the values is a bit more complicated than with series. We have multiple columns and multiple rows, so we need to address two values.\nprint (df[ \u0026#39;Name\u0026#39; ][ 1 ])\r## Bob\rSo first we choose the column Name and then we choose the second element (index one) of this column. In this case, this is Bob .\nWhen we omit the last index, we can also select only the one column. This is useful when we want to save specific columns of our data frame into a new one. What we can also do in this case is to select multiple columns.\nprint (df[[ \u0026#39;Name\u0026#39; , \u0026#39;Height\u0026#39; ]])\r## Name Height\r## 0 Anna 176\r## 1 Bob 187\r## 2 Charles 175\r\rDATA FRAME FUNCTIONS\rFor data frames we have a couple of basic functions and attributes that we already know from lists or NumPy arrays.\n\r\rBASIC FUNCTIONS AND ATTRIBUTES\r\r\r\r\rdf.T\rTransposes the rows and columns of the data frame\r\rdf.dtypes\rReturns data types of the data frame\r\rdf.ndim\rReturns the number of dimensions of the data frame\r\rdf.shape\rReturns the shape of the data frame\r\rdf.size\rReturns the number of elements in the data frame\r\rdf.head(n)\rReturns the first n rows of the data frame (default is five)\r\rdf.tail(n)\rReturns the last n rows of the data frame (default is five)\r\r\r\r\rSTATISTICAL FUNCTIONS\rFor the statistical functions, we will now extend our data frame a little bit and add some more persons.\ndata = { \u0026#39;Name\u0026#39; : [ \u0026#39;Anna\u0026#39; , \u0026#39;Bob\u0026#39; , \u0026#39;Charles\u0026#39; ,\r\u0026#39;Daniel\u0026#39; , \u0026#39;Evan\u0026#39; , \u0026#39;Fiona\u0026#39; ,\r\u0026#39;Gerald\u0026#39; , \u0026#39;Henry\u0026#39; , \u0026#39;India\u0026#39; ],\r\u0026#39;Age\u0026#39; : [ 24 , 32 , 35 , 45 , 22 , 54 , 55 , 43 , 25 ],\r\u0026#39;Height\u0026#39; : [ 176 , 187 , 175 , 182 , 176 ,\r189 , 165 , 187 , 167 ]}\rdf = pd.DataFrame(data)\r\r\rSTATISTICAL FUNCTIONS\r\r\r\r\rFUNCTION\rDESCRIPTION\r\rcount()\rCount the number of non-null elements\r\rsum()\rReturns the sum of values of the selected columns\r\rmean()\rReturns the arithmetic mean of values of the selected columns\r\rmedian()\rReturns the median of values of the selected columns\r\rmode()\rReturns the value that occurs most often in the columns selected\r\rstd()\rReturns standard deviation of the values\r\rmin()\rReturns the minimum value\r\rmax()\rReturns the maximum value\r\rabs()\rReturns the absolute values of the elements\r\rprod()\rReturns the product of the selected elements\r\rdescribe()\rReturns data frame with all statistical values summarized\r\r\r\rNow, we are not going to dig deep into every single function here. But let’s take a look at how to apply some of them.\nprint (df[ \u0026#39;Age\u0026#39; ].mean())\r## 37.22222222222222\rprint (df[ \u0026#39;Height\u0026#39; ].median())\r## 176.0\rHere we choose a column and then apply the statistical functions on it. What we get is just a single scalar with the desired value.\nWe can also apply the functions to the whole data frame. In this case, we get returned another data frame with the results for each column.\nprint (df.mean())\r## Age 37.222222\r## Height 178.222222\r## dtype: float64\r\rAPPLYING NUMPY FUNCTIONS\rInstead of using the built-in Pandas functions, we can also use the methods we already know. For this, we just use the apply function of the data frame and then pass our desired method.\nimport numpy as np\rprint (df[ \u0026#39;Age\u0026#39; ].apply(np.sin))\r## 0 -0.905578\r## 1 0.551427\r## 2 -0.428183\r## 3 0.850904\r## 4 -0.008851\r## 5 -0.558789\r## 6 -0.999755\r## 7 -0.831775\r## 8 -0.132352\r## Name: Age, dtype: float64\rIn this example, we apply the sine function onto our ages. It doesn’t make any sense but it demonstrates how this works.\n\rLAMBDA EXPRESSIONS\rA very powerful in Python are lambda expression . They can be thought of as nameless functions that we pass as a parameter.\nprint (df[ \u0026#39;Age\u0026#39; ].apply( lambda x: x * 100 ))\r## 0 2400\r## 1 3200\r## 2 3500\r## 3 4500\r## 4 2200\r## 5 5400\r## 6 5500\r## 7 4300\r## 8 2500\r## Name: Age, dtype: int64\rBy using the keyword lambda we create a temporary variable that represents the individual values that we are applying the operation onto. After the colon, we define what we want to do. In this case, we multiply all values of the column Age by 100.\ndf = df[[ \u0026#39;Age\u0026#39; , \u0026#39;Height\u0026#39; ]]\rprint (df.apply( lambda x: x.max() - x.min()))\r## Age 33\r## Height 24\r## dtype: int64\rHere we removed the Name column, so that we only have numerical values. Since we are applying our expression on the whole data frame now, x refers to the whole columns. What we do here is calculating the difference between the maximum value and the minimum value.\n\rITERATING\rIterating over data frames is quite easy with Pandas. We can either do it in the classic way or use specific functions for it.\nfor x in df[ \u0026#39;Age\u0026#39; ]:\rprint (x)\r## 24\r## 32\r## 35\r## 45\r## 22\r## 54\r## 55\r## 43\r## 25\rAs you can see, iterating over a column’s value is very simple and nothing new. This would print all the ages. When we iterate over the whole data frame, our control variable takes on the column names.\n\r\rSTATISTICAL FUNCTIONS\r\r\r\r\riteritems()\rIterator for key-value pairs\r\riterrows()\rIterator for the rows (index, series)\r\ritertuples()\rIterator for the rows as named tuples\r\r\r\rLet’s take a look at some practical examples.\nfor key, value in df.iteritems():\rprint ( \u0026#39;{}: {}\u0026#39; .format(key, value))\r## Age: 0 24\r## 1 32\r## 2 35\r## 3 45\r## 4 22\r## 5 54\r## 6 55\r## 7 43\r## 8 25\r## Name: Age, dtype: int64\r## Height: 0 176\r## 1 187\r## 2 175\r## 3 182\r## 4 176\r## 5 189\r## 6 165\r## 7 187\r## 8 167\r## Name: Height, dtype: int64\rHere we use the iteritems function to iterate over key-value pairs. What we get is a huge output of all rows for each column.\nOn the other hand, when we use iterrows , we can print out all the column-values for each row or index.\nfor index, value in df.iterrows():\rprint (index,value)\r## 0 Age 24\r## Height 176\r## Name: 0, dtype: int64\r## 1 Age 32\r## Height 187\r## Name: 1, dtype: int64\r## 2 Age 35\r## Height 175\r## Name: 2, dtype: int64\r## 3 Age 45\r## Height 182\r## Name: 3, dtype: int64\r## 4 Age 22\r## Height 176\r## Name: 4, dtype: int64\r## 5 Age 54\r## Height 189\r## Name: 5, dtype: int64\r## 6 Age 55\r## Height 165\r## Name: 6, dtype: int64\r## 7 Age 43\r## Height 187\r## Name: 7, dtype: int64\r## 8 Age 25\r## Height 167\r## Name: 8, dtype: int64\r\rSORTING\rOne very powerful thing about Pandas data frames is that we can easily sort them.\n\rSORT BY INDEX\rdf = pd.DataFrame(np.random.rand( 10 , 2 ),\rindex =[ 1 , 5 , 3 , 6 , 7 , 2 , 8 , 9 , 0 , 4 ],\rcolumns =[ \u0026#39;A\u0026#39; , \u0026#39;B\u0026#39; ])\rHere we create a new data frame, which is filled with random numbers. We specify our own indices and as you can see, they are completely unordered.\nprint (df.sort_index())\r## A B\r## 0 0.700317 0.561898\r## 1 0.155794 0.118885\r## 2 0.125663 0.841498\r## 3 0.484446 0.735259\r## 4 0.337182 0.801139\r## 5 0.132799 0.756596\r## 6 0.690977 0.900216\r## 7 0.774688 0.597023\r## 8 0.345127 0.797047\r## 9 0.075948 0.634691\rBy using the method sort_index , we sort the whole data frame by the index column. The result is now sorted:\n\rINPLACE PARAMETER\rWhen we use functions that manipulate our data frame, we don’t actually change it but we return a manipulated copy. If we wanted to apply the changes on the actual data frame, we would need to do it like this:\ndf = df.sort_index()\rBut Pandas offers us another alternative as well. This alternative is the parameter inplace . When this parameter is set to True , the changes get applied to our actual data frame\ndf.sort_index( inplace = True )\r\rSORT BY COLUMNS\rNow, we can also sort our data frame by specific columns.\ndata = { \u0026#39;Name\u0026#39; : [ \u0026#39;Anna\u0026#39; , \u0026#39;Bob\u0026#39; , \u0026#39;Charles\u0026#39; ,\r\u0026#39;Daniel\u0026#39; , \u0026#39;Evan\u0026#39; , \u0026#39;Fiona\u0026#39; ,\r\u0026#39;Gerald\u0026#39; , \u0026#39;Henry\u0026#39; , \u0026#39;India\u0026#39; ],\r\u0026#39;Age\u0026#39; : [ 24 , 24 , 35 , 45 , 22 , 54 , 54 , 43 , 25 ],\r\u0026#39;Height\u0026#39; : [ 176 , 187 , 175 , 182 , 176 ,\r189 , 165 , 187 , 167 ]}\rdf = pd.DataFrame(data)\rdf.sort_values( by =[ \u0026#39;Age\u0026#39; , \u0026#39;Height\u0026#39; ], inplace = True )\rprint (df)\r## Name Age Height\r## 4 Evan 22 176\r## 0 Anna 24 176\r## 1 Bob 24 187\r## 8 India 25 167\r## 2 Charles 35 175\r## 7 Henry 43 187\r## 3 Daniel 45 182\r## 6 Gerald 54 165\r## 5 Fiona 54 189\rHere we have our old data frame slightly modified. We use the function sort_values to sort our data frames. The parameter by states the columns that we are sorting by. In this case, we are first sorting by age and if two persons have the same age, we sort by height.\n\rJOINING AND MERGING\rAnother powerful concept in Pandas is joining and merging data frames.\nnames = pd.DataFrame({\r\u0026#39;id\u0026#39; : [ 1 , 2 , 3 , 4 , 5 ],\r\u0026#39;name\u0026#39; : [ \u0026#39;Anna\u0026#39; , \u0026#39;Bob\u0026#39; , \u0026#39;Charles\u0026#39; ,\r\u0026#39;Daniel\u0026#39; , \u0026#39;Evan\u0026#39; ],\r})\rages = pd.DataFrame({\r\u0026#39;id\u0026#39; : [ 1 , 2 , 3 , 4 , 5 ],\r\u0026#39;age\u0026#39; : [ 20 , 30 , 40 , 50 , 60 ]\r})\rdf = pd.merge(names,ages, on = \u0026#39;id\u0026#39; )\rdf.set_index( \u0026#39;id\u0026#39; , inplace = True )\rFirst we use the method merge and specify the column to merge on. We then have a new data frame with the combined data but we also want our id column to be the index. For this, we use the set_index method.\rNow when we have two separate data frames which are related to one another, we can combine them into one data frame. It is important that we have a common column that we can merge on. In this case, this is id .\n\rJOINS\rIt is not necessarily always obvious how we want to merge our data frames. This is where joins come into play. We have four types of joins.\n\r\rJOIN MERGE TYPES\r\r\r\r\rleft\rUses all keys from left object and merges with right\r\rright\rUses all keys from right object and merges with left\r\router\rUses all keys from both objects and merges them\r\rinner\rUses only the keys which both objects have and merges them\r\r\r(default)\r\r\r\rNow let’s change our two data frames a little bit.\nnames = pd.DataFrame({\r\u0026#39;id\u0026#39; : [ 1 , 2 , 3 , 4 , 5 , 6 ],\r\u0026#39;name\u0026#39; : [ \u0026#39;Anna\u0026#39; , \u0026#39;Bob\u0026#39; , \u0026#39;Charles\u0026#39; ,\r\u0026#39;Daniel\u0026#39; , \u0026#39;Evan\u0026#39; , \u0026#39;Fiona\u0026#39; ],\r})\rages = pd.DataFrame({\r\u0026#39;id\u0026#39; : [ 1 , 2 , 3 , 4 , 5 , 7 ],\r\u0026#39;age\u0026#39; : [ 20 , 30 , 40 , 50 , 60 , 70 ],\r\u0026#39;Height\u0026#39; : [ 176 , 187 , 175 , 182 , 176 ,\r189 ]\r})\rOur names frame now has an additional index 6 and an additional name. And our ages frame has an additional index 7 with an additional name.\ndf = pd.merge(names,ages, on = \u0026#39;id\u0026#39; , how = \u0026#39;inner\u0026#39; )\rdf.set_index( \u0026#39;id\u0026#39; , inplace = True )\rIf we now perform the default inner join , we will end up with the same data frame as in the beginning. We only take the keys which both objects have. This means one to five.\ndf = pd.merge(names,ages, on = \u0026#39;id\u0026#39; , how = \u0026#39;left\u0026#39; )\rdf.set_index( \u0026#39;id\u0026#39; , inplace = True )\rWhen we use the left join , we get all the keys from the names data frame but not the additional index 7 from ages. This also means that Fiona won’t be assigned any age.\nThe same principle goes for the right join just the other way around\ndf = pd.merge(names,ages, on = \u0026#39;id\u0026#39; , how = \u0026#39;right\u0026#39; )\rdf.set_index( \u0026#39;id\u0026#39; , inplace = True )\rNow, we only have the keys from the ages frame and the 6 is missing. Finally, if we use the outer join , we combine all keys into one data frame.\ndf = pd.merge(names,ages, on = \u0026#39;id\u0026#39; , how = \u0026#39;outer\u0026#39; )\rdf.set_index( \u0026#39;id\u0026#39; , inplace = True )\r\rQUERYING DATA\rLike in databases with SQL, we can also query data from our data frames in Pandas. For this, we use the function loc , in which we put our expression.\nprint (df.loc[df[ \u0026#39;age\u0026#39; ] == 24 ])\r## Empty DataFrame\r## Columns: [name, age, Height]\r## Index: []\rprint (df.loc[(df[ \u0026#39;age\u0026#39; ] == 24 ) \u0026amp;\r(df[ \u0026#39;Height\u0026#39; ] \u0026gt; 180 )])\r## Empty DataFrame\r## Columns: [name, age, Height]\r## Index: []\rprint (df.loc[df[ \u0026#39;age\u0026#39; ] \u0026gt; 30 ][ \u0026#39;name\u0026#39; ])\r## id\r## 3 Charles\r## 4 Daniel\r## 5 Evan\r## 7 NaN\r## Name: name, dtype: object\rHere we have some good examples to explain how this works. The first expression returns all rows where the value for Age is 24.\nThe second query is a bit more complicated. Here we combine two conditions. The first one is that the age needs to be 24 but we then combine this with the condition that the height is greater than 180. This leaves us with one row.\nIn the last expression, we can see that we are only choosing one column to be returned. We want the names of all people that are older than 30.\n\rREAD DATA FROM FILES\rSimilar to NumPy, we can also easily read data from external files into Pandas. Let’s say we have an CSV-File like this (opened in Excel):\nThe only thing that we need to do now is to use the function read_csv to import our data into a data frame.\ndf = pd.read_csv( \u0026#39;data.csv\u0026#39; )\rdf.set_index( \u0026#39;id\u0026#39; , inplace = True )\rprint (df)\rWe also set the index to the id column again. This is what we have imported:\nThis of course, also works the other way around. By using the method to_csv , we can also save our data frame into a CSV-file.\ndata = { \u0026#39;Name\u0026#39; : [ \u0026#39;Anna\u0026#39; , \u0026#39;Bob\u0026#39; , \u0026#39;Charles\u0026#39; ,\r\u0026#39;Daniel\u0026#39; , \u0026#39;Evan\u0026#39; , \u0026#39;Fiona\u0026#39; ,\r\u0026#39;Gerald\u0026#39; , \u0026#39;Henry\u0026#39; , \u0026#39;India\u0026#39; ],\r\u0026#39;Age\u0026#39; : [ 24 , 24 , 35 , 45 , 22 , 54 , 54 , 43 , 25 ],\r\u0026#39;Height\u0026#39; : [ 176 , 187 , 175 , 182 , 176 ,\r189 , 165 , 187 , 167 ]}\rdf = pd.DataFrame(data)\rdf.to_csv( \u0026#39;mydf.csv\u0026#39; )\r\rPLOTTING DATA\rSince Pandas builds on Matplotlib, we can easily visualize the data from our data frame.\ndata = { \u0026#39;Name\u0026#39; : [ \u0026#39;Anna\u0026#39; , \u0026#39;Bob\u0026#39; , \u0026#39;Charles\u0026#39; ,\r\u0026#39;Daniel\u0026#39; , \u0026#39;Evan\u0026#39; , \u0026#39;Fiona\u0026#39; ,\r\u0026#39;Gerald\u0026#39; , \u0026#39;Henry\u0026#39; , \u0026#39;India\u0026#39; ],\r\u0026#39;Age\u0026#39; : [ 24 , 24 , 35 , 45 , 22 , 54 , 54 , 43 , 25 ],\r\u0026#39;Height\u0026#39; : [ 176 , 187 , 175 , 182 , 176 ,\r189 , 165 , 187 , 167 ]}\rdf = pd.DataFrame(data)\rdf.sort_values( by =[ \u0026#39;Age\u0026#39; , \u0026#39;Height\u0026#39; ])\rdf.hist()\rplt.show()\rIn this example, we use the method hist to plot a histogram of our numerical columns. Without specifying anything more, this is what we end up with:\nBut we can also just use the function plot to plot our data frame or individual columns.\ndf.plot()\rplt.show()\rThe result is the following:\n\r\r\r",
    "ref": "/blog/pandas/"
  },{
    "title": "Python - Matplotlib- Plot Types",
    "date": "",
    "description": "Matplotlib- Plot Types",
    "body": "\r\r\rMATPLOTLIB PLOT TYPES\rMatplotlib offers a huge arsenal of different plot types. Here we are going to take a look at these.\nHISTOGRAMS\rLet’s start out with some statistics here. So-called histograms represent the distribution of numerical values. For example, we could graph the distribution of heights amongst students in a class.\nmu, sigma = 172 , 4\rx = mu + sigma * np.random.randn( 10000 ) \rWe start by defining a mean value mu (average height) and a standard deviation sigma . To create our x-values, we use our mu and sigma combined with 10000 randomly generated values. Notice that we are using the randn function here. This function generates values for a standard normal distribution , which means that we will get a bell curve of values.\nplt.hist(x, 100 , density = True , facecolor = \u0026#39;blue\u0026#39; ) \rThen we use the hist function, in order to plot our histogram. The second parameter states how many values we want to plot. Also, we want our values to be normed. So we set the parameter density to True . This means that our y-values will sum up to one and we can view them as percentages. Last but not least, we set the color to blue.\nNow, when we show this plot, we will realize that it is a bit confusing. So we are going to add some labeling here.\nplt.xlabel( \u0026#39;Height\u0026#39; )\rplt.ylabel( \u0026#39;Probability\u0026#39; )\rplt.title( \u0026#39;Height of Students\u0026#39; )\rplt.text( 160 , 0.125 , \u0026#39;µ = 172, σ = 4\u0026#39; )\rplt.axis([ 155 , 190 , 0 , 0.15 ])\rplt.grid( True )\rFirst we label the two axes. The x-values represent the height of the students, whereas the y-values represent the probability that a randomly picked student has the respective height. Besides the title, we also add some text to our graph. We place it at the x-value 160 and the y-value of 0.125. The text just states the values for µ (mu) and σ (sigma).\nLast but not least, we set the ranges for the two axes. Our x-values range from 155 to 190 and our y-values from 0 to 0.15. Also, the grid is turned on. This is what our graph looks like at the end:\nWe can see the Gaussian bell curve which is typical for the standard normal distribution.\n\rBAR CHART\rFor visualizing certain statistics, bar charts are oftentimes very useful, especially when it comes to categories. In our case, we are going to plot the skill levels of three different people in the IT realm.\nbob = ( 90 , 67 , 87 , 76 )\rcharles = ( 80 , 80 , 47 , 66 )\rdaniel = ( 40 , 95 , 76 , 89 )\rskills = ( \u0026#39;Python\u0026#39; , \u0026#39;Java\u0026#39; , \u0026#39;Networking\u0026#39; , \u0026#39;Machine Learning\u0026#39; )\rHere we have the three persons Bob, Charles and Daniel . They are represented by tuples with four values that indicate their skill levels in Python programming, Java programming, networking and machine learning.\nwidth = 0.2\rindex = np.arange( 4 )\rplt.bar(index, bob,\rwidth =width, label = \u0026#39;Bob\u0026#39; )\rplt.bar(index + width, charles,\rwidth =width, label = \u0026#39;Charles\u0026#39; )\rplt.bar(index + width * 2 , daniel,width =width, label = \u0026#39;Daniel\u0026#39; )\rWe then use the bar function to plot our bar chart. For this, we define an array with the indices one to four and a bar width of 0.2. For each person we plot the four respective values and label them.\nplt.xticks(index + width, skills)\rplt.ylim( 0 , 120 )\rplt.title( \u0026#39;IT Skill Levels\u0026#39; )\rplt.ylabel( \u0026#39;Skill Level\u0026#39; )\rplt.xlabel( \u0026#39;IT Skill\u0026#39; )\rplt.legend()\rThen we label the x-ticks with the method xticks and set the limit of the y-axis to 120 to free up some space for our legend. After that we set a title and label the axes. The result looks like this:\nWe can now see who is the most skilled in each category. Of course we could also change the graph so that we have the persons on the x-axis with the skill-colors in the legend.\n\rPIE CHART\rPie charts are used to display proportions of numbers. For example, we could graph how many percent of the students have which nationality.\nlabels = ( \u0026#39;American\u0026#39; , \u0026#39;German\u0026#39; , \u0026#39;French\u0026#39; , \u0026#39;Other\u0026#39; )\rvalues = ( 47 , 23 , 20 , 10 ) \rWe have one tuple with our four nationalities. They will be our labels. And we also have one tuple with the percentages.\nplt.pie(values, labels =labels,\rautopct = \u0026#39;%.2f%%\u0026#39; , shadow = True )\rplt.title( \u0026#39;Student Nationalities\u0026#39; )\rplt.show()\rNow we just need to use the pie function, to draw our chart. We pass our values and our labels. Then we set the autopct parameter to our desired percentage format. Also, we turn on the shadow of the chart and set a title. And this is what we end up with:\nAs you can see, this chart is perfect for visualizing percentages.\n\rSCATTER PLOTS\rSo-called scatter plots are used to represent two-dimensional data using dots.\nx = np.random.rand( 50 )\ry = np.random.rand( 50 )\rplt.scatter(x,y)\rplt.show()\rHere we just generate 50 random x-values and 50 random y-values. By using the scatter function, we can then plot them.\n\rBOXPLOT\rBoxplot diagrams are used, in order to split data into quartiles . We do that to get information about the distribution of our values. The question we want to answer is: How widely spread is the data in each of the quartiles.\nmu, sigma = 172 , 4\rvalues = np.random.normal(mu,sigma, 200 )\rplt.boxplot(values)\rplt.title( \u0026#39;Student\u0026#39;s Height\u0026#39; )\rplt.ylabel( \u0026#39;Height\u0026#39; )\rplt.show()\rIn this example, we again create a normal distribution of the heights of our students. Our mean value is 172, our standard deviation 4 and we generate 200 values. Then we plot our boxplot diagram.\nHere we see the result. Notice that a boxplot doesn’t give information about the frequency of the individual values. It only gives information about the spread of the values in the individual quartiles. Every quartile has 25% of the values but some have a very small spread whereas others have quite a large one.\n\r3D PLOTS\rNow last but not least, let’s take a look at 3D-plotting. For this, we will need to import another plotting module. It is called mpl_toolkits and it is part of the Matplotlib stack.\nfrom mpl_toolkits import mplot3d\rSpecifically, we import the module mplot3d from this library. Then, we can use 3d as a parameter when defining our axes.\nax = plt.axes( projection = \u0026#39;3d\u0026#39; )\rplt.show()\rWe can only use this parameter, when mplot3d is imported. Now, our plot looks like this:\nSince we are now plotting in three dimensions, we will also need to define three axes.\nz = np.linspace( 0 , 20 , 100 )\rx = np.sin(z)\ry = np.cos(z)\rax = plt.axes( projection = \u0026#39;3d\u0026#39; )\rax.plot3D(x,y,z)\rplt.show()\rIn this case, we are taking the z-axis as the input. The z-axis is the one which goes upwards. We define the x-axis and the y-axis to be a sine and cosine function. Then, we use the function plot3D to plot our function. We end up with this:\n\rSURFACE PLOTS\rNow in order to plot a function with a surface, we need to calculate every point on it. This is impossible, which is why we are just going to calculate enough to estimate the graph. In this case, x and y will be the input and the z-function will be the 3D-result which is composed of them.\nax = plt.axes( projection = \u0026#39;3d\u0026#39; )\rdef z_function(x, y):\rreturn np.sin(np.sqrt(x ** 2 + y ** 2 ))\rx = np.linspace(- 5 , 5 , 50 )\ry = np.linspace(- 5 , 5 , 50 )\rWe start by defining a z_function which is a combination of sine, square root and squaring the input. Our inputs are just 50 numbers from -5 to 5.\nX, Y = np.meshgrid(x,y)\rZ = z_function(X,Y)\rax.plot_surface(X,Y,Z)\rplt.show()\rThen we define new variables for x and y (we are using capitals this time). What we do is converting the x- and y-vectors into matrices using the meshgrid function. Finally, we use the z_function to calculate our z-values and then we plot our surface by using the method plot_surface\n\r\r",
    "ref": "/blog/numpy/"
  },{
    "title": "Python - Matplotlib",
    "date": "",
    "description": "Introduction to Matplotlib",
    "body": "\r\r\r`\r# Matplotlib\nVisualizing our data is crucial for data science. It gives us an overview and helps us to analyze data and make conclusions. Matplotlib is the library which we use for plotting and visualizing.\nPLOTTING MATHEMATICAL FUNCTIONS\rNow first we will be drawing some mathematical functions.\nWe need importing the matplotlib.pyplot module and also NumPy.\nimport numpy as np\rimport matplotlib.pyplot as plt\rWe are also using alias for pyplot. In this case, it is plt .\rIn order to plot a function, we need the x-values or the input and the y-values or the output.\rSo first let us generate the x-values.\nx_values = np.linspace( 0 , 18 , 100 )\rWe wll be doing this by using the already known linspace function. Here we create an array with 100 values between 0 and 18. To now get our y-values, we just need to apply the respective function on our\nx-values. For this example, we are going with the sine function.\ny_values = np.sin(x_values)\rRemember that the function gets applied to every single item of the input array. So in this case, we have an array with the sine value of every element of the x-values array. We just need to plot them now.\nplt.plot(x_values, y_values)\rplt.show()\rWe do this by using the function plot and passing our x-values and y-values. At the end we call the show function, to display our plot.\nThat was very simple. Now, we can go ahead and define our own function that we want to plot.\nx = np.linspace( 0 , 10 , 100 )\ry = ( 6 * x - 30 ) ** 2\rplt.plot(x, y)\rplt.show()\rThe result looks like this:\nThis function (6x – 30)² is plotted with Matplotlib.\n\rVISUALIZING VALUES\rWhat we can also do, instead of plotting functions, is just visualizing values in form of single dots for example.\nnumbers = 10 * np.random.random( 100 )\rplt.plot(numbers, \u0026#39;bo\u0026#39; )\rplt.show()\rHere we generate 100 random numbers from 0 to 10. We then plot these numbers as blue dots. This is defined by the second parameter ‘bo’ , where the first letter indicates the color (blue) and the second one the shape (dots).\n\rMULTIPLE GRAPHS\rWe can plot multiple functions in different color and shape.\nx = np.linspace( 0 , 5 , 200 )\ry1 = 2 * x\ry2 = x ** 2\ry3 = np.log(x)\rplt.plot(x, y1)\rplt.plot(x, y2)\rplt.plot(x, y3)\rplt.show()\rIn this example, we first generate 200 x-values from 0 to 5. Then we define three different functions y1, y2 and y3 . We plot all these and view the plotting window. This is what it looks like:\n\rSUBPLOTS\rNow, sometimes we want to draw multiple graphs but we don’t want them in the same plot necessarily. For this reason, we have so-called subplots . These are plots that are shown in the same window but independently from each other.\nx = np.linspace( 0 , 5 , 200 )\ry1 = np.sin(x)\ry2 = np.sqrt(x)\rplt.subplot( 211 )\rplt.plot(x, y1, \u0026#39;r-\u0026#39; )\rplt.subplot( 212 )\rplt.plot(x, y2, \u0026#39;g--\u0026#39; )\rplt.show()\rBy using the function subplot we state that everything we plot now belongs to this specific subplot. The parameter we pass defines the grid of our window. The first digit indicates the number of rows, the second the number of columns and the last one the index of the subplot. So in this case, we have two rows and one column. Index one means that the respective subplot will be at the top.\nAs you can see, we have two subplots in one window and both have a different color and shape. Notice that the ratios between the x-axis and the y-axis differ in the two plots.\n\rMULTIPLE PLOTTING WINDOWS\rInstead of plotting into subplots, we can also go ahead and plot our graphs into multiple windows. In Matplotlib we call these figures .\nplt.figure( 1 )\rplt.plot(x, y1, \u0026#39;r-\u0026#39; )\rplt.figure( 2 )\rplt.plot(x, y2, \u0026#39;g--\u0026#39; )\rBy doing this, we can show two windows with their graphs at the same time. Also, we can use subplots within figures.\n\rPLOTTING STYLES\rIn order to use a style, we need to import the style module of Matplotlib and then call the function use .\nfrom matplotlib import style\rstyle.use( \u0026#39;ggplot\u0026#39; )\rBy using the from … import … notation we don’t need to specify the parent module matplotlib . Here we apply the style of ggplot . This adds a grid and some other design changes to our plots. For more information, check out the link above.\n\rLABELING DIAGRAMS\rIn order to make our graphs understandable, we need to label them properly. We should label the axes, we should give our windows titles and in some cases we should also add a legend.\n\rSETTING TITLES\rLet’s start out by setting the titles of our graphs and windows.\nx = np.linspace( 0 , 50 , 100 )\ry = np.sin(x)\rplt.title( \u0026#39;Sine Function\u0026#39; )\rplt.suptitle( \u0026#39;Data Science\u0026#39; )\rplt.grid( True )\rplt.plot(x,y)\rplt.show()\rIn this example, we used the two functions title and suptitle . The first function adds a simple title to our plot and the second one adds an additional centered title above it. Also, we used the grid function, to turn on the grid of our plot.\nIf you want to change the title of the window, you can use the figure function that we already know.\nplt.figure( \u0026#39;MyFigure\u0026#39; )\r\rLABELING AXES\rAs a next step, we are going to label our axes. For this, we use the two functions xlabel and ylabel .\nplt.xlabel( \u0026#39;x-values\u0026#39; )\rplt.ylabel( \u0026#39;y-values\u0026#39; )\rYou can choose whatever labels you like. When we combine all these pieces of code, we end up with a graph like this:\nIn this case, the labels aren’t really necessary because it is obvious what we see here. But sometimes we want to describe what our values actually mean and what the plot is about.\n\rLEGENDS\rSometimes we will have multiple graphs and objects in a plot. We then use legends to label these individual elements, in order to make everything more readable.\nx = np.linspace( 10 , 50 , 100 )\ry1 = np.sin(x)\ry2 = np.cos(x)\ry3 = np.log(x/ 3 )\rplt.plot(x,y1, \u0026#39;b-\u0026#39; , label = \u0026#39;Sine\u0026#39; )\rplt.plot(x,y2, \u0026#39;r-\u0026#39; , label = \u0026#39;Cosine\u0026#39; )\rplt.plot(x,y3, \u0026#39;g-\u0026#39; , label = \u0026#39;Logarithm\u0026#39; )\rplt.legend( loc = \u0026#39;upper left\u0026#39; )\rplt.show()\rHere we have three functions, sine , cosine and a logarithmic function. We draw all graphs into one plot and add a label to them. In order to make these labels visible, we then use the function legend and specify a location for it. Here we chose the upper left . Our result looks like this:\n\rSAVING DIAGRAMS\rSo now that we know quite a lot about plotting and graphing, let’s take a look at how to save our diagrams.\nplt.savefig( \u0026#39;functions.png\u0026#39; )\rActually, this is quite simple. We just plot whatever we want to plot and then use the function savefig to save our figure into an image file.\n\r",
    "ref": "/blog/matplotlib/"
  },{
    "title": "Python - Numpy",
    "date": "",
    "description": "Introduction to Numpy",
    "body": "\r\r\rNUMPY ARRAYS\rWe can’t do a lot of data science with NumPy alone. But it provides the basis for all the high-level libraries or modules for data science. It is essential for the efficient management of arrays and linear algebra.\nIn order to use NumPy, we of course have to import the respective module first.\nimport numpy as np\rAs you can see, we are also defining an alias here, so that we can address NumPy by just writing np .\nCREATING ARRAYS\rTo create a NumPy array, we just use the respective function array and pass a list to it.\na = np.array([ 10 , 20 , 30 ])\rb = np.array([ 1 , 77 , 2 , 3 ])\rNow we can access the values in the same way as we would do it with a list.\nprint (a[ 0 ])\r## 10\rprint (b[ 2 ])\r## 2\r\rMULTI-DIMENSIONAL ARRAYS\rThe arrays we created are one-dimensional arrays. With NumPy, we can create large multi-dimensional arrays that have the same structure as a matrix.\na = np.array([\r[ 10 , 20 , 30 ],\r[ 40 , 50 , 60 ]\r])\rprint (a)\r## [[10 20 30]\r## [40 50 60]]\rHere, we pass two lists within a list as a parameter. This creates a 2x3 matrix. When we print the array, we get the following result:\n[[10 20 30]\r[40 50 60]]\nSince we now have two dimensions, we also need to address two indices, in order to access a specific element.\nprint (a[ 1 ][ 2 ])\r## 60\rIn this case, we are addressing the second row (index one) and the third element or column (index two). Therefore, our result is 60 .\rWe can extend this principle as much as we want. For example, let’s create a much bigger array.\na = np.array([\r[\r[ 10 , 20 , 30 , 40 ], [ 8 , 8 , 2 , 1 ], [ 1 , 1 , 1 , 2 ]\r],\r[\r[ 9 , 9 , 2 , 39 ], [ 1 , 2 , 3 , 3 ], [ 0 , 0 , 3 , 2 ]\r],\r[\r[ 12 , 33 , 22 , 1 ], [ 22 , 1 , 22 , 2 ], [ 0 , 2 , 3 , 1 ]\r]\r], dtype = float )\rHere we have a 3x3x4 matrix and slowly but surely it becomes a bit irritating and we can’t really grasp the structure of the array. This is especially the case when we get into four or more dimensions, since we only perceive three dimensions in everyday life.\nYou can imagine this three-dimensional array as a cube. We have three rows, four columns and three pages or layers. Such visualizations fail in higher dimensions.\nAnother thing that is worth mentioning is the parameter dtype . It stands for data type and allows us to specify which data type our values have. In this case we specified float and therefore our values will be stored as floating point numbers with the respective notation.\n\rFILLING ARRAYS\rInstead of manually filling our arrays with values, we can also use pre-defined functions in certain cases. The only thing we need to specify is the desired function and the shape of the array.\n\rFULL FUNCTION\rBy using the full function for example, we fill an array of a certain shape with the same number. In this case we create a 3x5x4 matrix, which is filled with sevens.\na = np.full(( 3 , 5 , 4 ), 7 )\rprint (a)\r## [[[7 7 7 7]\r## [7 7 7 7]\r## [7 7 7 7]\r## [7 7 7 7]\r## [7 7 7 7]]\r## ## [[7 7 7 7]\r## [7 7 7 7]\r## [7 7 7 7]\r## [7 7 7 7]\r## [7 7 7 7]]\r## ## [[7 7 7 7]\r## [7 7 7 7]\r## [7 7 7 7]\r## [7 7 7 7]\r## [7 7 7 7]]]\r\rZEROS AND ONES\rFor the cases that we want arrays full of zeros or ones, we even have specific functions.\na = np.zeros(( 3 , 3 ))\rb = np.ones(( 2 , 3 , 4 , 2 ))\rHere we create a 3x3 array full of zeros and a four-dimensional array full of ones.\n\rEMPTY AND RANDOM\rOther options would be to create an empty array or one that is filled with random numbers. For this, we use the respective functions once again.\na = np.empty(( 4 , 4 ))\rb = np.random.random(( 2 , 3 ))\rThe function empty creates an array without initializing the values at all. This makes it a little bit faster but also more dangerous to use, since the user needs to manually initialize all the values.\nWhen using the random function, make sure that you are referring to the module np.random . You need to write it two times because otherwise you are calling the library.\n\rRANGES\rInstead of just filling arrays with the same values, we can fill create sequences of values by specifying the boundaries. For this, we can use two different functions, namely arange and linspace .\na = np.arange( 10 , 50 , 5 )\rThe function arange creates a list with values that range from the minimum to the maximum. The step-size has to be specified in the parameters.\n[10 15 20 25 30 35 40 45]\nIn this example, we create have count from 10 to 45 by always adding 5. The result can be seen above.\nBy using linspace we also create a list from a minimum value to a maximum value. But instead of specifying the step-size, we specify the amount of values that we want to have in our list. They will all be spread evenly and have the same distance to their neighbors.\nb = np.linspace( 0 , 100 , 11 )\rHere, we want to create a list that ranges from 0 to 100 and contains 11 elements. This fits smoothly with a difference of 10 between all numbers. So the result looks like this:\n[ 0. 10. 20. 30. 40. 50. 60. 70. 80. 90. 100.]\nOf course, if we choose different parameters, the numbers don’t be that “beautiful”.\n\rNOT A NUMBER (NAN)\rThere is a special value in NumPy that represents values that are not numbers. It is called NaN and stands for Not a Number . We basically just use it as a placeholder for empty spaces. It can be seen as a value that indicates that something is missing at that place.\nWhen importing big data packets into our application, there will sometimes be missing data. Instead of just setting these values to zero or something else, we can set them to NaN and then filter these data sets out.\n\rATTRIBUTES OF ARRAYS\rNumPy arrays have certain attributes that we can access and that provide information about the structure of it.\n\r\rUMPY ARRAY ATTRIBUTES\r\r\r\r\ra.shape\rReturns the shape of the array\r\r\re.g. (3,3) or (3,4,7)\r\ra.ndim\rReturns how many dimensions our array has\r\ra.size\rReturns the amount of elements an array has\r\ra.dtype\rReturns the data type of the values in the array\r\r\r\r\rMATHEMATICAL OPERATIONS\rNow that we know how to create an array and what attributes it has, let’s take a look at how to work with arrays. For this, we will start out with basic mathematical operations.\n\rARITHMETIC OPERATIONS\ra = np.array([\r[ 1 , 4 , 2 ],\r[ 8 , 8 , 2 ]\r])\rprint (a + 2 )\r## [[ 3 6 4]\r## [10 10 4]]\rprint (a - 2 )\r## [[-1 2 0]\r## [ 6 6 0]]\rprint (a * 2 )\r## [[ 2 8 4]\r## [16 16 4]]\rprint (a / 2 )\r## [[0.5 2. 1. ]\r## [4. 4. 1. ]]\rWhen we perform basic arithmetic operations like addition, subtraction, multiplication and division to an array and a scalar, we apply the operation on every single element in the array. Let’s take a look at the results:\n[[ 3 6 4]\r[10 10 4]]\r[[-1 2 0]\r[ 6 6 0]]\r[[ 2 8 4]\r[16 16 4]]\r[[0.5 2. 1. ]\r[4. 4. 1. ]]\nAs you can see, when we multiply the array by two, we multiply every single value in it by two. This is also the case for addition, subtraction and division. But what happens when we apply these operations on two arrays?\na = np.array([\r[ 1 , 4 , 2 ],\r[ 8 , 8 , 2 ]\r])\rb = np.array([\r[ 1 , 2 , 3 ]\r])\rc = np.array([\r[ 1 ],\r[ 2 ]\r])\rd = np.array([\r[ 1 , 2 , 3 ],\r[ 3 , 2 , 1 ]\r]) \rIn order to apply these operations on two arrays, we need to take care of the shapes. They don’t have to be the same, but there has to be a reasonable way of performing the operations. We then again apply the operations on each element of the array.\nFor example, look at a and b . They have different shapes but when we add these two, they share at least the amount of columns.\nprint (a+b)\r## [[ 2 6 5]\r## [ 9 10 5]]\r[[ 2 6 5]\r[ 9 10 5]]\nSince they match the columns, we can just say that we add the individual columns, even if the amount of rows differs.\rThe same can also be done with a and c where the rows match and the columns differ.\nprint (a+c)\r## [[ 2 5 3]\r## [10 10 4]]\r[[ 2 5 3]\r[10 10 4]]\nAnd of course it also works, when the shapes match exactly. The only problem is when the shapes differ too much and there is no reasonable way of performing the operations. In these cases, we get ValueErrors .\n\rMATHEMATICAL FUNCTIONS\rAnother thing that the NumPy module offers us is mathematical functions that we can apply to each value in an array.\n\r\rNUMPY MATHEMATICAL FUNCTIONS\r\r\r\r\rnp.exp(a)\rTakes e to the power of each value\r\rnp.sin(a)\rReturns the sine of each value\r\rnp.cos(a)\rReturns the cosine of each value\r\rnp.tan(a)\rReturns the tangent of each value\r\rnp.log(a)\rReturns the logarithm of each value\r\rnp.sqrt(a)\rReturns the square root of each value\r\r\r\r\rAGGREGATE FUNCTIONS\rNow we are getting into the statistics. NumPy offers us some so-called aggregate functions that we can use in order to get a key statistic from all of our values.\n\r\rNUMPY AGGREGATE FUNCTIONS\r\r\r\r\ra.sum()\rReturns the sum of all values in the array\r\ra.min()\rReturns the lowest value of the array\r\ra.max()\rReturns the highest value of the array\r\ra.mean()\rReturns the arithmetic mean of all values in the array\r\rnp.median(a)\rReturns the median value of the array\r\rnp.std(a)\rReturns the standard deviation of the values in the array\r\r\r\r\rMANIPULATING ARRAYS\rNumPy offers us numerous ways in which we can manipulate the data of our arrays. Here, we are going to take a quick look at the most important functions and categories of functions.\rIf you just want to change a single value however, you can just use the basic indexing of lists.\na = np.array([\r[ 4 , 2 , 9 ],\r[ 8 , 3 , 2 ]\r])\ra[ 1 ][ 2 ] = 7 \rSHAPE MANIPULATION FUNCTIONS\rOne of the most important and helpful types of functions are the shape manipulating functions . These allow us to restructure our arrays without changing their values.\n\r\r\r\rSHAPE MANIPULATION FUNCTIONS\r\r\r\r\ra.reshape(x,y)\rReturns an array with the same values structured in a different shape\r\ra.flatten()\rReturns a flattened one-dimensional copy of the array\r\ra.ravel()\rDoes the same as flatten but works with the actual array instead of a copy\r\ra.transpose()\rReturns an array with the same values but swapped dimensions\r\ra.swapaxes()\rReturns an array with the same values but two swapped axes\r\ra.flat\rNot a function but an iterator for the flattened version of the array\r\r\r\rThere is one more element that is related to shape but it’s not a function. It is called flat and it is an iterator for the flattened one-dimensional version of the array. Flat is not callable but we can iterate over it with for loops or index it.\nfor x in a.flat:\rprint (x)\r## 4\r## 2\r## 9\r## 8\r## 3\r## 7\rprint (a.flat[ 5 ])\r## 7\r\r\rJOINING FUNCTIONS\rWe use joining functions when we combine multiple arrays into one new array.\n\r\rJOINING FUNCTIONS\r\r\r\r\rFUNCTION\rDESCRIPTION\r\rnp.concatenate(a,b)\rJoins multiple arrays along an existing axis\r\rnp.stack(a,b)\rJoins multiple arrays along a new axis\r\rnp.hstack(a,b)\rStacks the arrays horizontally (column-wise)\r\rnp.vstack(a,b)\rStacks the arrays vertically\r\r\r(row-wise)\r\r\r\rIn the following, you can see the difference between concatenate and stack :\na = np.array([ 10 , 20 , 30 ])\rb = np.array([ 20 , 20 , 10 ])\rprint (np.concatenate((a,b)))\r## [10 20 30 20 20 10]\rprint (np.stack((a,b)))\r## [[10 20 30]\r## [20 20 10]]\r[10 20 30 20 20 10]\r[[10 20 30]\r[20 20 10]]\nWhat concatenate does is, it joins the arrays together by just appending one onto the other. Stack on the other hand, creates an additional axis that separates the two initial arrays.\nSPLITTING FUNCTIONS\rWe can not only join and combine arrays but also split them again. This is done by using splitting functions that split arrays into multiple sub-arrays.\n\r\r\r\rSPLITTING FUNCTIONS\r\r\r\r\rnp.split(a, x)\rSplits one array into multiple arrays\r\rnp.hsplit(a, x)\rSplits one array into multiple arrays horizontally (column-wise)\r\rnp.vsplit(a, x)\rSplits one array into multiple arrays vertically (row-wise)\r\r\r\rWhen splitting a list with the split function, we need to specify into how many sections we want to split our array.\na = np.array([\r[ 10 , 20 , 30 ],\r[ 40 , 50 , 60 ],\r[ 70 , 80 , 90 ],\r[ 100 , 110 , 120 ]\r])\rprint (np.split(a, 2 ))\r## [array([[10, 20, 30],\r## [40, 50, 60]]), array([[ 70, 80, 90],\r## [100, 110, 120]])]\rprint (np.split(a, 4 ))\r## [array([[10, 20, 30]]), array([[40, 50, 60]]), array([[70, 80, 90]]), array([[100, 110, 120]])]\rThis array can be split into either two or four equally sized arrays on the default axis. The two possibilities are the following:\n1: [[10, 20, 30],[40, 50, 60]]\r2: [[70, 80, 90],[100, 110, 120]]\nOR\n1: [[10, 20, 30]]\r2: [[40, 50, 60]]\r3: [[70, 80, 90]]\r4: [[100, 110, 120]]\n\r\rADDING AND REMOVING\rThe last manipulating functions that we are going to look at are the ones which allow us to add and to remove items.\n\r\r\r\rADDING AND REMOVING FUNCTIONS\r\r\r\r\rnp.resize(a, (x,y))\rReturns a resized version of the array and fills empty spaces by repeating copies of a\r\rnp.append(a, […])\rAppends values at the end of the array\r\rnp.insert(a, x, …)\rInsert a value at the index x of the array\r\rnp.delete(a, x, y)\rDelete axes of the array\r\r\r\r\rLOADING AND SAVING ARRAYS\rNow last but not least, we are going to talk about loading and saving NumPy arrays. For this, we can use the integrated NumPy format or CSV-files.\n\rNUMPY FORMAT\rBasically, we are just serializing the object so that we can use it later. This is done by using the save function.\na = np.array([\r[ 10 , 20 , 30 ],\r[ 40 , 50 , 60 ],\r[ 70 , 80 , 90 ],\r[ 100 , 110 , 120 ]\r])\rnp.save( \u0026#39;myarray.npy\u0026#39; , a)\rNotice that you don’t have to use the file ending npy . In this example, we just use it for clarity. You can pick whatever you want.\rNow, in order to load the array into our script again, we will need the load function.\na = np.load( \u0026#39;myarray.npy\u0026#39; )\rprint (a)\r\rCSV FORMAT\rAs I already mentioned, we can also save our NumPy arrays into CSV files, which are just comma-separated text files. For this, we use the function savetxt .\nnp.savetxt( \u0026#39;myarray.csv\u0026#39; , a)\rOur array is now stored in a CSV-file which is very useful, because it can then also be read by other applications and scripts.\nIn order to read this CSV-file back into our script, we use the function loadtxt .\na = np.loadtxt( \u0026#39;myarray.csv\u0026#39; )\rprint (a)\rIf we want to read in a CSV-file that uses another separator than the default one, we can specify a certain delimiter.\na = np.loadtxt( \u0026#39;myarray.csv\u0026#39; , delimiter = \u0026#39;;\u0026#39; )\rprint (a)\rNow it uses semi-colons as separator when reading the file. The same can also be done with the saving or writing function\n\r\r",
    "ref": "/blog/numpy/"
  },{
    "title": "Python - Regular Expressions",
    "date": "",
    "description": "Introduction to Regular Expressions",
    "body": "\r\r\rREGULAR EXPRESSIONS\rIn programming, you will oftentimes have to deal with long texts from which we want to extract specific information. Also, when we want to process certain inputs, we need to check for a specific pattern. For example, think about emails. They need to have some text, followed by an @ character, then again some text and finally a dot and again some little text.\nIn order to make the validations easier, more efficient and more compact, we use so-called regular expressions .\nThe topic of regular expressions is very huge and you could write a whole book only about it. This is why we are not going to focus too much on the various placeholders and patterns of the expressions themselves but on the implementation of RegEx in Python.\rSo in order to confuse you right in the beginning, let’s look at a regular expression that checks if the format of an email-address is valid.\n^[a-zA-Z0-9.!#$%\u0026amp;\u0026#39;*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*$\rNow you can see why this is a huge field to learn. We are going to focus on quite simple examples and how to properly implement them in Python.\nIDENTIFIER\rLet’s get started with some basic knowledge first. So-called identifiers define what kind of\rcharacter should be at a certain place. Here you have some examples:\n| |REGEX IDENTIFIERS | |\r|--------------------|--------------------------------------|\r| IDENTIFIER | DESCRIPTION |\r| \\d | Some digit |\r| \\D | Everything BUT a digit |\r| \\s | White space |\r| \\S | Everything BUT a white space |\r| \\w | Some letter |\r| \\W | Everything BUT a letter |\r| . | Every character except for new lines |\r| \\b | White spaces around a word |\r| \\. | A dot |\r\rMODIFIER\rThe modifiers extend the regular expressions and the identifiers. They might be seen as some kind of operator for regular expressions.\n| REGEX MODIFIERS | |\r|-----------------|--------------------------------------------|\r| MODIFIER | DESCRIPTION |\r| {x,y} | A number that has a length between x and y |\r| + | At least one |\r| ? | None or one |\r| * | Everything |\r| $ | At the end of a string |\r| ^ | At the beginning of a string |\r| | | Either Or |\r| | Example: x | y = either x or y |\r| [] | Value range |\r| {x} | x times |\r| {x,y} | x to y times |\r\rESCAPE CHARACTERS\r| REGEX ESCAPE CHARATCERS | |\r|-------------------------|-------------|\r| CHARACTER | DESCRIPTION |\r| \\n | New Line |\r| \\t | Tab |\r| \\s | White Space |\r\rAPPLYING REGULAR EXPRESSIONS\rFINDING STRINGS\nIn order to apply these regular expressions in Python, we need to import the module re .\nimport re\rNow we can start by trying to find some patterns in our strings.\ntext = \u0026#39;\u0026#39;\u0026#39;\rMike is 20 years old and George is 29!\rMy grandma is even 104 years old!\r\u0026#39;\u0026#39;\u0026#39;\rages = re.findall( r\u0026#39;\\d{1,3}\u0026#39; , text)\rprint (ages)\r## [\u0026#39;20\u0026#39;, \u0026#39;29\u0026#39;, \u0026#39;104\u0026#39;]\rIn this example, we have a text with three ages in it. What we want to do is to filter these out and print them separately.\nAs you can see, we use the function findall in order to apply the regular expression onto our string. In this case, we are looking for numbers that are one to three digits long. Notice that we are using an r character before we write our expression. This indicates that the given string is a regular expression.\nAt the end, we print our result and get the following output:\n[‘20’, ‘29’, ‘104’]\n\rMATCHING STRINGS\rWhat we can also do is to check if a string matches a certain regular expression. For example, we can apply our regular expression for mails here.\nimport re\rtext = \u0026#39;test@mail.com\u0026#39;\rresult = re.fullmatch( r\u0026#39;^[a-zA-Z0-9.!#$%\u0026amp;\u0026#39;*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*$\u0026#39; , text)\rif result != None :\rprint ( \u0026#39;VALID!\u0026#39; )\relse :\rprint ( \u0026#39;INVALID!\u0026#39; )\rWe are not going to talk about the regular expression itself here. It is very long and complicated. But what we see here is a new function called fullmatch . This function returns the checked string if it matches the regular expression. In this case, this happens when the string has a valid mail format.\nIf the expression doesn’t match the string, the function returns None . In our example above, we get the message “VALID!” since the expression is met. If we enter something like “Hello World!”, we will get the other message.\nMANIPULATING STRINGS\rFinally, we are going to take a look at manipulating strings with regular expressions. By using the function sub we can replace all the parts of a string that match the expression by something else.\nimport re\rtext = \u0026#39;\u0026#39;\u0026#39;\rMike is 20 years old and George is 29!\rMy grandma is even 104 years old!\r\u0026#39;\u0026#39;\u0026#39;\rtext = re.sub( r\u0026#39;\\d{1,3}\u0026#39; , \u0026#39;100\u0026#39; , text)\rprint (text)\r## ## Mike is 100 years old and George is 100!\r## My grandma is even 100 years old!\rIn this example, we replace all ages by 100 . This is what gets printed:\rMike is 100 years old and George is 100!\rMy grandma is even 100 years old!\nThese are the basic functions that we can operate with in Python when dealing with regular expressions. If you want to learn more about regular expressions just google and you will find a lot of guides. Play around with the identifiers and modifiers a little bit until you feel like you understand how they work.\n\r\r\r",
    "ref": "/blog/regular-expressions/"
  },{
    "title": "Python - Logging",
    "date": "",
    "description": "Introduction to Python Logging",
    "body": "\r\r\rLOGGING\rNo matter what we do in computer science, sooner or later we will need logs. Every system that has a certain size produces errors or conditions in which specific people should be warned or informed. Nowadays, everything gets logged or recorded. Bank transactions, flights, networking activities, operating systems and much more. Log files help us to find problems and to get information about the state of our systems. They are an essential tool for avoiding and understanding errors.\rUp until now, we have always printed some message onto the console screen when we encountered an error. But when our applications grow, this becomes confusing and we need to categorize and outsource our logs. In addition, not every message is equally relevant. Some messages are urgent because a critical component fails and some just provide nice information.\nSECURITY LEVELS\rIn Python, we have got five security levels. A higher level means higher importance or urgency.\n\rDEBUG\rINFO\rWARNING\rERROR\rCRITICAL\n\rNotice that when we choose a certain security level, we also get all the messages of the levels above. So for example, INFO also prints the messages of WARNING, ERROR and CRITICAL but not of DEBUG .\rDEBUG is mainly used for tests, experiments or in order to check something. We typically use this mode, when we are looking for errors (troubleshooting).\rWe use INFO when we want to log all the important events that inform us about what is happening. This might be something like “User A logged in successfully!” or “Now we have 17 users online!”\rWARNING messages are messages that inform us about irregularities and things that might go wrong and become a problem. For example messages like “Only 247 MB of RAM left!”\rAn ERROR message gets logged or printed when something didn’t go according to the plan. When we get an exception this is a classical error.\rCRITICAL messages tell us that critical for the whole system or application happened. This might be the case when a crucial component fails and we have to immediately stop all operations.\n\rCREATING LOGGERS\rIn order to create a logger in Python, we need to import the logging module.\nimport logging\rNow we can just log messages by directly using the respective functions of the logging module.\nlogging.info( \u0026#39;First informational message!\u0026#39; )\rlogging.critical( \u0026#39;This is serious!\u0026#39; )\rThis works because we are using the root logger. We haven’t created our own loggers yet. The output looks like this:\nCRITICAL:root:This is serious!\nINFO:root:Logger successfully created!\nSo let’s create our own logger now. This is done by either using the constructor of the Logger class or by using the method getLogger .\nlogger = logging.getLogger()\rlogger = logging.Logger( \u0026#39;MYLOGGER\u0026#39; )\rNotice that we need to specify a name for our logger, if we use the constructor. Now we can log our messages.\nlogger.info( \u0026#39;Logger successfully created!\u0026#39; )\rlogger.log(logging.INFO, \u0026#39;Successful!\u0026#39; )\rlogger.critical( \u0026#39;Critical Message!\u0026#39; )\rlogger.log(logging.CRITICAL, \u0026#39;Critical!\u0026#39; )\rHere we also have two different options for logging messages. We can either directly call the function of the respective security level or we can use the method log and specify the security level in the parameters.\nBut when you now execute the script, you will notice that it will only print the critical messages. This has two reasons. First of all, we need to adjust the level of the logger and second of all, we need to remove all of the handlers from the default root logger.\nfor handler in logging.root.handlers:\nlogging.root.removeHandler(handler)\rlogging.basicConfig( level =logging.INFO)\rHere we just use a for loop in order to remove all the handlers from the root logger. Then we use the basicConfig method, in order to set our logging level to INFO . When we now run our code again, the output is the following:\nINFO:MYLOGGER:Logger successfully created!\rINFO:MYLOGGER:Successful!\rCRITICAL:MYLOGGER:Critical Message!\rCRITICAL:MYLOGGER:Critical!\n\rLOGGING INTO FILES\rWhat we are mainly interested in is logging into files. For this, we need a so-called FileHandler . It is an object that we add to our logger, in order to make it log everything into a specific file.\nimport logging\rlogger = logging.getLogger( \u0026#39;MYLOGGER\u0026#39; )\rlogger.setLevel(logging.INFO)\rhandler = logging.FileHandler( \u0026#39;logfile.log\u0026#39; )\rhandler.setLevel(logging.INFO)\rlogger.addHandler(handler)\rlogger.info( \u0026#39;Log this into the file!\u0026#39; )\rlogger.critical( \u0026#39;This is critical!\u0026#39; )\rWe start again by defining a logger. Then we set the security level to INFO by using the function setLevel . After that, we create a FileHandler that logs into the file logfile.log . Here we also need to set the security level. Finally, we add the handler to our logger using the addHandler function and start logging messages.\n\rFORMATTING LOGS\rOne thing that you will notice is that we don’t have any format in our logs. We don’t know which logger was used or which security level our message has. For this, we can use a so-called formatter .\nimport logging\rlogger = logging.getLogger()\rlogger.setLevel(logging.INFO)\rhandler = logging.FileHandler( \u0026#39;logfile.log\u0026#39; )\rhandler.setLevel(logging.INFO)\rformatter = logging.Formatter( \u0026#39;%(asctime)s: %(levelname)s - %(message)s\u0026#39; )\rhandler.setFormatter(formatter)\rlogger.addHandler(handler)\rlogger.info( \u0026#39;This will get into the file!\u0026#39; )\rWe create a formatter by using the constructor of the respective class. Then we use the keywords for the timestamp, the security level name and the message. Last but not least, we assign the formatter to our handler and start logging again. When we now look into our file, we will find a more detailed message.\r2019-06-25 15:41:43,523: INFO - This will get into the file!\rThese log messages can be very helpful, if they are used wisely. Place them wherever something important or alarming happens in your code\n\r\r",
    "ref": "/blog/python-logging/"
  },{
    "title": "Python - XML Processing",
    "date": "",
    "description": "Introduction to Python XML Processing",
    "body": "\r\r\rXML PROCESSING\rUp until now, we either saved our data into regular text files or into professional databases. Sometimes however, our script is quite small and doesn’t need a big database but we still want to structure our data in files. For this, we can use XML .\nXML stands for Extensible Markup Language and is a language that allows us to hierarchically structure our data in files. It is platform-independent and also application-independent. XML files that you create with a Python script, can be read and processed by a C++ or Java application.\nXML PARSER\rIn Python, we can choose between two modules for parsing XML files – SAX and DOM .\n\rSIMPLE API FOR XML (SAX)\rSAX stands for Simple API for XML and is better suited for large XML files or in situations where we have very limited RAM memory space. This is because in this mode we never load the full file into our RAM. We read the file from our hard drive and only load the little parts that we need right at the moment into the RAM. An additional effect of this is that we can only read from the file and not manipulate it and change values.\n\rDOCUMENT OBJECT MODEL (DOM)\rDOM stands for Document Object Model and is the generally recommended option. It is a language-independent API for working with XML. Here we always load the full XML file into our RAM and then save it there in a hierarchical structure. Because of that, we can use all of the features and also manipulate the file.\nObviously, DOM is a lot faster than SAX because it is using the RAM instead of the hard disk. The main memory is way more efficient than the hard drive. We only use SAX when our RAM is so limited that we can’t even load the full XML file into it without problems.\rThere is no reason to not use both options in the same projects. We can choose depending on the use case.\n\rXML STRUCTURE\rFor this,we are going to use the following XML file:\n\u0026lt;? xml version= \u0026#39;1.0\u0026#39; ?\u0026gt;\r\u0026lt; group \u0026gt;\r\u0026lt; person id= \u0026#39;1\u0026#39; \u0026gt;\r\u0026lt; name \u0026gt;John Smith\u0026lt;/ name \u0026gt;\r\u0026lt; age \u0026gt;20\u0026lt;/ age \u0026gt;\r\u0026lt; weight \u0026gt;80\u0026lt;/ weight \u0026gt;\r\u0026lt; height \u0026gt;188\u0026lt;/ height \u0026gt;\r\u0026lt;/ person \u0026gt;\r\u0026lt; person id= \u0026#39;2\u0026#39; \u0026gt;\r\u0026lt; name \u0026gt;Mike Davis\u0026lt;/ name \u0026gt;\r\u0026lt; age \u0026gt;45\u0026lt;/ age \u0026gt;\r\u0026lt; weight \u0026gt;82\u0026lt;/ weight \u0026gt;\r\u0026lt; height \u0026gt;185\u0026lt;/ height \u0026gt;\r\u0026lt;/ person \u0026gt;\r\u0026lt; person id= \u0026#39;3\u0026#39; \u0026gt;\r\u0026lt; name \u0026gt;Anna Johnson\u0026lt;/ name \u0026gt;\r\u0026lt; age \u0026gt;33\u0026lt;/ age \u0026gt;\r\u0026lt; weight \u0026gt;67\u0026lt;/ weight \u0026gt;\r\u0026lt; height \u0026gt;167\u0026lt;/ height \u0026gt;\r\u0026lt;/ person \u0026gt;\r\u0026lt; person id= \u0026#39;4\u0026#39; \u0026gt;\r\u0026lt; name \u0026gt;Bob Smith\u0026lt;/ name \u0026gt;\r\u0026lt; age \u0026gt;60\u0026lt;/ age \u0026gt;\r\u0026lt; weight \u0026gt;70\u0026lt;/ weight \u0026gt;\r\u0026lt; height \u0026gt;174\u0026lt;/ height \u0026gt;\r\u0026lt;/ person \u0026gt;\r\u0026lt; person id= \u0026#39;5\u0026#39; \u0026gt;\r\u0026lt; name \u0026gt;Sarah Pitt\u0026lt;/ name \u0026gt;\r\u0026lt; age \u0026gt;12\u0026lt;/ age \u0026gt;\r\u0026lt; weight \u0026gt;50\u0026lt;/ weight \u0026gt;\r\u0026lt; height \u0026gt;152\u0026lt;/ height \u0026gt;\r\u0026lt;/ person \u0026gt;\r\u0026lt;/ group \u0026gt; \rAs you can see, the structure is quite simple. The first row is just a notation and indicates that we are using XML version one. After that we have various tags. Every tag that gets opened also gets closed at the end.\rBasically, we have one group tag. Within that, we have multiple person tags that all have the attribute id . And then again, every person has four tags with their values. These tags are the attributes of the respective person. We save this file as group.xml .\n\rXML WITH SAX\rIn order to work with SAX, we first need to import the module:\nimport xml.sax\rNow, what we need in order to process the XML data is a content handler . It handles and processes the attributes and tags of the file.\nimport xml.sax\rhandler = xml.sax.ContentHandler()\rparser = xml.sax.make_parser()\rparser.setContentHandler(handler)\rparser.parse( \u0026#39;group.xml\u0026#39; )\rFirst we create an instance of the ContentHandler class. Then we use the method make_parser, in order to create a parser object. After that, we set our handler to the content handler of our parser. We can then parse the file by using the method parse .\nNow, when we execute our script, we don’t see anything. This is because we need to define what happens when an element gets parsed.\n\rCONTENT HANDLER CLASS\rFor this, we will define our own content handler class. Let’s start with a very simple example.\nimport xml.sax\rclass GroupHandler(xml.sax.ContentHandler):\rdef startElement( self , name, attrs):\rprint (name)\rhandler = GroupHandler()\rparser = xml.sax.make_parser()\rparser.setContentHandler(handler)\rparser.parse( \u0026#39;group.xml\u0026#39; )\rWe created a class GroupHandler that inherits from ContentHandler . Then we overwrite the function startElement . Every time an element gets processed, this function gets called. So by manipulating it, we can define what shall happen during the parsing process.\nNotice that the function has two parameters – name and attr . These represent the tag name and the attributes. In our simple example, we just print the tag names. So, let’s get to a more interesting example.\n\rPROCESSING XML DATA\rThe following example is a bit more complex and includes two more functions.\nimport xml.sax\rclass GroupHandler(xml.sax.ContentHandler):\rdef startElement( self , name, attrs):\rself .current = name\rif self .current == \u0026#39;person\u0026#39; :\rprint ( \u0026#39;--- Person ---\u0026#39; )\rid = attrs[ \u0026#39;id\u0026#39; ]\rprint ( \u0026#39;ID: %s\u0026#39; % id)\rdef endElement( self , name):\rif self .current == \u0026#39;name\u0026#39; :\rprint ( \u0026#39;Name: %s\u0026#39; % self .name)\relif self .current == \u0026#39;age\u0026#39; :\rprint ( \u0026#39;Age: %s\u0026#39; % self .age)\relif self .current == \u0026#39;weight\u0026#39; :\rprint ( \u0026#39;Weight: %s\u0026#39; % self .weight)\relif self .current == \u0026#39;height\u0026#39; :\rprint ( \u0026#39;Height: %s\u0026#39; % self .height)\rself .current = \u0026#39;\u0026#39; def characters( self , content):\rif self .current == \u0026#39;name\u0026#39; :\rself .name = content\relif self .current == \u0026#39;age\u0026#39; :\rself .age = content\relif self .current == \u0026#39;weight\u0026#39; :\rself .weight = content\relif self .current == \u0026#39;height\u0026#39; :\rself .height = content\rhandler = GroupHandler()\rparser = xml.sax.make_parser()\rparser.setContentHandler(handler)\rparser.parse( \u0026#39;group.xml\u0026#39; ) \rThe first thing you will notice here is that we have three functions instead of one. When we start processing an element, the function startElement gets called. Then we go on to process the individual characters which are name, age, weight and height . At the end of the element parsing, we call the endElement function.\nIn this example, we first check if the element is a person or not. If this is the case we print the id just for information. We then go on with the characters method. It checks which tag belongs to which attribute and saves the values accordingly. At the end, we print out all the values. This is what the results look like:\n— Person —\rID: 1\rName: John Smith\rAge: 20\rWeight: 80\rHeight: 188\r— Person —\rID: 2\rName: Mike Davis\rAge: 45\rWeight: 82\rHeight: 185\r— Person —\r…\n\rXML WITH DOM\rNow, let’s look at the DOM option. Here we can not only read from XML files but also change values and attributes. In order to work with DOM, we again need to import the respective module.\nimport xml.dom.minidom\rWhen working with DOM, we need to create a so-called DOM-Tree and view all elements as collections or sequences.\ndomtree = xml.dom.minidom.parse( \u0026#39;group.xml\u0026#39; )\rgroup = domtree.documentElement \rWe parse the XML file by using the method parse . This returns a DOM-tree, which we save into a variable. Then we get the documentElement of our tree and in our case this is group . We also save this one into an object.\npersons = group.getElementsByTagName( \u0026#39;person\u0026#39; )\rfor person in persons:\rprint ( \u0026#39;--- Person ---\u0026#39; )\rif person.hasAttribute( \u0026#39;id\u0026#39; ):\rprint ( \u0026#39;ID: %s\u0026#39; % person.getAttribute( \u0026#39;id\u0026#39; ))\rname = person.getElementsByTagName( \u0026#39;name\u0026#39; )[ 0 ]\rage = person.getElementsByTagName( \u0026#39;age\u0026#39; )[ 0 ]\rweight = person.getElementsByTagName( \u0026#39;weight\u0026#39; )[ 0 ]\rheight = person.getElementsByTagName( \u0026#39;height\u0026#39; )[ 0 ] \rNow, we can get all the individual elements by using the getElementsByTagName function. For example, we save all our person tags into a variable by using this method and specifying the name of our desired tags. Our persons variable is now a sequence that we can iterate over.\nBy using the functions hasAttribute and getAttribute, we can also access the attributes of our tags. In this case, this is only the id . In order to get the tag values of the individual person, we again use the method getElementsByTagName .\nWhen we do all that and execute our script, we get the exact same result as with SAX .\r— Person —\rID: 1\rName: John Smith\rAge: 20\rWeight: 80\rHeight: 188\r— Person —\rID: 2\rName: Mike Davis\rAge: 45\rWeight: 82\rHeight: 185\r— Person —\r…\n\rMANIPULATING XML FILES\rSince we are now working with DOM , let’s manipulate our XML file and change some values.\npersons = group.getElementsByTagName( \u0026#39;person\u0026#39; )\rpersons[ 0 ].getElementsByTagName( \u0026#39;name\u0026#39; )[ 0 ].childNodes[ 0 ].nodeValue = \u0026#39;New Name\u0026#39;\rAs you can see, we are using the same function, to access our elements. Here we adress the name tag of the first person object. Then we need to access the childNodes and change their nodeValue . Notice that we only have one element name and also only one child node but we still need to address the index zero, for the first element.\nIn this example, we change the name of the first person to New Name . Now in order to apply these changes to the real file, we need to write into it.\ndomtree.writexml( open ( \u0026#39;group.xml\u0026#39; , \u0026#39;w\u0026#39; ))\rWe use the writexml method of our initial domtree object. As a parameter, we pass a file stream that writes into our XML file. After doing that, we can look at the changes.\n\u0026lt; person id= \u0026#39;1\u0026#39; \u0026gt;\r\u0026lt; name \u0026gt;New Name\u0026lt;/ name \u0026gt;\r\u0026lt; age \u0026gt;20\u0026lt;/ age \u0026gt;\r\u0026lt; weight \u0026gt;80\u0026lt;/ weight \u0026gt;\r\u0026lt; height \u0026gt;188\u0026lt;/ height \u0026gt;\r\u0026lt;/ person \u0026gt;\rWe can also change the attributes by using the function setAttribute .\npersons[ 0 ].setAttribute( ‘id’ , ‘10’ )\rHere we change the attribute id of the first person to 10 .\n\u0026lt; person id= \u0026#39;10\u0026#39; \u0026gt;\r\u0026lt; name \u0026gt;New Name\u0026lt;/ name \u0026gt;\r\u0026lt; age \u0026gt;20\u0026lt;/ age \u0026gt;\r\u0026lt; weight \u0026gt;80\u0026lt;/ weight \u0026gt;\r\u0026lt; height \u0026gt;188\u0026lt;/ height \u0026gt;\r\u0026lt;/ person \u0026gt;\r\rCREATING NEW ELEMENTS\rWe first need to define a new person element.\nnewperson = domtree.createElement( \u0026#39;person\u0026#39; )\rnewperson.setAttribute( \u0026#39;id\u0026#39; , \u0026#39;6\u0026#39; )\rSo we use the domtree object and the respective method, to create a new XML element. Then we set the id attribute to the next number.\nAfter that, we create all the elements that we need for the person and assign values to them.\nname = domtree.createElement( \u0026#39;name\u0026#39; )\rname.appendChild(domtree.createTextNode( \u0026#39;Paul Smith\u0026#39; ))\rage = domtree.createElement( \u0026#39;age\u0026#39; )\rage.appendChild(domtree.createTextNode( \u0026#39;45\u0026#39; ))\rweight = domtree.createElement( \u0026#39;weight\u0026#39; )\rweight.appendChild(domtree.createTextNode( \u0026#39;78\u0026#39; ))\rheight = domtree.createElement( \u0026#39;height\u0026#39; )\rheight.appendChild(domtree.createTextNode( \u0026#39;178\u0026#39; )) \rFirst, we create a new element for each attribute of the person. Then we use the method appendChild to put something in between the tags of our element. In this case we create a new TextNode , which is basically just text.\nLast but not least, we again need to use the method appendChild in order to define the hierarchical structure. The attribute elements are the childs of the person element and this itself is the child of the group element.\nnewperson.appendChild(name)\rnewperson.appendChild(age)\rnewperson.appendChild(weight)\rnewperson.appendChild(height)\rgroup.appendChild(newperson)\rdomtree.writexml( open ( \u0026#39;group.xml\u0026#39; , \u0026#39;w\u0026#39; ))\rWhen we write these changes into our file, we can see the following results:\n\u0026lt; person id= \u0026#39;6\u0026#39; \u0026gt;\r\u0026lt; name \u0026gt;Paul Smith\u0026lt;/ name \u0026gt;\r\u0026lt; age \u0026gt;45\u0026lt;/ age \u0026gt;\r\u0026lt; weight \u0026gt;78\u0026lt;/ weight \u0026gt;\r\u0026lt; height \u0026gt;178\u0026lt;/ heigh\r\r\r",
    "ref": "/blog/python-xml-processing/"
  },{
    "title": "Python - Database Programming",
    "date": "",
    "description": "Introduction to Python Database Programming",
    "body": "\r\r\rDatabase Programming\rDatabases are one of the most popular ways to store and manage data in computer science. Because of that, in this post we are going to take a look at database programming with Python.\rNotice that for most databases we use the query language SQL , which stands for Structured Query Language . We use this language in order to manage the database, the tables and the rows and columns.\nCONNECTING TO SQLITE\rThe database that comes pre-installed with Python is called SQLite . It is also the one which we are going to use. Of course, there are also other libraries for MySQL, MongoDB etc.\rIn order to use SQLite in Python, we need to import the respective module – sqlite3 .\nimport sqlite3\rNow, to create a new database file on our disk, we need to use the connect method.\nconn = sqlite3.connect( \u0026#39;mydata.db\u0026#39; )\rThis right here creates the new file mydata.db and connects to this database. It returns a connection object which we save in the variable conn .\n\rEXECUTING STATEMENTS\rSo, we have established a connection to the database. But in order to execute SQL statements, we will need to create a so-called cursor .\nc = conn.cursor()\rWe get this cursor by using the method cursor of our connection object that returns it. Now we can go ahead and execute all kinds of statements.\n\rCREATING TABLES\rFor example, we can create our first table like this:\nc.execute( \u0026#39;\u0026#39;\u0026#39;CREATE TABLE persons (\rfirst_name TEXT,\rlast_name TEXT,\rage INTEGER\r)\u0026#39;\u0026#39;\u0026#39; )\rHere we use the execute function and write our query. What we are passing here is SQL code. As I already said, understanding SQL is not the main objective here. We are focusing on the Python part. Nevertheless, it’s quite obvious what’s happening here. We are creating a new table with the name persons and each person will have the three attributes first_name, last_name and age .\nNow our statement is written but in order to really execute it, we ne need to commit to our connection.\nconn.commit()\rWhen we do this, our statement gets executed and our table created. Notice that this works only once, since after that the table already exists and can’t be created again.\rAt the end, don’t forget to close the connection, when you are done with everything.\nconn.close()\r\rINSERTING VALUES\rNow let’s fill up our table with some values. For this, we just use an ordinary INSERT statement.\nc.execute( \u0026#39;\u0026#39;\u0026#39;INSERT INTO persons VALUES\r(\u0026#39;John\u0026#39;, \u0026#39;Smith\u0026#39;, 25),\r(\u0026#39;Anna\u0026#39;, \u0026#39;Smith\u0026#39;, 30),\r(\u0026#39;Mike\u0026#39;, \u0026#39;Johnson\u0026#39;, 40)\u0026#39;\u0026#39;\u0026#39; )\rconn.commit()\rconn.close()\rSo basically, we are just adding three entries to our table. When you run this code, you will see that everything went fine. But to be on the safe side, we will try to now extract the values from the database into our program.\n\rSELECTING VALUES\rIn order to get values from the database, we need to first execute a SELECT statement. After that, we also need to fetch the results.\nc.execute( \u0026#39;\u0026#39;\u0026#39;SELECT * FROM persons\rWHERE last_name = \u0026#39;Smith\u0026#39;\u0026#39;\u0026#39;\u0026#39; )\rprint (c.fetchall())\rconn.commit()\rconn.close()\rAs you can see, our SELECT statement that gets all the entries where the last_name has the value Smith . We then need to use the method fetchall of the cursor, in order to get our results. It returns a list of tuples, where every tuple is one entry. Alternatively, we could use the method fetchone to only get the first entry or fetchmany to get a specific amount of entries. In our case however, the result looks like this:\r[(‘John’, ‘Smith’, 25), (‘Anna’, ‘Smith’, 30)]\n\rCLASSES AND TABLES\rNow in order to make the communication more efficient and easier, we are going to create a Person class that has the columns as attributes.\nclass Person():\rdef __init__ ( self , first= None ,last= None , age= None ):\rself .first = first\rself .last = last\rself .age = age\rdef clone_person( self , result):\rself .first = result[ 0 ]\rself .last = result[ 1 ]\rself .age = result[ 2 ]\rHere we have a constructor with default parameters. In case we don’t specify any values, they get assigned the value None . Also, we have a function clone_person that gets passed a sequence and assigns the values of it to the object. In our case, this sequence will be the tuple from the fetching results.\n\rFROM TABLE TO OBJECT\rSo let’s create a new Person object by getting its data from our database.\nc.execute( \u0026#39;\u0026#39;\u0026#39;SELECT * FROM persons\rWHERE last_name = \u0026#39;Smith\u0026#39;\u0026#39;\u0026#39;\u0026#39; )\rperson1 = Person()\rperson1.clone_person(c.fetchone())\rprint (person1.first)\rprint (person1.last)\rprint (person1.age)\rHere we fetch the first entry of our query results, by using the fetchone function. The result is the following:\rJohn\rSmith\r25\n\rFROM OBJECT TO TABLE\rWe can also do that the other way around. Let’s create a person objects, assign values to the attributes and then insert this object into our database.\nperson2 = Person( \u0026#39;Bob\u0026#39; , \u0026#39;Davis\u0026#39; , 23 )\rc.execute( \u0026#39;\u0026#39;\u0026#39;INSERT INTO persons VALUES\r(\u0026#39;{}\u0026#39;, \u0026#39;{}\u0026#39;, \u0026#39;{}\u0026#39;)\u0026#39;\u0026#39;\u0026#39;\r.format(person2.first,\rperson2.last,\rperson2.age))\rconn.commit()\rconn.close()\rHere we used the basic format function in order to put our values into the statement. When we execute it, our object gets inserted into the database. We can check this by printing all objects of the table persons .\nc.execute( \u0026#39;SELECT * FROM persons\u0026#39; )\rprint (c.fetchall())\rIn the results, we find our new object:\r[(\u0026#39;John\u0026#39;, \u0026#39;Smith\u0026#39;, 25), (\u0026#39;Anna\u0026#39;, \u0026#39;Smith\u0026#39;, 30), (\u0026#39;Mike\u0026#39;, \u0026#39;Johnson\u0026#39;, 40), (\u0026#39;Bob\u0026#39;, \u0026#39;Davis\u0026#39;, 23) ]\rPREPARED STATEMENTS\rThere is a much more secure and elegant way to put the values of our attributes into the SQL statements. We can use prepared statements .\nperson = Person( \u0026#39;Julia\u0026#39; , \u0026#39;Johnson\u0026#39; , 28 )\rc.execute( \u0026#39;INSERT INTO persons VALUES (?, ?, ?)\u0026#39; ,\r(person.first, person.last, person.age))\rconn.commit()\rconn.close()\rWe replace the values with question marks and pass the values as a tuple in the function. This makes our statements cleaner and also less prone to SQL injections.\n\r\r\r",
    "ref": "/blog/python-database-programming/"
  },{
    "title": "Python - Networking",
    "date": "",
    "description": "Introduction to Python Networking",
    "body": "\r\r\r\rNETWORK PROGRAMMING\r\rSOCKETS\rCREATING SOCKETS\rSERVER SOCKET METHODS\rCLIENT-SERVER ARCHITECTURE\rSERVER SOCKET METHODS\rCLIENT SOCKET METHODS\rOTHER SOCKET METHODS\rCREATING A SERVER\rCREATING A CLIENT\rCONNECTING SERVER AND CLIENT\rPORT SCANNER\rTHREADED PORT SCANNER\r\r\r\rNETWORK PROGRAMMING\rNow we get into one of the most interesting intermediate topics – network programming . It is about communicating with other applications and devices via some network. That can be the internet or just the local area network.\nSOCKETS\rWHAT ARE SOCKETS?\nWhenever we talk about networking in programming, we also have to talk about sockets . They are the endpoints of the communication channels or basically, the endpoints that talk to each other. The communication may happen in the same process or even across different continents over the internet.\rWhat’s important is that in Python we have different access levels for the network services. At the lower layers, we can access the simple sockets that allow us to use the connection-oriented and connectionless protocols like TCP or UDP, whereas other Python modules like FTP or HTTP are working on a higher layer – the application layer .\n\rCREATING SOCKETS\rIn order to work with sockets in Python, we need to import the module socket .\nimport socket\rNow, before we start defining and initializing our socket, we need to know a couple of things in advance:\r· Are we using an internet socket or a UNIX socket?\r· Which protocol are we going to use?\r· Which IP-address are we using?\r· Which port number are we using?\nThe first question can be answered quite simply. Since we want to communicate over a network instead of the operating system, we will stick with the internet socket .\nThe next question is a bit trickier. We choose between the protocols TCP ( Transmission Control Protocol) and UDP ( User Datagram Protocol). TCP is connection-oriented and more trustworthy than UDP. The chances of losing data are minimal in comparison to UDP. On the other hand, UDP is much faster than TCP. So the choice depends on the task we want to fulfil. For our examples, we will stick with TCP since we don’t care too much about speed for now.\nThe IP-address should be the address of the host our application will run on. For now, we will use 127.0.0.1 which is the localhost address. This applies to every machine. But notice that this only works when you are running your scripts locally.\nFor our port we can basically choose any number we want. But be careful with low numbers, since all numbers up to 1024 are standardized and all numbers from 1024 to 49151 are reserved . If you choose one of these numbers, you might have some conflicts with other applications or your operating system.\nimport socket\rs = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\rHere we created our first socket, by initializing an instance of the class socket . Notice that we passed two parameters here. The first one AF_INET states that we want an internet socket rather than a UNIX socket . The second one SOCK_STREAM is for the protocol that we choose. In this case it stands\rfor TCP . If we wanted UDP , we would have to choose SOCK_DGRAM.\nA server opens up a session with every client that connects to it. This way, servers are able to serve multiple clients at once and individually.\n\rSERVER SOCKET METHODS\rThere are three methods of the socket class that are of high importance for the servers.\rSo we have a socket that uses the IP protocol (internet) and the TCP protocol. Now, before we get into the actual setup of the socket, we need to talk a little bit about clients and servers.\n\rCLIENT-SERVER ARCHITECTURE\rIn a nutshell, the server is basically the one who provides information and serves data, whereas the clients are the ones who request and receive the data from the server.A server opens up a session with every client that connects to it. This way, servers are able to serve multiple clients at once and individually.\n\rSERVER SOCKET METHODS\r\r\rSERVER SOCKET METHODS\r\r\r\r\rMETHOD\rDESCRIPTION\r\rbind()\rBinds the address that consists of hostname and port to the socket\r\rlisten()\rWaits for a message or a signal\r\raccept()\rAccepts the connection with a client\r\r\r\r\rCLIENT SOCKET METHODS\rFor the client, there is only one specific and very important method, namely connect . With this method the client attempts to connect to a server which then has to accept this with the respective method.\rThere are three methods of the socket class that are of high importance for the servers.\n\rOTHER SOCKET METHODS\r\r\rOTHER SOCKET METHODS\r\r\r\r\rMETHOD\rDESCRIPTION\r\rrecv()\rReceives a TCP message\r\rsend()\rSends a TCP message\r\rrecvfrom()\rReceives a UDP message\r\rsendto()\rSends a UDP message\r\rclose()\rCloses a socket\r\rgethostname()\rReturns hostname of a socket\r\r\r\r\rCREATING A SERVER\rNow that we understand the client-server architecture, we are going to implement our server. We decided that we want to use TCP and an internet socket. For the address we will use the localhost address 127.0.0.1 and as a port, we will choose 9999 .\ns = socket.socket(socket.AF_INET,\rsocket.SOCK_STREAM)\rs.bind(( \u0026#39;127.0.0.1\u0026#39; , 9999 ))\rs.listen()\rprint ( \u0026#39;Listening...\u0026#39; )\rHere we initialize our socket. We then use the method bind , in order to assign the IP-address and the port we chose. Notice that we are passing a tuple as a parameter here. Last but not least, we put our socket to listening mode by using the method listen .After that, we just have to create a loop that accepts the client requests that will eventually come in.server.py\nimport socket\rs = socket.socket(socket.AF_INET,\rsocket.SOCK_STREAM)\rs.bind(( \u0026#39;127.0.0.1\u0026#39; , 9999 ))\rs.listen()\rprint ( \u0026#39;Listening...\u0026#39; )\rwhile True :\rclient, address = s.accept()\rprint ( \u0026#39;Connected to {}\u0026#39; .format(address))\rmessage = \u0026#39;Hello Client!\u0026#39;\rclient.send(message.encode( \u0026#39;ascii\u0026#39; ))\rclient.close()\rThe method accept waits for a connection attempt to come and accepts it. It then returns a client for responses and the address of the client that is connected. We can then use this client object in order to send the message. But it’s important that we encode the message first, because otherwise we can’t send it properly. At the end, we close the client because we don’t need it anymore.\rAlso, there are some other socket methods that are quite important in general.\n\rCREATING A CLIENT\rNow our server is done and we just need some clients that connect to it. Our clients shall request a resource from the server. In this case, this is the message “Hello Client!” .\rFor our client we also need a socket but this time it will not use the function bind but the function connect . So let’s start writing our code into a new file.\nimport socket\rs = socket.socket(socket.AF_INET,\rsocket.SOCK_STREAM)\rs.connect(( \u0026#39;127.0.0.1\u0026#39; , 9999 ))\rWe just create an ordinary internet socket that uses TCP and then connect it to the localhost IP-address at the port 9999.\rTo now get the message from the server and decode it, we will use the recv function.\rclient.py\nimport socket\rs = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\rs.connect(( \u0026#39;127.0.0.1\u0026#39; , 9999 ))\rmessage = s.recv( 1024 )\rs.close()\rprint (message.decode( \u0026#39;ascii\u0026#39; ))\rAfter we connect to the server, we try to receive up to 1024 bytes from it. We then save the message into our variable and then we decode and print it.\n\rCONNECTING SERVER AND CLIENT\rNow in order to connect these two entities, we first need to run our server. If there is no server listening on the respective port, our client can’t connect to anything. So we run our server.py script and start listening.\rAfter that, we can run our client.py script many times and they will all connect to the server. The results will look like this:\nServer\rListening…\rConnected to (‘127.0.0.1’, 4935)\rConnected to (‘127.0.0.1’, 4942)\rConnected to (‘127.0.0.1’, 4943)\rConnected to (‘127.0.0.1’, 4944)\rConnected to (‘127.0.0.1’, 4945)\nClient\rHello Client!\rOne thing you might optimize on that script if you want is the exception handling. If there is no server listening and our client tries to connect, we get a ConnectionRefusedError and our script crashes. Now you can fix this with the knowledge from the first book.\n\rPORT SCANNER\rNow we have learned a lot about multithreading, locking, queues and sockets. With all that knowledge, we can create a highly efficient and well working port scanner .\rWhat a port scanner basically does is: It tries to connect to certain ports at a host or a whole network, in order to find loopholes for future attacks. Open ports mean a security breach. And with our skills, we can already code our own penetration testing tool.\nimport socket\rtarget = \u0026#39;10.0.0.5\u0026#39;\rdef portscan(port):\rtry :\rs = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\rconn = s.connect((target, port))\rreturn True\rexcept :\rreturn False\rfor x in range ( 1 , 501 ):\rif (portscan(x)):\rprint ( \u0026#39;Port {} is open!\u0026#39; .format(x))\relse :\rprint ( \u0026#39;Port {} is closed!\u0026#39; .format(x))\rSo this scanner is quite simple. We define a target address. In this case, this is 10.0.0.5 . Our function portscan simply tries to connect to a certain port at that host. If it succeeds, the function returns True . If we get an error or an exception, it returns False .\rThis is as simple as a port scan can get. We then use a for loop to scan the first 500 ports and we always print if the port is open or closed.\nJust choose a target address and run this script. You will see that it works.\rPort 21 is closed!\rPort 22 is open!\rPort 23 is closed!\rPort 24 is closed!\rPort 25 is open!\nBut you will also notice that it is extremely slow. That’s because we serially scan one port after the other. And I think we have already learned how to handle that.\n\rTHREADED PORT SCANNER\rIn order to speed up the scanning process, we are going to use multithreading . And to make sure that every port gets scanned and also that no port is scanned twice, we will use queues.\nimport socket\rfrom queue import Queue\rimport threading\rtarget = \u0026#39;10.0.0.5\u0026#39;\rq = Queue()\rfor x in range ( 1 , 501 ):\rq.put(x)\rdef portscan(port):\rtry :\rs = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\rconn = s.connect((target, port))\rreturn True\rexcept :\rreturn False\rdef worker():\rwhile True :\rport = q.get()\rif portscan(port):\rprint ( \u0026#39;Port {} is open!\u0026#39;.format(port))\rSo we start by creating a queue and filling it up with all numbers from 1 to 500. We then have two functions. The portscan function does the scanning itself and the worker function gets all the ports from the queue in order to pass them to the portscan function and prints the result. In order to not get confused with the output, we only print when a port is open because we don’t care when a port is closed.\nNow we just have to decide how many threads we want to start and then we can go for it.\nfor x in range ( 30 ):\rt = threading.Thread( target =worker)\rt.start()\rIn this example, we start 30 threads at the same time. If you run this, you will see that it increases the scanning speed a lot. Within a few seconds, all the 500 ports are scanned. So if you want, you can increase the number to 5000.\nThe results for my virtual server are the following:\rPort 25 is open!\rPort 22 is open!\rPort 80 is open!\rPort 110 is open!\rPort 119 is open!\rPort 143 is open!\rPort 443 is open!\rPort 465 is open!\nAs you can see, there are a lot of vulnerabilities here. You now just have to google which ports are interesting and depending on your side you may either prepare for an attack or fix the security breaches. For example port 22 is SSH and quite dangerous.\n\r\r",
    "ref": "/blog/python-networking/"
  },{
    "title": "Python - Queues",
    "date": "",
    "description": "Introduction to Python Queues",
    "body": "\r\r\r\rQueues\r\rQUEUING RESOURCES\rLIFO QUEUES\rPRIORITIZING QUEUES\r\r\r\rQueues\rIn Python, queues are structures that take in data in a certain order to then output it in a certain order. The default queue type is the so-called FIFO queue . This stands for first in first out and the name describes exactly what it does. The elements that enter the queue first are also the elements that will leave the queue first.\nimport queue\rq = queue.Queue()\rfor x in range ( 5 ):\rq.put(x)\rfor x in range ( 5 ):\rprint (q.get(x))\rIn order to work with queues in Python, we need to import the module queue . We can then create an instance of the class Queue by using the constructor.\nAs you can see, we apply two functions here – put and get . The put function adds an element to the queue that can then be extracted by the get function.\nHere, we keep numbers one to five into our queue. Then, we just get the elements and print them. The order stays the same, since the default queue is FIFO .\nQUEUING RESOURCES\rLet’s say we have a list of numbers that need to be processed. We decide to use multiple threads, in order to speed up the process. But there might be a problem. The threads don’t know which number has already been processed and they might do the same work twice, which would be unnecessary. Also, solving the problem with a counter variable won’t always work, because too many threads access the same variable and numbers might get skipped.\rIn this case we can just use queues to solve our problems. We fill up our queue with the numbers and every thread just uses the get function, to get the next number and process it.\rLet’s assume we have the following worker function:\nimport threading\rimport queue\rimport math\rq = queue.Queue()\rthreads = []\rdef worker():\rwhile True :\ritem = q.get()\rif item is None :\rbreak\rprint (math.factorial(item))\rq.task_done()\rWe start out with an empty queue and an empty list for threads. Our function has an endless loop that gets numbers from the list and calculates the factorial of them. For this factorial function, we need to import the module math . But you can ignore this part, since it is only used because the computation requires a lot of resources and takes time. At the end, we use the function task_done of the queue, in order to signal that the element was processed.\nfor x in range ( 5 ):\rt = threading.Thread( target =worker)\rt.start()\rthreads.append(t)\rzahle = [ 1340000 , 13 , 3, 300 , 98 , 88 , 11 , 23 ]\rfor item in zahle:\rq.put(item)\rq.join()\rfor i in range ( 5 ):\rq.put( None )\rWe then use a for loop to create and start five threads that we also add to our list. After that, we create a list of numbers, which we then all put into the queue.\nThe method join of the queue waits for all elements to be extracted and processed. Basically, it is going to wait for all the task_done functions. After that, we put None elements into the queue, so that our loops break.\nNotice that our threads can’t process the same element twice or even skip one because they can only get them by using the get function.\rIf we would use a counter for this task, two threads might increase it at the same time and then skip an element. Or they could just access the same element simultaneously. Queues are irreplaceable for tasks like this.\n\rLIFO QUEUES\rAlternative to the FIFO queues is LIFO queues . That stands for last in first out . You can imagine this queue like some sort of stack. The element you put last on top of the stack is the first that you can get from it.\nimport queue\rq = queue.LifoQueue()\rnumbers = [ 1 , 2 , 3 , 4 , 5 ]\rfor x in numbers:\rq.put(x)\rwhile not q.empty():\rprint (q.get())\rBy using the LifoQueue class from the queue module, we can create an instance of this type. When we now put in the numbers one to five in ascending order, we will get them back in descending order.\rThe result would be:\r5 4 3 2 1\n\rPRIORITIZING QUEUES\rWhat you can also do in Python, is creating prioritized queues . In these, every element gets assigned a level of priority that determines when they will leave the queue.\nimport queue\rq = queue.PriorityQueue()\rq.put(( 8 , \u0026#39;Some string\u0026#39; ))\rq.put(( 1 , 2023 ))\rq.put(( 90 , True ))\rq.put(( 2 , 10.23 ))\rwhile not q.empty():\rprint (q.get())\rHere, we create a new instance of the class PriorityQueue . When we put a new element into this queue, we need to pass a tuple as a parameter. The first element of the tuple is the level of importance (the lower the number, the higher the priority) and the second element is the actual object or value that we want to put into the queue.\nWhen we execute the print statement of the loop, we get the following results:\n(1, 2023)\r(2, 10.23)\r(8, ‘Some string’)\r(90, True)\nAs you can see, the elements got sorted by their priority number. If you only want to access the actual value, you need to address the index one because it is the second value of the tuple.\rwhile not q.empty():\rprint (q.get()[ 1 ])\n\r\r",
    "ref": "/blog/python-queues/"
  },{
    "title": "Python - Multithreading",
    "date": "",
    "description": "Introduction to Python Multithreading",
    "body": "\r\r\r\rMultithreading\r\rHow a thread works\r\rhow to start thread\rStart Vs Run\rWaiting for threads\rThread classes\rSynchronizing Threads\r\r\rSemaphores\r\rDaemon Threads\r\r\r\r\rMultithreading\rThreads are lightweight processes that perform certain actions in a program and they are part of a process themselves. These threads can work in parallel with each other in the same way as two individual applications can.\nSince threads in the same process share the memory space for the variables and the data, they can exchange information and communicate efficiently. Also, threads need fewer resources than processes. That’s why they’re often called lightweight processes.\nHow a thread works\rA thread has a beginning or a start, a working sequence and an end. But it can also be stopped or put on hold at any time. The latter is also called sleep .\rThere are two types of threads: Kernel Threads and User Threads . Kernel threads are part of the operating system, whereas user threads are managed by the programmer. That’s why we will focus on user threads in this book.\nIn Python, a thread is a class that we can create instances of. Each of these instances then represents an individual thread which we can start, pause or stop. They are all independent from each other and they can perform different operations at the same time.\nFor example, in a video game, one thread could be rendering all the graphics, while another thread processes the keyboard and mouse inputs. It would be unthinkable to serially perform these tasks one after the other.\nhow to start thread\rIn order to work with threads in Python, we will need to import the respective library threading .\nimport threading\rThen, we need to define our target function. This will be the function that contains the code that our thread shall be executing. Let’s just keep it simple for the beginning and write a hello world function.\nimport threading\rdef hello():\rprint ( \u0026#39;Hello World!\u0026#39; )\rt1 = threading.Thread( target =hello)\rt1.start()\rAfter we have defined the function, we create our first thread. For this, we use the class Thread of the imported threading module. As a parameter, we specify the target to be the hello function. Notice that we don’t put parentheses after our function name here, since we are not calling it but just referring to it. By using the start method we put our thread to work and it executes our function.\n\rStart Vs Run\rIn this example, we used the function start to put our thread to work. Another alternative would be the function run . The difference between these two functions gets important, when we are dealing with more than just one thread.\rWhen we use the run function to execute our threads, they run serially one after the other. They wait for each other to finish. The start function puts all of them to work simultaneously.\rThe following example demonstrates this difference quite well.\nimport threading\rdef function1():\rfor x in range ( 1000 ):\rprint ( \u0026#39;ONE\u0026#39; )\rdef function2():\rfor x in range ( 1000 ):\rprint ( \u0026#39;TWO\u0026#39; )\rt1 = threading.Thread( target =function1)\rt2 = threading.Thread( target =function2)\rt1.start()\rt2.start()\rWhen you run this script, you will notice that the output alternates between ONEs and TWOs . Now if you use the run function instead of the start function, you will see 1000 times ONE followed by 1000 times TWO . This shows you that the threads are run serially and not in parallel.\nOne more thing that you should know is that the application itself is also the main thread, which continues to run in the background. So while your threads are running, the code of the script will be executed unless you wait for the threads to finish.\n\rWaiting for threads\rimport threading\rdef function():\rfor x in range ( 500000 ):\rprint ( \u0026#39;HELLO WORLD!\u0026#39; )\rt1 = threading.Thread( target =function)\rt1.start()\rprint ( \u0026#39;THIS IS THE END!\u0026#39; )\rIf you execute this code, you will start printing the text “HELLO WORLD!” 500,000 times. But what you will notice is that the last print statement gets executed immediately after our thread starts and not after it ends.\nt1 = threading.Thread( target =function)\rt1.start()\rt1.join()\rprint ( \u0026#39;THIS IS THE END!\u0026#39; )\rBy using the join function here, we wait for the thread to finish before we move on with the last print statement. If we want to set a maximum time that we want to wait, we just pass the number of seconds as a parameter.\nt1 = threading.Thread( target =function)\rt1.start()\rt1.join( 5 )\rprint ( \u0026#39;THIS IS THE END!\u0026#39; )\rIn this case, we will wait for the thread to finish but only a maximum of five seconds. After this time has passed we will proceed with the code.\rNotice that we are only waiting for this particular thread. If we would have other threads running at the same time, we would have to call the join function on each of them in order to wait for all of them.\n\rThread classes\rAnother way to build our threads is to create a class that inherits the Thread class. We can then modify the run function and implement our functionality. The start function is also using the code from the run function so we don’t have to worry about that.\nimport threading\rclass MyThread(threading.Thread):\rdef __init__ ( self , message):\rthreading.Thread. __init__ ( self )\rself .message = message\rdef run( self ):\rfor x in range ( 100 ):\rprint ( self .message)\rmt1 = MyThread( \u0026#39;This is my thread message!\u0026#39; )\rmt1.start()\rIt is basically the same but it offers more modularity and structure, if you want to use attributes and additional functions.\n\rSynchronizing Threads\rSometimes you are going to have multiple threads running that all try to access the same resource. This may lead to inconsistencies and problems. In order to prevent such things there is a concept called locking . Basically, one thread is locking all of the other threads and they can only continue to work when the lock is removed.\nI came up with the following quite trivial example. It seems a bit abstract but you can still get the concept here.\nimport threading\rimport time\rx = 8192\rdef halve():\rglobal x\rwhile (x \u0026gt; 1 ):\rx /= 2\rprint (x)\rtime.sleep( 1 )\rprint ( \u0026#39;END!\u0026#39; )\rdef double():\rglobal x\rwhile (x \u0026lt; 16384 ):\rx *= 2\rprint (x)\rtime.sleep( 1 )\rprint ( \u0026#39;END!\u0026#39; )\rt1 = threading.Thread( target =halve)\rt2 = threading.Thread( target =double)\rt1.start()\rt2.start()\rHere we have two functions and the variable x that starts at the value 8192 . The first function halves the number as long as it is greater than one, whereas the second function doubles the number as long as it is less than 16384 .\nAlso, I’ve imported the module time in order to use the function sleep . This function puts the thread to sleep for a couple of seconds (in this case one second). So it pauses. We just do that, so that we can better track what’s happening.\rWhen we now start two threads with these target functions, we will see that the script won’t come to an end. The halve function will constantly decrease the number and the double function will constantly increase it.\nimport threading\rimport time\rx = 8192\rlock = threading.Lock()\rdef halve():\rglobal x, lock\rlock.acquire()\rwhile (x \u0026gt; 1 ):\rx /= 2\rprint (x)\rtime.sleep( 1 )\rprint ( \u0026#39;END!\u0026#39; )\rlock.release()\rdef double():\rglobal x, lock\rlock.acquire()\rwhile (x \u0026lt; 16384 ):\rx *= 2\rprint (x)\rtime.sleep( 1 )\rprint ( \u0026#39;END!\u0026#39; )\rlock.release()\rt1 = threading.Thread( target =halve)\rt2 = threading.Thread( target =double)\rt1.start()\rt2.start()\rSo here we added a couple of elements. First of all we defined a Lock object. It is part of the threading module and we need this object in order to manage the locking.\rNow, when we want to try to lock the resource, we use the function acquire . If the lock was already locked by someone else, we wait until it is released again before we continue with the code. However, if the lock is free, we lock it ourselves and release it at the end using the release function.\rHere, we start both functions with a locking attempt. The first function that gets executed will lock the other function and finish its loop. After that it will release the lock and the other function can do the same.\rSo the number will be halved until it reaches the number one and then it will be doubled until it reaches the number 16384 .\n\r\r\rSemaphores\rSometimes we don’t want to completely lock a resource but just limit it to a certain amount of threads or accesses. In this case, we can use so-called semaphores .\rTo demonstrate this concept, we will look at another very abstract example.\nimport threading\rimport time\rsemaphore = threading.BoundedSemaphore( value = 5 )\rdef access(thread_number):\rprint ( \u0026#39;{}: Trying access...\u0026#39;.format(thread_number))\rsemaphore.acquire()\rprint ( \u0026#39;{}: Access granted!\u0026#39;.format(thread_number))\rprint ( \u0026#39;{}: Waiting 5 seconds...\u0026#39;.format(thread_number))\rtime.sleep( 5 )\rsemaphore.release()\rprint ( \u0026#39;{}: Releasing!\u0026#39;.format(thread_number))\rfor thread_number in range ( 10 ):\rt = threading.Thread( target =access,args =(thread_number,))\rt.start()\rWe first use the BoundedSemaphore class to create our semaphore object. The parameter value determines how many parallel accesses we allow. In this case, we choose five.\rWith our access function, we try to access the semaphore. Here, this is also done with the acquire function. If there are less than five threads utilizing the semaphore, we can acquire it and continue with the code. But when it’s full, we need to wait until some other thread frees up one space.\rWhen we run this code, you will see that the first five threads will immediately run the code, whereas the remaining five threads will need to wait five seconds until the first threads release the semaphore.\rThis process makes a lot of sense when we have limited resources or limited computational power in a system and we want to limit the access to it.\nWith events we can manage our threads even better. We can pause a thread and wait for a certain event to happen, in order to continue it.\nimport threading\revent = threading.Event()\rdef function():\rprint ( \u0026#39;Waiting for event...\u0026#39; )\revent.wait()\rprint ( \u0026#39;Continuing!\u0026#39; )\rthread = threading.Thread( target =function)\rthread.start()\rx = input ( \u0026#39;Trigger event?\u0026#39; )\rif (x == \u0026#39;yes\u0026#39; ):\revent.set()\rTo define an event we use the Event class of the threading module. Now we define our function which waits for our event. This is done with the wait function. So we start the thread and it waits.\rThen we ask the user, if he wants to trigger the event. If the answer is yes, we trigger it by using the set function. Once the event is triggered, our function no longer waits and continues with the code.\nDaemon Threads\rSo-called daemon threads are a special kind of thread that runs in the background. This means that the program can be terminated even if this thread is still running. Daemon threads are typically used for background tasks like synchronizing, loading or cleaning up files that are not needed anymore. We define a thread as a daemon by setting the respective parameter in the constructor for Thread to True .\nimport threading\rimport time\rpath = \u0026#39;text.txt\u0026#39;\rtext = \u0026#39;\u0026#39;\rdef readFile():\rglobal path, text\rwhile True :\rwith open (path) as file:\rtext = file.read()\rtime.sleep( 3 )\rdef printloop():\rglobal text\rfor x in range ( 30 ):\rprint (text)\rtime.sleep( 1 )\rt1 = threading.Thread( target =readFile, daemon = True )\rt2 = threading.Thread( target =printloop)\rt1.start()\rt2.start()\rSo, here we have two functions. The first one constantly reads in the text from a file and saves it into the text variable. This is done in an interval of three seconds. The second one prints out the content of text every second but only 30 times.\nAs you can see, we start the readFile function in a daemon thread and the printloop function in an ordinary thread. So when we run this script and change the content of the text.txt file while it is running, we will see that it prints the actual content all the time. Of course, we first need to create that file manually.\nAfter it printed the content 30 times however, the whole script will stop, even though the daemon thread is still reading in the files. Since the ordinary threads are all finished, the program ends and the daemon thread just gets terminated With locking we can now let one function finish before the next function starts. Of course, in this example this is not very useful but we can do the same thing in much more complex situations.\n\r\r",
    "ref": "/blog/python-multithreading/"
  },{
    "title": "Python - Classes and Objects",
    "date": "",
    "description": "Introduction to Python Classes and Objects",
    "body": "\r\r\r\rClasses and Objects\r\rCreating Classes\r\rConstructor\rAdding Functions\rClass Variables\rDestructors\rCreating Objects\rHidden Attributes\r\rInheritence\r\r\r\r\rClasses and Objects\rPython is an object-oriented language which means that the code can be divided into individual units, namely objects . Each of these objects is an instance of a so-called class . You can think of the class as some sort of blueprint. For example, the blueprint of a car could be the class and an object would be the actual physical car. So a class has specific attributes and functions but the values vary from object to object.\nCreating Classes\rIn Python, we use the keyword class in order to define a new class. Whatever which is indented after the colon belongs to the class.\nclass Car:\rdef __init__ ( self , manufacturer, model, hp):\rself .manufacturer = manufacturer\rself .model = model\rself .hp = hp\rAfter the class keyword, we put the class name. In this example, this is Car .\nConstructor\rWhat we find first in this case, is a special function called init . This is the so-called constructor. Every time we create an instance or an object of our class, we use this constructor. As you can see, it accepts a couple of parameters. The first one is the parameter self and it is mandatory. Every function of the class needs to have at least this parameter.\rThe other parameters are just our custom attributes. In this case, we have chosen the manufacturer, the model and the horse power (hp).\rWhen we write self.attribute , we refer to the actual attribute of the respective object. We then assign the value of the parameters to it.\n\rAdding Functions\rWe can simply create and add functions to our class that perform certain actions. These functions can also access the attributes of the class.\nclass Car:\rdef __init__ ( self , manufacturer, model, hp):\rself .manufacturer = manufacturer\rself .model = model\rself .hp = hp\rdef print_info( self ):\rprint ( \u0026#39;Manufacturer: {}, Model: {}, HP; {}\u0026#39;.format( self .manufacturer, self.model,self.hp))\rHere we have the function print_info that prints out information about the attributes of the respective object. Notice that we also need the parameter self here.\n\rClass Variables\rIn the following code, you can see that we can use one and the same variable across all the objects of the class, when it is defined without referring to self .\nclass Car:\ramount_cars = 0\rdef __init__ ( self , manufacturer, model, hp):\rself .manufacturer = manufacturer\rself .model = model\rself .hp = hp\rCar.amount_cars += 1\rdef print_car_amount( self ):\rprint ( \u0026#39;Amount: {}\u0026#39;.format(Car.amount_cars)) def print_info( self ):\rprint ( \u0026#39;Manufacturer: {}, Model: {}, HP; {}\u0026#39;.format( self .manufacturer, self.model,self.hp)) \rThe variable amount_cars doesn’t belong to the individual object since it’s not addressed with self . It is a class variable and its value is the same for all objects or instances.\rWhenever we create a new car object, it increases by one. Then, every object can access and print the amount of existing cars.\n\rDestructors\rIn Python, we can also specify a method that gets called when our object gets destroyed or deleted and is no longer needed. This function is called destructor and it is the opposite of the constructor .\nclass Car:\ramount_cars = 0\rdef __init__ ( self , manufacturer, model, hp):\rself.manufacturer = manufacturer\rself.model = model\rself.hp = hp\rCar.amount_cars += 1\rdef __del__ ( self ):\rprint ( \u0026#39;Object gets deleted!\u0026#39; )\rCar.amount_cars -= 1 def print_car_amount( self ):\rprint ( \u0026#39;Amount: {}\u0026#39;.format(Car.amount_cars)) def print_info( self ):\rprint ( \u0026#39;Manufacturer: {}, Model: {}, HP; {}\u0026#39;.format( self .manufacturer, self.model,self.hp)) \rThe destructor function is called del . In this example, we print an informational message and decrease the amount of existing cars by one, when an object gets deleted.\n\rCreating Objects\rNow that we have implemented our class, we can start to create some objects of it.\nmyCar1 = Car( \u0026#39;Tesla1\u0026#39; , \u0026#39;Model X1\u0026#39; , 5251 )\rFirst, we specify the name of our object, like we do with ordinary variables. In this case, the object is called myCar1 . We then create an object of the Car class by writing the class name as a function. This calls the constructor, so we can pass our parameters. We can then use the functions of our car object.\nmyCar1.print_info()\r## Manufacturer: Tesla1, Model: Model X1, HP; 5251\rmyCar1.print_car_amount()\r## Amount: 1\rThe results look like this:\rManufacturer: Tesla, Model: Model X, HP; 525\rAmount: 1\rWhat you can also do is directly access the attributes of an object.\nprint (myCar1.manufacturer)\r## Tesla1\rprint (myCar1.model)\r## Model X1\rprint (myCar1.hp)\r## 5251\rNow we can create some more cars and see how the amount changes.\nmyCar1 = Car( \u0026#39;Tesla1\u0026#39; , \u0026#39;Model X1\u0026#39; , 525 )\r## Object gets deleted!\rmyCar2 = Car( \u0026#39;BMW1\u0026#39; , \u0026#39;X31\u0026#39; , 2001 )\rmyCar3 = Car( \u0026#39;VW1\u0026#39; , \u0026#39;Golf1\u0026#39; , 1001)\rmyCar4 = Car( \u0026#39;Porsche1\u0026#39; , \u0026#39;9111\u0026#39; , 5201 )\rdel myCar3\r## Object gets deleted!\rmyCar1.print_car_amount()\r## Amount: 3\rHere we first create four different car objects. We then delete one of them and finally we print out the car amount. The result is the following:\rObject gets deleted!\rAmount: 3\rNotice that all the objects get deleted automatically when our program ends. But we can manually delete them before that happens by using the del keyword.\n\rHidden Attributes\rIf we want to create hidden attributes that can only be accessed within the class, we can do this with underlines .\nclass MyClass:\rdef __init__ ( self ):\rself.__hidden = \u0026#39;Hello\u0026#39;\rprint ( self .__hidden) # Works\rm1 = MyClass()\r# print (m1.__hidden) # Doesn\u0026#39;t Work \r## Hello\rBy putting two underlines before the attribute name, we make it invisible from outside the class. The first print function works because it is inside of the class. But when we try to access this attribute from the object, we can’t.\n\r\rInheritence\rOne very important and powerful concept of object-oriented programming is inheritance . It allows us to use existing classes and to extend them with new attributes and functions.\rFor example, we could have the parent class which represents a Person and then we could have many child classes like Dancer, Policeman, Artist etc. All of these would be considered a person and they would have the same basic attributes. But they are special kinds of persons with more attributes and functions.\nclass Person:\rdef __init__ ( self , name, age):\rself .name = name\rself .age = age\rdef get_older( self , years):\rself .age += years\rclass Programmer(Person):\rdef __init__ ( self , name, age, language):\rsuper (Programmer, self ). __init__ (name, age)\rself .language = language\rdef print_language( self ):\rprint ( \u0026#39;Favorite Programming Language: {}\u0026#39;.format( self .language)) \r\r\r",
    "ref": "/blog/python-basics-2/"
  },{
    "title": "Python Strings Basics",
    "date": "",
    "description": "Introduction to Python Strings",
    "body": "\r\r\rSTRINGS\rA string defines characters sequences. Strings always need to be surrounded by quotation marks. Otherwise the interpreter will not realize that they are meant to be treated like text. The keyword for String in Python is str .\nTraversing a String\r\r# Traversing\rname = \u0026quot;Welcome\u0026quot;\rfor ch in name:\rprint(ch, \u0026#39;-\u0026#39;, end = \u0026#39; \u0026#39;)\r# Reversing a string\r## W - e - l - c - o - m - e -\rname = \u0026quot;Reverse me\u0026quot;\r# The Slice notation in python has the syntax -\r# list[\u0026lt;start\u0026gt;:\u0026lt;stop\u0026gt;:\u0026lt;step\u0026gt;]\r# So, when you do a[::-1], it starts from the end towards the first taking each element. So it reverses a. This is applicable for lists/tuples as well.\rprint(name[::-1])\r## em esreveR\rlgth = len(name)\rfor a in range(-1, (-lgth-1), -1):\rprint(name[a])\r## Split the string into a list of characters, reverse the list, then rejoin into a single string\r## e\r## m\r## ## e\r## s\r## r\r## e\r## v\r## e\r## R\rprint(\u0026#39;\u0026#39;.join(reversed(\u0026quot;Hello world\u0026quot;)))\r## dlrow olleH\r\rChecking identities of two strings\rYou use == when comparing values and is when comparing identities.\nlang = [\u0026#39;Java\u0026#39;,\u0026#39;Python\u0026#39;]\rmore_lang = lang\rprint(lang == more_lang) # -\u0026gt; True\r## True\rprint(lang is more_lang) # -\u0026gt; True\r## True\reven_more_lang = [\u0026#39;Java\u0026#39;,\u0026#39;Python\u0026#39;]\rprint(lang == even_more_lang) #-\u0026gt; True\r## True\rprint(lang is even_more_lang) #-\u0026gt; False\r## False\rprint(id(lang))\r## 774779336\rprint(id(more_lang))\r## 774779336\rprint(id(even_more_lang))\r\r## 774503176\r\rChecking capital letters\rThe istitle() method checks if each word is capitalized.\n\rprint( \u0026#39;The Hilman\u0026#39;.istitle() ) #=\u0026gt; True\r## True\rprint( \u0026#39;The Cat\u0026#39;.istitle() ) #=\u0026gt; False\r## True\rprint( \u0026#39;the rice\u0026#39;.istitle() ) #=\u0026gt; False\r## False\r\rChecking if string contains another\r\rprint( \u0026#39;A\u0026#39; in \u0026#39;The string containing A\u0026#39; ) #=\u0026gt; True\r## True\rprint( \u0026#39;Apple in\u0026#39; in \u0026#39;The string containing A\u0026#39; ) #=\u0026gt; False\r## False\r\rFinding the index of a substring in a string\r\rprint(\u0026#39;The\u0026#39;.find(\u0026#39;The string containing A\u0026#39;))\r\r## -1\r\r\r",
    "ref": "/blog/python-strings/"
  },{
    "title": "Python - Basics",
    "date": "",
    "description": "Introduction to Python.",
    "body": "\r\r\rCREATING VARIABLES\rCreating variables in Python is very simple. We just choose a name and assign a value.\nmyNumber = 10\rmyText = \u0026#39;Hello\u0026#39; \rHere, we defined two variables. The first one is an integer and the second one a string. You can basically choose whatever name you want but there are some limitations. For example you are not allowed to use reserved keywords like int or dict . Also, the name is not allowed to start with a number or a special character other than the underline.\n\rUSING VARIABLES\rNow that we have defined our variables, we can start to use them. For example, we could print the values.\nprint (myNumber)\r## 10\rprint (myText)\r## Hello\rSince we are not using quotation marks, the text in the parentheses is treated like a variable name. Therefore, the interpreter prints out the values 10 and “Hello” .\n\rTYPECASTING\rSometimes, we will get a value in a data type that we can’t work with properly. For example we might get a string as an input but that string contains a number as its value. Here “10” is not same to 10 . We can’t do calculations with a string, even if the text represents a number. For that reason we need to typecast.\nvalue = \u0026#39;10\u0026#39;\rnumber = int (value)\rTypecasting is done by using the specific data type function. In this case we are converting a string to an integer by using the int keyword. You can also reverse this by using the str keyword. This is a very important thing and we will need it quite often.\n\rPython loops and types\r1. For loop\r\rnumbers = [10,20,30,40]\rfor num in numbers:\rprint(num)\r\r## 10\r## 20\r## 30\r## 40\rfor num in range(10,41,10):\rprint(num)\r## 10\r## 20\r## 30\r## 40\r\r2. While loop\rnumber = 0\rwhile number \u0026lt; 10:\rnumber += 1\rif number == 5:\rbreak\rprint(number)\r## 1\r## 2\r## 3\r## 4\rnum = 0\rwhile num \u0026lt; 10:\rnum += 1\rif num == 5:\rcontinue\rprint(num)\r## 1\r## 2\r## 3\r## 4\r## 6\r## 7\r## 8\r## 9\r## 10\r\r\rData Types\rVariables and Data types basically are just placeholders for values. In programming, that’s the same. The difference is that we have a lot of different data types, and variables cannot only store values of numbers but even of whole objects.\rIn this post we are going to take a look at variables in Python and the differences of the individual data types. Also, we will talk about type conversions.\n1) NUMERICAL DATA TYPES\rThe types you probably already know from mathematics are numerical data types. There are different kinds of numbers that can be used for mathematical operations.\n\r\rNUMERICAL DATA TYPES\r\r\r\r\r\rInteger\rint\rwhole number\r\rFloat\rfloat\rfloating point number\r\rComplex\rcomplex\rcomplex number\r\r\r\rAs you can see, it’s quite simple. An integer is just a regular whole number, which we can do basic calculations with. A float extends the integer and allows decimal places because it is a floating point number. And a complex number is what just a number that has a real and an imaginary component. If you don’t understand complex numbers mathematically, forget about them. You don’t need them for your programming right now.\n\r2) STRINGS\rA string defines characters sequences. Strings always need to be surrounded by quotation marks. Otherwise the interpreter will not realize that they are meant to be treated like text. The keyword for String in Python is str. A string is a derived data type. Strings are immutable. This means that once defined, they cannot be changed. Many Python methods, such as replace() , join() , or split() modify strings\n2.1) Traversing a String\rYou can traverse a string as a substring by using the Python slice operator ([]). It cuts off a substring from the original string and thus allows to iterate over it partially. To use this method, provide the starting and ending indices along with a step value and then traverse the string.\n\rname = \u0026quot;Welcome\u0026quot;\rfor ch in name:\rprint(ch, \u0026#39;-\u0026#39;, end = \u0026#39; \u0026#39;)\r\r## W - e - l - c - o - m - e -\r\r2.2) Reversing a String\rname = \u0026quot;Reverse me\u0026quot;\rThe Slice notation in python has the syntax -\rlist[::]\rSo, when you do a[::-1], it starts from the end towards the first taking each element. So it reverses a. This is applicable for lists/tuples as well.\nReverse using :: operator\nprint(name[::-1])\r## em esreveR\rReverse using for loop\n\rlgth = len(name)\rfor a in range(-1, (-lgth-1), -1):\rprint(name[a], end = \u0026#39; \u0026#39;)\r## e m e s r e v e R\rReverse using functions\n## Split the string into a list of characters, reverse the list, then rejoin into a single string\rprint(\u0026#39;\u0026#39;.join(reversed(\u0026quot;Hello world\u0026quot;)))\r## dlrow olleH\rReverse using list comprehension\n\rname1 = \u0026quot;There are so many stars in the sky\u0026quot;\rn1 = str.split(name1,\u0026quot; \u0026quot;)\rprint(\u0026#39; \u0026#39;.join([y[::-1] for y in n1]))\r## erehT era os ynam srats ni eht yks\r\r2.3) Formating a String\rString formating allows to replace contents in a string with dynamic values using format() function.\n\rcustom_string = \u0026quot;String formatting\u0026quot;\rprint(f\u0026quot;{custom_string} is a useful technique\u0026quot;)\r## String formatting is a useful technique\r\rprint (\u0026quot;Name: %s College Id No: %d Branch: %s Percentile: %f\u0026quot; % (\u0026#39;Vikas\u0026#39;, 38, \u0026#39;CSE\u0026#39;,88.9)) \r## Name: Vikas College Id No: 38 Branch: CSE Percentile: 88.900000\r\r2.4) Length of a Text string\r# length of the text string\rvar3 = \u0026#39;There are so many stars in the sky\u0026#39;\rprint(len(var3))\r## 34\r\r2.5) multiple assignment in python\r\ra, b, c = 1, 2, \u0026quot;Computer Vision\u0026quot;\rprint(a,b,c)\r#try for more than 3 variables\r## 1 2 Computer Vision\r\r2.5) swaping is as easy as this\rvalue1 =90 ; value2 =34\rprint(value1,\u0026#39;-----\u0026#39;,value2)\r## 90 ----- 34\rvalue1,value2 = value2,value1\rprint(value1,\u0026#39;-----\u0026#39;,value2)\r## 34 ----- 90\r\r\r3) Booleans\rBoolean are the most simple data type in Python. They can only have one of two values, namely True or False . It’s a binary data type. We will use it a lot when we get to conditions and loops. The keyword here is bool.\n#Boolean variables\rvar = not True\rvar1 = True\rvar2 = False\rprint(\u0026quot;Values of var, var1 and var2 are \u0026quot; + str(var) + \u0026quot; \u0026quot; +str(var1) + \u0026quot; and \u0026quot; + str(var2))\r## Values of var, var1 and var2 are False True and False\r\r4) Sequences\r\r\rSEQUENCE TYPES\r\r\r\r\r\rList\rlist\rColection of values\r\rTuple\rtuple\rImutable list\r\rDictionary\rdict\rList of key nd value pairs\r\r\r\r\r4.1) Sequences - Lists\rList. Lists are used to store multiple items in a single variable. Lists are one of 4 built-in data types in Python used to store collections of data, the other 3 are Tuple, Set, and Dictionary, all with different qualities and usage.\nnumbers = [10, 20, 30 ,40]\rnames = [\u0026#39;Arun\u0026#39;,\u0026#39;Varun\u0026#39;,\u0026#39;Karun\u0026#39;]\rmixed = [10,\u0026#39;Arun\u0026#39;, 28.3,True ]\rprint(numbers[3])\r## 40\rprint(names[0])\r## Arun\rprint(mixed[3])\r## True\rnumbers[3] = 3\rnames[2] = \u0026#39;Bob\u0026#39;\rprint(numbers[3])\r## 3\rprint(names[2])\r## Bob\r# empty list\r# indexing start with 0\rmy_list = []\rprint(my_list)\r# list of integers\r## []\rmy_list = [1, 2, 3]\rprint(my_list)\r# list with mixed datatypes\r## [1, 2, 3]\rmy_list = [1, \u0026#39;Data Science\u0026#39;, 2.5]\rprint(my_list)\r## [1, \u0026#39;Data Science\u0026#39;, 2.5]\rlist1 = [ \u0026#39;ABC\u0026#39;, 1234 , 2.34, \u0026#39;def\u0026#39;, 71.2 ]\rtinylist = [123, \u0026#39;john\u0026#39;]\rlist2= list1 +tinylist\r# Check the output of each print statement\rprint (list1) \r## [\u0026#39;ABC\u0026#39;, 1234, 2.34, \u0026#39;def\u0026#39;, 71.2]\rprint (list1[-2]) \r## def\rprint (list1[0:3]) \r## [\u0026#39;ABC\u0026#39;, 1234, 2.34]\rprint (list1[2:]) \r## [2.34, \u0026#39;def\u0026#39;, 71.2]\rprint (tinylist * 2) \r## [123, \u0026#39;john\u0026#39;, 123, \u0026#39;john\u0026#39;]\rprint (list1 + tinylist)\r## [\u0026#39;ABC\u0026#39;, 1234, 2.34, \u0026#39;def\u0026#39;, 71.2, 123, \u0026#39;john\u0026#39;]\rprint (list2)\r## [\u0026#39;ABC\u0026#39;, 1234, 2.34, \u0026#39;def\u0026#39;, 71.2, 123, \u0026#39;john\u0026#39;]\rDelete an element from list\n\rlist1 = [\u0026#39;Data\u0026#39;, \u0026#39;Science STTP\u0026#39;, 11, 15]\rprint (list1)\r## [\u0026#39;Data\u0026#39;, \u0026#39;Science STTP\u0026#39;, 11, 15]\rdel list1[2]\rprint (\u0026quot;After deleting value at index 2 : \u0026quot;, list1)\r## After deleting value at index 2 : [\u0026#39;Data\u0026#39;, \u0026#39;Science STTP\u0026#39;, 15]\rMerging two lists\na1 = [1,2,3,4,5,9]\ra2 = [2,4,512,1,3]\ra3 = [\u0026#39;Sawan\u0026#39;, \u0026#39;Gyan\u0026#39;, \u0026#39;Puneet\u0026#39;]\rprint(a1+a2+a3)\r## [1, 2, 3, 4, 5, 9, 2, 4, 512, 1, 3, \u0026#39;Sawan\u0026#39;, \u0026#39;Gyan\u0026#39;, \u0026#39;Puneet\u0026#39;]\r\r4.1.1) Sequences - Lists - Operations\r\r\rLIST OPERATIONS\r\r\r\r\rOPERATION\rRESULT\r\r[10, 20, 30] + [40, 50, 60]\r[10, 20, 30, 40, 50, 60]\r\r[10, “Bob”] * 3\r[10, “Bob”, 10, “Bob”, 10, “Bob”]\r\r\r\r\r4.1.2) Sequences - Lists - Functions\r\r\rLIST FUNCTIONS\r\r\r\r\rFUNCTION\rDESCRIPTION\r\rlen(list)\rReturns the length of a list\r\rmax(list)\rReturns the item with maximum value\r\rmin(list)\rReturns the item with minimum value\r\rlist(element)\rTypecasts element into list\r\r\r\r\r4.1.3) Sequences - Lists - methods\r\r\rLIST METHODS\r\r\r\r\rMETHOD\rDESCRIPTION\r\rlist.append(x)\rAppends element to the list\r\rlist.count(x)\rCounts how many times an element appears in the list\r\rlist.index(x)\rReturns the first index at which the given element occurs\r\rlist.pop()\rRemoves and returns last element\r\rlist.reverse()\rReverses the order of the elements\r\rlist.sort()\rSorts the elements of a list\r\r\r\rRemoving duplicates from list\ndef remove_duplicates():\rli = [3, 2, 2, 1, 1, 1]\rli1 = list(set(li)) #=\u0026gt; [1, 2, 3]\rprint(li1) \rremove_duplicates() \r## [1, 2, 3]\r\r4.2) Sequences - Tupples\rtpl = (10,20,30)\rlen(tpl)\r## 3\rmax(tpl)\r## 30\rmin(tpl)\r## 10\rtuple1 = (\u0026#39;ICICI\u0026#39;,\u0026#39;Branch\u0026#39;, \u0026#39;Malwa\u0026#39;)\rprint(tuple1)\r## (\u0026#39;ICICI\u0026#39;, \u0026#39;Branch\u0026#39;, \u0026#39;Malwa\u0026#39;)\rtuple1[2]\r## \u0026#39;Malwa\u0026#39;\r\r4.3) Sequences - Dictionaries\rA dictionary is indexed by keys, Unlike a sequence, which is indexed by a range of numbers. A Key can be any immutable type, strings and numbers can always be keys.\nA dictionary can be considered as an unordered set of key: value pairs, with the requirement that the keys are unique (within one dictionary). Each key is separated from its value by a colon (:), the items are separated by commas, and the entire unordered ke:value pair in enclosed within curly braces.\nA dictionary can be initialized to be an empty dictionary by using a pair of braces : {}. Placing a comma-separated list of key:value pairs within the braces adds initial key:value pairs to the dictionary; this is also the way dictionaries are written on output.\nTuples can be used as keys if they contain only strings, numbers, or tuples; if a tuple contains any mutable object either directly or indirectly, it cannot be used as a key. You can’t use lists as keys, since lists can be modified in place using index assignments, slice assignments, or methods like append() and extend().\nThe main operations on a dictionary are storing a value with some key and extracting the value given the key.\nTo delete a key:value pair you can use ‘del’. If you store a value using a key that is already in use, then the old value associated with that key is overwritten.\nUse list(d.keys()) to obtain a list of all the keys used in the dictionary, in arbitrary order.\nUse sorted(d.keys()) if you wanted it to be in a sorted order.\nTo check whether a single key is in the dictionary, use the in keyword.\ndic = dict({\u0026#39;Name\u0026#39;:\u0026#39;Arun\u0026#39;, \u0026#39;Age\u0026#39;: 50})\rprint(dic[\u0026#39;Name\u0026#39;])\r## Arun\r\r`4.4) Numpy arrays\rNumPy is a general-purpose fundamental package for scientific computing with Python. It contains various features including these important ones:\nA powerful N-dimensional array object\nSophisticated (broadcasting) functions\nTools for integrating C/C++ and Fortran code\nUseful linear algebra, Fourier transform, and random number capabilities\nBesides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container of generic data. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases.\n\rimport numpy as np\r# use [ ] for every row inside np.array([]) for matrix\rb1 = np.array([1,2,3,5,5]) #Declaring a NumPy Array b2 = np.array([4,5,6,7,7])\rprint(b1+b2)\r## [ 5 7 9 12 12]\rprint(b2 * 3)\r## [12 15 18 21 21]\rprint(\u0026quot;No. of dimensions: \u0026quot;, b1.ndim) # Rows in array, considered as a matrix. # Printing shape of array\r## No. of dimensions: 1\rprint(\u0026quot;Shape of array: \u0026quot;, b1.shape) # Dimension # Printing size (total number of elements) of array\r## Shape of array: (5,)\rprint(\u0026quot;Size of array: \u0026quot;, b1.size) # elements in a row or column elements. # Printing the datatype of elements in array\r## Size of array: 5\rprint(\u0026quot;Array stores elements of type: \u0026quot;, b1.dtype)\r## Array stores elements of type: int32\rArray creation: You can create arrays in NumPy in various ways.\nFor example, you can create an array from a regular Python list or tuple using the array function.\nThe type of the resulting array is deduced from the type of the elements in the sequences.\nOften, we need to declare arrays whose sizes are known but elements are initially unknown. Hence, NumPy offers many functions to create arrays with initial placeholder content. This minimizes the necessity of growing arrays, which is generally an expensive operation.\nExamples: np.zeros, np.ones, np.full, np.empty, etc.\rTo create sequences of numbers, NumPy provides a function analogous to range that returns arrays instead of lists.\narange: returns evenly spaced values within a given interval. In this, step size is specified.\nlinspace: returns evenly spaced values within a given interval. Number of elements are returned.\nReshaping array: The reshape method is used to reshape an array. If you have an array (a1, a2, a3, …, aN) and you want to reshape and convert it into another array of shape (b1, b2, b3, …, bM), you can do it easily using the reshape method. But the only precondition is that a1 x a2 x a3 … x aN = b1 x b2 x b3 … x bM . (i.e. , the total number of elements in the array should be the same, or the original size of array should remain unchanged.)\nFlatten array: The flatten method is used to convert an array into one dimension. It accepts order argument. Default value is ‘C’ (for row-major order), and you can use ‘F’ to use the flatten method for column major order.\nLet us see some examples.\n# array creation import numpy as np\r# Creating array from a list with type float\rA = np.array([[1, 2, 4], [5, 8, 7]], dtype = \u0026#39;float\u0026#39;)\r# Create a 3X4 array with all zeros. Please note, we have used double paranthesis. B = np.zeros((3, 4))\r# Create an array of complex numbers C = np.full((3, 3), 6, dtype = \u0026#39;complex\u0026#39;)\r# Create an array with random values\rnp.random.seed(2) # A seed is set to ensure that the results are consistent if you use this array in future computations also.\rD = np.random.randn(2, 2)\rE = np.random.random((2, 2)) # Exercise : Find out the difference between D and E print (\u0026quot;Array created using passed list:\\n\u0026quot;, A)\r## Array created using passed list:\r## [[1. 2. 4.]\r## [5. 8. 7.]]\rprint (\u0026quot;\\nAn array initialized with all zeros:\\n\u0026quot;, B)\r## ## An array initialized with all zeros:\r## [[0. 0. 0. 0.]\r## [0. 0. 0. 0.]\r## [0. 0. 0. 0.]]\rprint (\u0026quot;\\nAn array initialized with all 6s.\u0026quot;\r\u0026quot;Array type is complex:\\n\u0026quot;, C)\r## ## An array initialized with all 6s.Array type is complex:\r## [[6.+0.j 6.+0.j 6.+0.j]\r## [6.+0.j 6.+0.j 6.+0.j]\r## [6.+0.j 6.+0.j 6.+0.j]]\rprint (\u0026quot;\\nA random array:\\n\u0026quot;, D)\r## ## A random array:\r## [[-0.41675785 -0.05626683]\r## [-2.1361961 1.64027081]]\rprint (\u0026quot;\\nAnother random array:\\n\u0026quot;, E)\r## ## Another random array:\r## [[0.4203678 0.33033482]\r## [0.20464863 0.61927097]]\rArray Reshaping\nA = np.array([[1, 2, 3, 4],\r[5, 6, 7, 8],\r[9, 1, 2, 3]])\rnew_A = A.reshape(3, 2, 2) # (number of matrices, rows, column)\r# Flatten array\rB = np.array([[1, 2, 3], [4, 5, 6]])\rflat_B= B.flatten()\r#B.flatten(\u0026#39;F\u0026#39;)\rprint (\u0026quot;\\nOriginal array:\\n\u0026quot;, A)\r## ## Original array:\r## [[1 2 3 4]\r## [5 6 7 8]\r## [9 1 2 3]]\rprint (\u0026quot;Reshaped array:\\n\u0026quot;, new_A)\r## Reshaped array:\r## [[[1 2]\r## [3 4]]\r## ## [[5 6]\r## [7 8]]\r## ## [[9 1]\r## [2 3]]]\rprint (\u0026quot;\\nOriginal array:\\n\u0026quot;, B)\r## ## Original array:\r## [[1 2 3]\r## [4 5 6]]\rprint (\u0026quot;Fattened array:\\n\u0026quot;, flat_B)\r#print (\u0026quot;Column Fattened array:\\n\u0026quot;, column_flat_B)\r## Fattened array:\r## [1 2 3 4 5 6]\r4.5) Numpy - Sequences\rimport numpy as np\r# Create a sequence of integers # from 0 to 40 with steps of 5\ra = np.arange(0, 40, 5) # use if you know sequence range and increment, it excludes last value\r# Create a sequence of 15 values in range 0 to 5\rb = np.linspace(0, 10, 5) # use if you know sequence range and number of samples, it includes last value\rprint (\u0026quot;\\nA sequential array with steps of 5:\\n\u0026quot;, a)\r## ## A sequential array with steps of 5:\r## [ 0 5 10 15 20 25 30 35]\rprint (\u0026quot;\\nA sequential array with 15 values between\u0026quot;\r\u0026quot;0 and 5:\\n\u0026quot;, b)\r## ## A sequential array with 15 values between0 and 5:\r## [ 0. 2.5 5. 7.5 10. ]\r# simple plotting\r# Here, we are demonstrating the growth in GDP of China and America over a peroid of time\rfrom matplotlib import pyplot as plt\ryears = [1950, 1960, 1970, 1980, 1990, 2000, 2010]\rAmerica_gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3]\rChina_gdp = [30.2, 240.3, 675.9, 1262.5, 3579.6, 7089.7, 10958.3]\rplt.plot(years, America_gdp, color=\u0026#39;blue\u0026#39;, marker=\u0026#39;*\u0026#39;, linestyle=\u0026#39;solid\u0026#39;)\rplt.plot(years, China_gdp, color=\u0026#39;red\u0026#39;, marker=\u0026#39;o\u0026#39;, linestyle=\u0026#39;solid\u0026#39;)\rplt.show()\r```\n\r\r\r",
    "ref": "/blog/python-basics/"
  },{
    "title": "Android - Layout",
    "date": "",
    "description": "Introduction to Android Layout",
    "body": "\r\r\rLayout\rAn Layout represents a invisible component in an Android user interface. Layout in android are similar to Layout of rooms on a plot of given size. Layout is a container for different visible UI controls for example a Layout can hold “TextView”, “EditText” , “Button” which represents the different visible components on the UI screen of an Android application. There are different arrangements possible with given UI controls within a Layout therefore we have many different kinds of layout in an Android application. In Android the term layout refers to defining how the View components are displayed on the screen relative to each other A layout is typically defined partly by the View and partly by the ViewGroup which contains the View. The view group is the base class for layouts and views containers.\n\rAndroid Layout\rAndroid Layout is used to define the user interface which holds the UI controls or widgets that will appear on the screen of an android application or activity. Generally, every application is combination of View and ViewGroup. As we know, an android application contains a large number of activities and we can say each activity is one page of the application. So, each activities contains multiple user interface components and those components are the instances of the View and ViewGroup.\n\rExample\r\u0026lt;LinearLayout android:layout_height=\u0026quot;match_parent\u0026quot; android:layout_width=\u0026quot;match_parent\u0026quot; android:orientation=\u0026quot;horizontal\u0026quot; xmlns:android=\u0026quot;http://schemas.android.com/apk/res/android\u0026quot; /\u0026gt;\r\r",
    "ref": "/blog/introduction-to-android-layout/"
  },{
    "title": "Android - Layout",
    "date": "",
    "description": "Introduction to Android Layout",
    "body": "\r\r\rLayout\rAn Layout represents a invisible component in an Android user interface. Layout in android are similar to Layout of rooms on a plot of given size. Layout is a container for different visible UI controls for example a Layout can hold “TextView”, “EditText” , “Button” which represents the different visible components on the UI screen of an Android application. There are different arrangements possible with given UI controls within a Layout therefore we have many different kinds of layout in an Android application. In Android the term layout refers to defining how the View components are displayed on the screen relative to each other A layout is typically defined partly by the View and partly by the ViewGroup which contains the View. The view group is the base class for layouts and views containers.\n\rAndroid Layout\rAndroid Layout is used to define the user interface which holds the UI controls or widgets that will appear on the screen of an android application or activity. Generally, every application is combination of View and ViewGroup. As we know, an android application contains a large number of activities and we can say each activity is one page of the application. So, each activities contains multiple user interface components and those components are the instances of the View and ViewGroup.\n\rExample\r\u0026lt;LinearLayout android:layout_height=\u0026quot;match_parent\u0026quot; android:layout_width=\u0026quot;match_parent\u0026quot; android:orientation=\u0026quot;horizontal\u0026quot; xmlns:android=\u0026quot;http://schemas.android.com/apk/res/android\u0026quot; /\u0026gt;\r\r",
    "ref": "/blog/introduction-to-android-layout/"
  },{
    "title": "Android - Layout",
    "date": "",
    "description": "Introduction to Android Layout",
    "body": "\r\r\rLayout\rAn Layout represents a invisible component in an Android user interface. Layout in android are similar to Layout of rooms on a plot of given size. Layout is a container for different visible UI controls for example a Layout can hold “TextView”, “EditText” , “Button” which represents the different visible components on the UI screen of an Android application. There are different arrangements possible with given UI controls within a Layout therefore we have many different kinds of layout in an Android application. In Android the term layout refers to defining how the View components are displayed on the screen relative to each other A layout is typically defined partly by the View and partly by the ViewGroup which contains the View. The view group is the base class for layouts and views containers.\n\rAndroid Layout\rAndroid Layout is used to define the user interface which holds the UI controls or widgets that will appear on the screen of an android application or activity. Generally, every application is combination of View and ViewGroup. As we know, an android application contains a large number of activities and we can say each activity is one page of the application. So, each activities contains multiple user interface components and those components are the instances of the View and ViewGroup.\n\rExample\r\u0026lt;LinearLayout android:layout_height=\u0026quot;match_parent\u0026quot; android:layout_width=\u0026quot;match_parent\u0026quot; android:orientation=\u0026quot;horizontal\u0026quot; xmlns:android=\u0026quot;http://schemas.android.com/apk/res/android\u0026quot; /\u0026gt;\r\r",
    "ref": "/blog/introduction-to-android-layout/"
  },{
    "title": "Kontakt",
    "date": "",
    "description": "",
    "body": "",
    "ref": "/contact/"
  }]
